{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Map_reduce_using_hadoop (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_hRzxj-6JPF"
      },
      "source": [
        "## Installing hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9zhSf8YyWAz",
        "outputId": "38535f3c-5e75-485c-9c6c-1980f236cb55"
      },
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-06 16:59:54--  https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 135.181.209.10, 135.181.214.104, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500749234 (478M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.0.tar.gz.2’\n",
            "\n",
            "hadoop-3.3.0.tar.gz 100%[===================>] 477.55M  23.1MB/s    in 21s     \n",
            "\n",
            "2021-05-06 17:00:16 (22.4 MB/s) - ‘hadoop-3.3.0.tar.gz.2’ saved [500749234/500749234]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kmKu1L8ymJR",
        "outputId": "2e3757fd-adbd-42cd-ecac-dccc6c5581f3"
      },
      "source": [
        "!tar -xzvf hadoop-3.3.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSDataOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/TrashPolicyDefault.Emptier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/HarFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/XAttrSetFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchedRemoteIterator.BatchedEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ParentNotDirectoryException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.CreateParent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathIsNotDirectoryException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathHandle.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/CryptoFSDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/class-use/CryptoFSDataOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/crypto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/AbstractFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsConstants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/FTPException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/FTPException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/class-use/FTPFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/FTPFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ftp/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/UnsupportedMultipartUploaderException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystemMultipartUploader.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BlockLocation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclStatus.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntryType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsCreateModes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsPermission.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/FsAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/permission/class-use/AclEntryScope.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Trash.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/HarFs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FilterFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchListingOperations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.Rename.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/PathData.PathType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/TouchCommands.Touchz.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.DuplicatedOptionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.NotEnoughArgumentsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/TouchCommands.Touch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.UnknownOptionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.IllegalNumberOfArgumentsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/PathData.FileTypeRequirement.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.TooManyArgumentsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/class-use/CommandFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/FindOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/FilterExpression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/FindOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/FilterExpression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/Expression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/class-use/BaseExpression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/Expression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/BaseExpression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/shell/find/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/RawLocalFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.ChecksumParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StreamCapabilities.StreamCapability.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DelegationTokenRenewer.Renewable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShell.Help.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PathNotFoundException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ByteBufferReadable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.ChecksumCombineMode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/CanUnbuffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileContext.Util.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/LocatedFileStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShellPermissions.Chown.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StreamCapabilitiesPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/UnsupportedFileSystemException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/BatchedRemoteIterator.BatchedListEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/OpenFileParameters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.BiFunctionRaisingIOE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.CallableRaisingIOE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/AbstractFSBuilderImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FunctionsRaisingIOE.FunctionRaisingIOE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FutureDataInputStreamBuilderImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/impl/class-use/FsLinkResolution.FsLinkResolutionFunction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ReadOption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GlobalStorageStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.BufferSize.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FileStatusProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.LocalFileSystemPathHandleProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DUHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/StorageType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.HandleOpt.Data.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/CommonConfigurationKeysPublic.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GetSpaceUsed.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystem.Statistics.StatisticsData.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/GetSpaceUsed.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/DelegationTokenRenewer.RenewAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileAlreadyExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSProtos.FsPermissionProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ByteBufferPositionedReadable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ChecksumFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystem.MountPoint.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFs.MountPoint.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystem.MountPoint.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFs.MountPoint.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/NotInMountpointException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/Constants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ConfigUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/class-use/ViewFileSystemUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/NotInMountpointException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/Constants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ConfigUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/viewfs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FSBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/QuotaUsage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/ContentSummary.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsServerDefaults.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/PartialListing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/Options.CreateOpts.Perms.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FileSystem.Statistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/fs/FsShellPermissions.Chgrp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IOUtils.NullOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IOUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VIntWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/CompressedWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/EnumSetWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/RawComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BytesWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VLongWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.RawKeyValueIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/util/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/DummyErasureDecoder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/coder/class-use/DummyErasureEncoder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ErasureCodeConstants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ErasureCodeNative.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/class-use/ECSchema.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/grouper/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/codec/class-use/DummyErasureCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ErasureCodeConstants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ErasureCodeNative.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/ECSchema.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/erasurecode/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/NullWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ShortWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/GenericWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.ValueBytes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteBufferPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableFactories.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/FloatWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/NullWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ElasticByteBufferPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Writable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Reader.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SecureIOUtils.AlreadyExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IOUtils.NullOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IOUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VIntWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/CompressedWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/EnumSetWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/RawComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BytesWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VLongWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.RawKeyValueIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/NullWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ShortWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/GenericWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.ValueBytes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteBufferPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableFactories.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/FloatWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/NullWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ElasticByteBufferPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Writable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Reader.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SecureIOUtils.AlreadyExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MultipleIOException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BinaryComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.CompressionType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MD5Hash.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/TwoDArrayWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ByteWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Stringifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MD5Hash.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DoubleWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SetFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/WritableFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/AbstractMapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Closeable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Writer.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IntWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/UTF8.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayPrimitiveWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ReadaheadPool.ReadaheadRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/FloatWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Text.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ShortWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Writer.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DataInputByteBuffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Metadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VersionedWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ObjectWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SortedMapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SecureIOUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MultipleIOException.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/IntWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BooleanWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/LongWritable.DecreasingComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/VersionMismatchException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DataOutputOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/MapFile.Merger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Reader.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/SequenceFile.Sorter.SegmentDescriptor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/ArrayFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/Text.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BooleanWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DoubleWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/DefaultStringifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BloomMapFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/class-use/BytesWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MultipleIOException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIOException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/Errno.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.PmemMappedRegion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.NoMlockCacheManipulator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.SupportState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.Pmem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.CacheManipulator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.POSIX.Stat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.Windows.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/nativeio/class-use/NativeIO.Windows.AccessRight.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BinaryComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.CompressionType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MD5Hash.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/TwoDArrayWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ByteWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Stringifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MD5Hash.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DoubleWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SetFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/WritableFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/AbstractMapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Closeable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Writer.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IntWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/UTF8.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Utils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/RawComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Compression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/MetaBlockDoesNotExist.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Compression.Algorithm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Utils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/RawComparable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Compression.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/MetaBlockDoesNotExist.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Compression.Algorithm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/Utils.Version.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/SimpleBufferedOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.Scanner.Entry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.Scanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/BoundedRangeFileInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/MetaBlockAlreadyExists.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/class-use/TFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/Utils.Version.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.Scanner.Entry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.Scanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/MetaBlockAlreadyExists.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/file/tfile/TFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayPrimitiveWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/JavaSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/WritableSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/JavaSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/WritableSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/class-use/JavaSerializationComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroSpecificSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroReflectSerializable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroSpecificSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroReflectSerializable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroReflectSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/class-use/AvroSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/AvroSerialization.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/avro/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/JavaSerializationComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/serializer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodecFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CodecPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodec.Util.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SnappyCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BlockDecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2DummyCompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Decompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2InputStream.STATE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2DummyDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2InputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/CBZip2OutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/BZip2Constants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/bzip2/class-use/Bzip2Compressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CodecConstants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplittableCompressionCodec.READ_MODE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BZip2Codec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplittableCompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/BlockCompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodecFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CodecPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodec.Util.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SnappyCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BlockDecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CodecConstants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplittableCompressionCodec.READ_MODE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BZip2Codec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplittableCompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/BlockCompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/SplitCompressionInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DoNotPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DirectDecompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Lz4Codec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Compressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DirectDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/GzipCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/Decompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DefaultCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/GzipCodec.GzipOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/ZStandardCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DeflateCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/CompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.PassthroughDecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.StubDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/DecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/class-use/PassthroughCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/Lz4Decompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/lz4/class-use/Lz4Compressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/SplitCompressionInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DoNotPool.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardCompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zstd/class-use/ZStandardDecompressor.ZStandardDirectDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DirectDecompressionCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Lz4Codec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Compressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DirectDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/GzipCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInGzipDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInZlibDeflater.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.CompressionHeader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionStrategy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionLevel.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibDecompressor.ZlibDirectDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibCompressor.CompressionHeader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/ZlibFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/zlib/class-use/BuiltInZlibInflater.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/Decompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DefaultCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/GzipCodec.GzipOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/ZStandardCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DeflateCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/CompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.PassthroughDecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.StubDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/DecompressorStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyCompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyDecompressor.SnappyDirectDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/snappy/class-use/SnappyDecompressor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/compress/PassthroughCodec.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ReadaheadPool.ReadaheadRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/FloatWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Text.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ShortWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Writer.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DataInputByteBuffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Metadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VersionedWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ObjectWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SortedMapWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SecureIOUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MultipleIOException.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/IntWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BooleanWritable.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/LongWritable.DecreasingComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/VersionMismatchException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DataOutputOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/MapFile.Merger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/Idempotent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/MultiException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.MultipleLinearRandomRetry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/FailoverProxyProvider.ProxyInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/DefaultFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.MultipleLinearRandomRetry.Pair.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.RetryAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/AtMostOnce.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryProxy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/FailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicy.RetryAction.RetryDecision.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/retry/class-use/RetryPolicies.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Reader.Option.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/SequenceFile.Sorter.SegmentDescriptor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/ArrayFile.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/Text.Comparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BooleanWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DoubleWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/DefaultStringifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BloomMapFile.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/io/BytesWritable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/UnsupportedCodecException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/class-use/UnsupportedCodecException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyShell.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.DelegationTokenExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/CachingKeyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.KeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/UserProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyShell.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderDelegationTokenExtension.DelegationTokenExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderDelegationTokenExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/CachingKeyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.KeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/UserProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderCryptoExtension.EncryptedKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderExtension.Extension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProviderCryptoExtension.CryptoExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/JavaKeyStoreProvider.KeyMetadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.Metadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/KeyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/class-use/JavaKeyStoreProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.EncryptedKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderExtension.Extension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.CryptoExtension.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.KeyMetadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.Metadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSTokenRenewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSMetadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.KMSEncryptedKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSTokenRenewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSMetadata.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.KMSEncryptedKeyVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/ValueQueue.SyncGenerationPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSDelegationToken.KMSDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/LoadBalancingKMSClientProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/KMSClientProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/class-use/ValueQueue.QueueRefiller.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/ValueQueue.SyncGenerationPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.KMSDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/KMSClientProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/ValueQueue.QueueRefiller.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/kms/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/KeyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/random/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/crypto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/HadoopIllegalArgumentException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/IrqHandler.Interrupted.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LaunchableService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/IrqHandler.InterruptData.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LauncherExitCodes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/AbstractLaunchableService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/IrqHandler.Interrupted.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LaunchableService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/HadoopUncaughtExceptionHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/IrqHandler.InterruptData.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LauncherExitCodes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/AbstractLaunchableService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLauncher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLaunchException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/ServiceLauncher.MinimalGenericOptionsParser.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/LauncherArguments.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/class-use/InterruptEscalator.ServiceForcedShutdown.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLauncher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLaunchException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/ServiceLauncher.MinimalGenericOptionsParser.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/LauncherArguments.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/InterruptEscalator.ServiceForcedShutdown.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/launcher/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/LoggingStateChangeListener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/CompositeService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/AbstractService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateChangeListener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceOperations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/LoggingStateChangeListener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/CompositeService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/AbstractService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateChangeListener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceOperations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/Service.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/CompositeService.CompositeServiceShutdownHook.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/LifecycleEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/Service.STATE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceOperations.ServiceListeners.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/class-use/ServiceStateModel.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/Service.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/CompositeService.CompositeServiceShutdownHook.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/LifecycleEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/Service.STATE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceOperations.ServiceListeners.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/ServiceStateModel.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/service/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ConfigRedactor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.DeprecationDelta.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ConfigRedactor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.DeprecationDelta.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurableBase.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/StorageUnit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configurable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.IntegerRanges.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configured.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Reconfigurable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ConfServlet.BadFormatException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/Configuration.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/ReconfigurationUtil.PropertyChange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/class-use/StorageSize.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurableBase.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/StorageUnit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configurable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.IntegerRanges.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configured.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Reconfigurable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ConfServlet.BadFormatException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/Configuration.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/ReconfigurationUtil.PropertyChange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/StorageSize.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/conf/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/SpanReceiverInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/SpanReceiverInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.SpanReceiverListInfoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/SpanReceiverInfoBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.TraceAdminService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPair.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPair.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.AddSpanReceiverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ConfigPairOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.ListSpanReceiversResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/class-use/TraceAdminPB.RemoveSpanReceiverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.SpanReceiverListInfoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/SpanReceiverInfoBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.TraceAdminService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPair.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPair.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.AddSpanReceiverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ConfigPairOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.ListSpanReceiversResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/tracing/TraceAdminPB.RemoveSpanReceiverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/class-use/GenericRefreshProtocolProtos.GenericRefreshRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseCollectionProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/RefreshCallQueueProtocolProtos.RefreshCallQueueRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/GenericRefreshProtocolProtos.GenericRefreshRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/proto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine2.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcMultiplexer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ExternalCall.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WeightedTimeCostProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Client.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.MetricsProxy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DefaultRpcScheduler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetryCache.CacheEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.RpcKind.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/StandbyException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcClientException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.Call.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RemoteException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WeightedRoundRobinMultiplexer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolMetaInfoServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcScheduler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcEngine.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/UnexpectedServerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolMetaInfoPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProcessingDetails.Timing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/FairCallQueue.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcScheduler.DecayTask.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngineCallback2.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DecayRpcSchedulerMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/IpcException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/UserIdentityProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcNoSuchProtocolException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProxyCombiner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcClientUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WritableRpcEngine.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallQueueManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcInvocationHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallerContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/Server.Connection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetryCache.CacheEntryWithPayload.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolSignature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine2.Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcWritable.Buffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/DefaultCostProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/WritableRpcEngine.Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcServerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RetriableException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/FairCallQueueMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngineCallback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RefreshRegistry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtocolProxy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/ProtobufRpcEngine.Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcNoSuchMethodException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CostProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/AsyncCallLimitExceededException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.VersionMismatch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RpcException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/RPC.Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/VersionedProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/class-use/CallerContext.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/GenericRefreshProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/GenericRefreshProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/RefreshCallQueueProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/class-use/RefreshCallQueueProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuthOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuth.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslAuth.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.RpcErrorCodeProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuthOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuth.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslAuth.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.RpcErrorCodeProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.SaslState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcRequestHeaderProto.OperationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.RpcStatusProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolSignatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcSaslProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngineProtos.RequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcResponseHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RpcKindProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCTraceInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.IpcConnectionContextProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/IpcConnectionContextProtos.UserInformationProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtobufRpcEngine2Protos.RequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/RpcHeaderProtos.RPCCallerContextProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolVersionsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolVersionProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.ProtocolInfoService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/class-use/ProtocolInfoProtos.GetProtocolSignatureRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.SaslState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcRequestHeaderProto.OperationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.RpcStatusProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolSignatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcSaslProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngineProtos.RequestHeaderProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcResponseHeaderProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RpcKindProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCTraceInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.IpcConnectionContextProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/IpcConnectionContextProtos.UserInformationProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtobufRpcEngine2Protos.RequestHeaderProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/RpcHeaderProtos.RPCCallerContextProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolVersionsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolVersionProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.ProtocolInfoService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/ProtocolInfoProtos.GetProtocolSignatureRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ipc/protobuf/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/FailoverFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocolHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceStateProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceStateProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAStateChangeRequestInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.ZKFCProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToObserverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HARequestSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.HAServiceProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.CedeActiveRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.GracefulFailoverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.GetServiceStatusResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToStandbyResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.TransitionToActiveResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/ZKFCProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/class-use/HAServiceProtocolProtos.MonitorHealthResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.ZKFCProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToObserverResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HARequestSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.HAServiceProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.CedeActiveRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.GracefulFailoverResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.GetServiceStatusResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToStandbyResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.TransitionToActiveResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/ZKFCProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/proto/HAServiceProtocolProtos.MonitorHealthResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/FenceMethod.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ActiveStandbyElector.ActiveNotFoundException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ShellCommandFencer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/PowerShellFencer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/FailoverFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocolHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/FenceMethod.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ActiveStandbyElector.ActiveNotFoundException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ShellCommandFencer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/PowerShellFencer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ActiveStandbyElector.ActiveStandbyElectorCallback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAAdmin.UsageInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.HAServiceState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/ServiceFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HealthCheckFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceTarget.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/BadFencingConfigurationException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.RequestSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/SshFenceByTcpPort.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/class-use/HAServiceProtocol.StateChangeRequestInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/HAServiceProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/ZKFCProtocolClientSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/HAServiceProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/class-use/ZKFCProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/ZKFCProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ActiveStandbyElector.ActiveStandbyElectorCallback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAAdmin.UsageInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.HAServiceState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/ServiceFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HealthCheckFailedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceTarget.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/BadFencingConfigurationException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.RequestSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/SshFenceByTcpPort.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/ha/HAServiceProtocol.StateChangeRequestInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.AbstractCallback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsJsonBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsRecordBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/GlobFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/RegexFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/GlobFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/class-use/RegexFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/filter/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MBeans.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/SampleStat.MinMax.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/QuantileEstimator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MetricsCache.Record.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/MetricsCache.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MBeans.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/SampleStat.MinMax.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/QuantileEstimator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MetricsCache.Record.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/MetricsCache.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/class-use/Servers.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/Servers.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/util/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsRecord.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.AbstractCallback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsJsonBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsRecordBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSource.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsRecord.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/AbstractMetric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsCollector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsTag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricStringBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsPlugin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.Callback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystemMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/class-use/MetricsSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/AbstractMetric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsCollector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsTag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricStringBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsPlugin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/source/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.Callback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystemMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/GraphiteSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/StatsDSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/RollingFileSystemSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/GraphiteSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/StatsDSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/StatsDSink.StatsD.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/GraphiteSink.Graphite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/PrometheusMetricsSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/class-use/FileSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/StatsDSink.StatsD.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/GraphiteSink.Graphite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.GangliaConfType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/GangliaSink30.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.GangliaConfType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/AbstractGangliaSink.GangliaSlope.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/class-use/GangliaSink31.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.GangliaSlope.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/FileSink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/sink/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/class-use/Metric.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/Metric.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/annotation/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeInt.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRollingAverages.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounterInt.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MetricsRegistry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/Interns.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeInt.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRollingAverages.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRatesWithAggregation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounterInt.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MetricsRegistry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/Interns.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRates.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeLong.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableStat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableRate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableQuantiles.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGauge.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/DefaultMetricsSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableGaugeFloat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableCounterLong.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/class-use/MutableMetric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRates.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeLong.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableStat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableRate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableQuantiles.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGauge.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableCounterLong.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/lib/MutableMetric.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/metrics2/MetricsSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/org/apache/hadoop/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/overview-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/overview-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/allclasses-noframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/build/source/hadoop-common-project/hadoop-common/target/api/package-list\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/DeprecatedProperties.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/GroupsMapping.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/UnixShellAPI.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/CommandsManual.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/CredentialProviderAPI.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/DownstreamDev.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/RackAwareness.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-common/AdminCompatibilityGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-native-client/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/serialized-form.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/index.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/stylesheet.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/index-all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/constant-values.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/help-doc.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/deprecated-list.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/allclasses-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/script.js\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSUtilClient.CorruptedBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReplicaAccessorBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/UnknownCipherSuiteException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/AddErasureCodingPolicyResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeInfo.AdminStates.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotAccessControlException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeInfo.DatanodeInfoBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.State.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/RollingUpgradeInfo.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/EncryptionZone.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CorruptFileBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/BlockType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.DiffReportListingEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshottableDirectoryStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ECBlockGroupStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/TrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/ReplaceDatanodeOnFailure.Policy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/BlockPinningException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/PipelineAck.ECN.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/TrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/ReplaceDatanodeOnFailure.Policy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/BlockPinningException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/PipelineAck.ECN.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/DatanodeAdminProperties.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/AddErasureCodingPolicyResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeInfo.AdminStates.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotAccessControlException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeInfo.DatanodeInfoBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshottableDirectoryStatus.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.State.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/RollingUpgradeInfo.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/EncryptionZone.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CorruptFileBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/BlockType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReportListing.DiffReportListingEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshottableDirectoryStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ECBlockGroupStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/DatanodeAdminProperties.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.SafeModeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.UpgradeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.StoragePolicySatisfierMode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsNamedFileStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CachePoolStats.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReportListing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsFileStatus.Flags.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.Expiration.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ReplicatedBlockStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.ReencryptAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/OpenFilesIterator.OpenFilesType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.DatanodeReportType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/ZoneReencryptionStatus.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/HdfsConstants.RollingUpgradeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotDiffReport.DiffReportEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/class-use/CacheDirectiveStats.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.SafeModeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.UpgradeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.StoragePolicySatisfierMode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsNamedFileStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CachePoolStats.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReportListing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsFileStatus.Flags.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.Expiration.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ReplicatedBlockStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.ReencryptAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/OpenFilesIterator.OpenFilesType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.DatanodeReportType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/ZoneReencryptionStatus.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/HdfsConstants.RollingUpgradeAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.DiffReportEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocol/CacheDirectiveStats.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/BasicInetPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/BasicInetPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/class-use/NioInetPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/NioInetPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/net/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/CannotObtainBlockLengthException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DeadNodeDetector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSMultipartUploaderFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingChunk.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.VerticalRange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/LongBitFormat.Enum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.BlockReadStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingCell.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.AlignedStripe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingChunk.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.VerticalRange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/LongBitFormat.Enum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.BlockReadStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingCell.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.AlignedStripe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.ChunkByteBuffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/LongBitFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripeRange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/ByteArrayManager.Conf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/IOUtilsClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/class-use/StripedBlockUtil.StripingChunkReadResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.ChunkByteBuffer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/LongBitFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripeRange.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/ByteArrayManager.Conf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/IOUtilsClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/StripedBlockUtil.StripingChunkReadResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/util/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSOpsCountStatistics.OpType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReplicaAccessor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/InMemoryAliasMapFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/ClientHAProxyFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/WrappedFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/InMemoryAliasMapFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/ClientHAProxyFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/WrappedFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/ConfiguredFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/AbstractNNFailoverProxyProvider.NNProxyInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/AbstractNNFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/IPFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/RequestHedgingProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/ConfiguredFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.NNProxyInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/AbstractNNFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/IPFailoverProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/namenode/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaNotFoundException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.DiskBalancerWorkEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaNotFoundException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancerWorkStatus.DiskBalancerWorkEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CachingStrategy.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CachingStrategy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancerWorkStatus.Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/CorruptMetaHeaderException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CachingStrategy.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CachingStrategy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancerWorkStatus.Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/CorruptMetaHeaderException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/datanode/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/SlowDiskReports.DiskOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorageReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DataNodeUsageReport.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/DatanodeStorage.State.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.SlotId.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.PathInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplicaInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.SlotId.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.PathInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitReplicaInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.ShmId.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitCache.ShortCircuitReplicaCreator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DomainSocketFactory.PathState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShmManager.PerDatanodeVisitorInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.Slot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/DfsClientShmManager.Visitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.SlotIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitCache.CacheVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/class-use/ShortCircuitShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.ShmId.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.ShortCircuitReplicaCreator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.PathState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.PerDatanodeVisitorInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.Slot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/DfsClientShmManager.Visitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.SlotIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitCache.CacheVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSUtilClient.CorruptedBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReplicaAccessorBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/UnknownCipherSuiteException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/CannotObtainBlockLengthException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DeadNodeDetector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSMultipartUploaderFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSOpsCountStatistics.OpType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReplicaAccessor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/NameNodeProxiesClient.ProxyAndInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/UnknownCryptoProtocolVersionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/ReadStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DistributedFileSystem.HdfsDataOutputStreamBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSClient.DFSDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSOpsCountStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/class-use/DFSInotifyEventInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/NameNodeProxiesClient.ProxyAndInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/PBHelperClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.AccessMode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/BlockTokenIdentifier.AccessMode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/block/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenIdentifier.WebHdfsDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenIdentifier.SWebHdfsDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenIdentifier.WebHdfsDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenIdentifier.SWebHdfsDelegationTokenIdentifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.UnlinkEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.EventType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/EventBatch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.TruncateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.UnlinkEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.EventType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/EventBatch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.TruncateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.AppendEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.RenameEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.AppendEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CloseEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.UnlinkEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.CreateEvent.INodeType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.RenameEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/MissingEventsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/class-use/Event.MetadataUpdateEvent.MetadataType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.AppendEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.RenameEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.AppendEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CloseEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.UnlinkEvent.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.CreateEvent.INodeType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.RenameEvent.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/MissingEventsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/Event.MetadataUpdateEvent.MetadataType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/inotify/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/UnknownCryptoProtocolVersionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/ReadStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Retry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.HttpClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.SyncFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.DeprecatedKeys.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.StripedRead.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Write.ByteArrayManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Failover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Retry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.HttpClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataOutputStream.SyncFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.DeprecatedKeys.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.StripedRead.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Write.ByteArrayManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Failover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/DfsPathCapabilities.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Read.ShortCircuit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Mmap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Write.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.Read.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/BlockReportOptions.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/BlockReportOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.ShortCircuit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsDataOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.BlockWrite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/CreateEncryptionZoneFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/class-use/HdfsClientConfigKeys.HedgedRead.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/DfsPathCapabilities.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Read.ShortCircuit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Mmap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Write.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.Read.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/BlockReportOptions.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/BlockReportOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.ShortCircuit.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/metrics/class-use/BlockReaderIoProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.FailureInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/BlockReaderFactory.FailureInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/DfsClientConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/DfsClientConf.ShortCircuitConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/SnapshotDiffReportGenerator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/class-use/BlockReaderFactory.BlockReaderPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/DfsClientConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/DfsClientConf.ShortCircuitConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/SnapshotDiffReportGenerator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.BlockReaderPeer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/impl/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.BlockWrite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/CreateEncryptionZoneFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.HedgedRead.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/client/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DistributedFileSystem.HdfsDataOutputStreamBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.WebHdfsInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/OAuth2ConnectionConfigurator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/AccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/OAuth2ConnectionConfigurator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/AccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/OAuth2Constants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/ConfRefreshTokenBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/AccessTokenTimer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/ConfCredentialBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/class-use/CredentialBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/OAuth2Constants.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/ConfRefreshTokenBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/AccessTokenTimer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/ConfCredentialBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/CredentialBasedAccessTokenProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/oauth2/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.WebHdfsInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/KerberosUgiAuthenticator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/WebHdfsFileSystem.ReadRunner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/ByteRangeInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/SWebHdfsFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/JsonUtilClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/SSLConnectionConfigurator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/class-use/ByteRangeInputStream.URLOpener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/KerberosUgiAuthenticator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.ReadRunner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/ByteRangeInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/SWebHdfsFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/JsonUtilClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/SSLConnectionConfigurator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OverwriteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StorageSpaceQuotaParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/FsActionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OldSnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ECPolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/SnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DestinationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.TemporaryRedirectOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OffsetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/AccessTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NameSpaceQuotaParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/BufferSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RecursiveParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/Param.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/UserParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PostOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OverwriteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StorageSpaceQuotaParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/FsActionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OldSnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ECPolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/SnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DestinationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.TemporaryRedirectOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OffsetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/AccessTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NameSpaceQuotaParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/BufferSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RecursiveParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/Param.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UserParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PostOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ConcatSourcesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GetOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/CreateParentParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ExcludeDatanodesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StorageTypeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DeleteOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StartAfterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/OwnerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/CreateFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenArgumentParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NoRedirectParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PutOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RenameOptionSetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/LengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrEncodingParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ModificationTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GetOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ReplicationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/HttpOpParam.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/AclPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/RenewerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NewLengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DoAsParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrSetFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/StoragePolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UnmaskedPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PostOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DelegationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/XAttrValueParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/DeleteOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/BlockSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/PutOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/class-use/GroupParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ConcatSourcesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GetOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/CreateParentParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ExcludeDatanodesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StorageTypeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DeleteOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StartAfterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/OwnerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/CreateFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/TokenArgumentParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NoRedirectParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PutOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RenameOptionSetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/LengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrEncodingParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ModificationTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GetOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/ReplicationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/HttpOpParam.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/AclPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/RenewerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/NewLengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DoAsParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrSetFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/StoragePolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/UnmaskedPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PostOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DelegationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/XAttrValueParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/DeleteOpParam.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/BlockSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/PutOpParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/GroupParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/resources/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/ByteRangeInputStream.URLOpener.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/web/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSClient.DFSDataInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSOpsCountStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/DFSInotifyEventInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/hdfs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/XAttr.NameSpace.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/XAttr.NameSpace.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/WebHdfs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/XAttr.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/SWebHdfs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/class-use/CacheFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/WebHdfs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/XAttr.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/SWebHdfs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/CacheFlag.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/org/apache/hadoop/fs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/overview-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/allclasses-noframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/build/source/hadoop-hdfs-project/hadoop-hdfs-client/target/api/package-list\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-client/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/WebHDFS.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/user_comments_for_Apache_Hadoop_HDFS_3.2.1_to_Apache_Hadoop_HDFS_3.3.0.xml\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/missingSinces.txt\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/pkg_org.apache.hadoop.hdfs.server.namenode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_topleftframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/changes-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_statistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/pkg_org.apache.hadoop.hdfs.server.aliasmap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/jdiff_help.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/alldiffs_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/classes_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/methods_index_removals.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/constructors_index_changes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/packages_index_all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/changes/fields_index_additions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/Apache_Hadoop_HDFS_3.3.0.xml\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/jdiff/xml/stylesheet-jdiff.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ViewFs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/configuration.xsl\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/MemoryStorage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsProvidedStorage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/federation.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/viewfs_TypicalMountTable.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-forward.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/federation-background.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-server.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.odg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/caching.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.odg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/LazyPersistWrites.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfsproxy-overview.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/images/hdfs-logo.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsUpgradeDomain.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/Federation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsDataNodeAdminGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/serialized-form.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/index.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/stylesheet.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/index-all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/constant-values.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/help-doc.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/deprecated-list.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/allclasses-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/script.js\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/BlackListBasedTrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/class-use/WhitelistBasedTrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/BlackListBasedTrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/WhitelistBasedTrustedChannelResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/datatransfer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/FSLimitException.PathComponentTooLongException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotInfo.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/BlockListAsLongs.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.FeatureInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/FSLimitException.MaxDirectoryItemsExceededException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/LayoutVersion.LayoutFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/class-use/SnapshotException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/FSLimitException.PathComponentTooLongException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/SnapshotInfo.Bean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/BlockListAsLongs.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.FeatureInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/FSLimitException.MaxDirectoryItemsExceededException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/LayoutVersion.LayoutFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/SnapshotException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocol/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/DFSTopologyNodeImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/class-use/DFSNetworkTopology.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/DFSNetworkTopology.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/net/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/WebHdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/DFSUtil.ServiceComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/AtomicFileOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/FoldedTreeSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.Stanza.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ConstEnumCounters.ConstEnumException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ConstEnumCounters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/EnumCounters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/AtomicFileOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/FoldedTreeSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.Stanza.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ConstEnumCounters.ConstEnumException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ConstEnumCounters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/EnumCounters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ReadOnlyList.Util.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/RwLock.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/LightWeightHashSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.UnmanglingError.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/MD5FileUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/DataTransferThrottler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/ReferenceCountMap.ReferenceCounter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/LightWeightLinkedSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Holder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Container.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.UndoInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Element.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/EnumDoubles.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/XMLUtils.InvalidXmlException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/class-use/Diff.Processor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ReadOnlyList.Util.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/RwLock.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/LightWeightHashSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.UnmanglingError.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/MD5FileUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/DataTransferThrottler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/ReferenceCountMap.ReferenceCounter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/LightWeightLinkedSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Holder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Container.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.UndoInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Element.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/EnumDoubles.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/XMLUtils.InvalidXmlException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/Diff.Processor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/util/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/class-use/Mover.Cli.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/Mover.Cli.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/mover/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKey.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/class-use/NameNodeMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/NameNodeMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/metrics/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNode.NameNodeHAContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.WithCount.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/JournalManager.CorruptionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlink.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Section.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.DstReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Snapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/startupprogress/class-use/StartupProgress.Counter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiffOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/metrics/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.User.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.TopWindow.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.User.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.TopWindow.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/class-use/RollingWindowManager.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/RollingWindowManager.Op.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/window/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/top/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ContentCounts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfyManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfier.DatanodeMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/BlockStorageMovementNeeded.DirPendingWorkInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/BlockStorageMovementAttemptedItems.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/sps/class-use/StoragePolicySatisfier.DatanodeWithStorage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Section.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.SnapshotOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.BlocksMapUpdateInfo.UpdatedReplicationInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorage.NameNodeFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSTreeTraverser.TraverseInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaCounts.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFileOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Entry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.PositionTrackingInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNode.OperationCategory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.TransferResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.SectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/AuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DefaultINodeAttributesProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiffOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKey.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNode.NameNodeHAContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.WithCount.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/JournalManager.CorruptionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlink.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Section.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.DstReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Snapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiffOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/ContentCounts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatPBINode.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Section.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.SnapshotOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.BlocksMapUpdateInfo.UpdatedReplicationInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorage.NameNodeFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSTreeTraverser.TraverseInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaCounts.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFileOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorageRetentionManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Entry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DirectoryWithQuotaFeature.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogLoader.PositionTrackingInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNode.OperationCategory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/TransferFsImage.TransferResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSPermissionChecker.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.SectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/AuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DefaultINodeAttributesProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiffOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CachePool.DirectiveList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodesInPath.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectory.SnapshotAndINode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistToken.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKey.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReference.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/ContentCounts.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Quota.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DirectoryWithQuotaFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SaverContext.DeduplicationMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaCounts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/AclEntryStatusFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.OpInstanceCache.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNUpgradeUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SectionName.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/MetaRecoveryContext.RequestStopException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSDirectory.DirOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.NameSystemSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NameNodeLayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/SerialNumberManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectoryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.EntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/HdfsAuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormat.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaByStorageTypeEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.LoaderContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CacheManager.PersistState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Entry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.WithName.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Content.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistToken.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.BlocksMapUpdateInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AuthorizationContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummaryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSDirAttrOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeFile.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.SaverContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.DelegationKeyOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.PersistTokenOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrCompactProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FilesUnderConstructionSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/EncryptionZoneManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectoryAttributes.CopyWithQuota.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotSection.Snapshot.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeFileAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.ErasureCodingSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatPBINode.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeDirectoryAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.DiffEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.StringTableSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/SerialNumberManager.StringTable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeDirectory.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/IsNameNodeActiveServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/EncryptionFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.CacheManagerSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSImageFormatProtobuf.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.ReclaimContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/QuotaByStorageTypeEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DefaultAuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INodeSymlinkOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/CheckpointFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/XAttrFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/StoragePolicySummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/NNStorage.NameNodeDirType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeReferenceSection.INodeReferenceOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.XAttrFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SecretManagerSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AccessControlEnforcer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FSEditLogOp.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeDirectorySection.DirEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/DfsServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.AclFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.INode.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INodeAttributeProvider.AuthorizationContext.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/Quota.Counts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/INode.QuotaDelta.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.FileSummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.INodeSection.QuotaByStorageTypeEntryProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/class-use/FsImageProto.SnapshotDiffSection.FileDiff.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CachePool.DirectiveList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodesInPath.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeature.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.SnapshotAndINode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistToken.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKey.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReference.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ContentCounts.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Quota.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.FileUnderConstructionEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DirectoryWithQuotaFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SaverContext.DeduplicationMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaCounts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/AclEntryStatusFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.OpInstanceCache.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFile.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SectionName.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.RequestStopException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlink.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSDirectory.DirOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.NameSystemSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NameNodeLayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectoryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.EntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.CreatedListEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/HdfsAuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.LoaderContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CacheManager.PersistState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Entry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.FileUnderConstructionFeatureOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.WithName.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DirectoryDiff.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Content.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistToken.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.BlocksMapUpdateInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AuthorizationContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/class-use/NamenodeWebHdfsMethods.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/web/resources/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummaryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeFile.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.SaverContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.DelegationKeyOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.PersistTokenOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrCompactProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FilesUnderConstructionSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/EncryptionZoneManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.CopyWithQuota.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotSection.Snapshot.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeFileAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.ErasureCodingSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeDirectoryAttributes.SnapshotCopy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.DiffEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.StringTableSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/SerialNumberManager.StringTable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeDirectory.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/IsNameNodeActiveServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/EncryptionFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.CacheManagerSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.ReclaimContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/QuotaByStorageTypeEntry.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DefaultAuditLogger.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INodeSymlinkOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/CheckpointFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/XAttrFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/StoragePolicySummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/NNStorage.NameNodeDirType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeReferenceSection.INodeReferenceOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/RemoteNameNodeInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/class-use/HAZKInfoProtos.ActiveNodeInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/HAZKInfoProtos.ActiveNodeInfo.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/proto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/RemoteNameNodeInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/class-use/NameNodeHAProxyFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/NameNodeHAProxyFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/ha/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.XAttrFeatureProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSectionOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SecretManagerSection.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AccessControlEnforcer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeDirectorySection.DirEntryOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/DfsServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.DirectoryDiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListBySkipList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotStatsMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryWithSnapshotFeature.DirectoryDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectoryDiffListFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryWithSnapshotFeature.DirectoryDiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffListBySkipList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotStatsMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryWithSnapshotFeature.DirectoryDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotFSImageFormat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DirectoryDiffListFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/Snapshot.Root.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FileDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotFSImageFormat.ReferenceMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FSImageFormatPBSnapshot.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffListByArrayList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FSImageFormatPBSnapshot.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/SnapshotManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/DiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/class-use/FileDiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/Snapshot.Root.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotFSImageFormat.ReferenceMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.Loader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffListByArrayList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.Saver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/SnapshotManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/DiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/snapshot/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.AclFeatureProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.INode.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.AuthorizationContext.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/Quota.Counts.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/INode.QuotaDelta.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.FileSummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.INodeSection.QuotaByStorageTypeEntryProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/namenode/FsImageProto.SnapshotDiffSection.FileDiff.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.RecoveryTaskStriped.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/class-use/DataNodeMetricHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetricHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/metrics/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskFileCorruptException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaAlreadyExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ErrorReportAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.BlockPoolReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FSCachingGetSpaceUsed.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.DiskBalancerMover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/UnexpectedReplicaStateException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/DatasetVolumeChecker.Callback.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/AbstractFuture.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/checker/class-use/DatasetVolumeChecker.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/StorageLocation.CheckContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BlockRecoveryWorker.RecoveryTaskStriped.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskFileCorruptException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaAlreadyExistsException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ErrorReportAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.BlockPoolReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FSCachingGetSpaceUsed.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaWaitingToBeRecovered.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/VolumeScanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.DiskBalancerMover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/UnexpectedReplicaStateException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/StorageLocation.CheckContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/SecureDataNodeStarter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaInPipeline.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ChunkChecksum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.NewShmInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FileIoProvider.OPERATION.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DataNodeLayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/FinalizedReplica.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplica.ReplicaDirInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BPServiceActorActionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplicaInPipeline.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.Visitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.RegisteredShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaBeingWritten.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ShortCircuitRegistry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/SecureDataNodeStarter.SecureResources.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.ScanInfoVolumeReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.VolumePair.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/BPServiceActorAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReportBadBlockAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/LocalReplica.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaUnderRecovery.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/ReplicaHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DiskBalancer.BlockMover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/class-use/DirectoryScanner.ReportCompiler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ChunkChecksum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.NewShmInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FileIoProvider.OPERATION.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DataNodeLayoutVersion.Feature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplica.ReplicaDirInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BPServiceActorActionException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplicaInPipeline.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.Visitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.VolumeCheckContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.FsVolumeReferences.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsDatasetSpi.Factory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.VolumeCheckContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsDatasetSpi.FsVolumeReferences.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/AvailableSpaceVolumeChoosingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.BlockIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/LengthInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/ReplicaInputStreams.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/RoundRobinVolumeChoosingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/ReplicaOutputStreams.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/class-use/FsVolumeSpi.ScanInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.BlockIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeReference.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/LengthInputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaInputStreams.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/ReplicaOutputStreams.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorAggressive.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorConservative.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorAbsolute.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorAggressive.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorConservative.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorAbsolute.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/AddBlockPoolException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.ReservedSpaceCalculatorPercentage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsDatasetFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/ReservedSpaceCalculator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsVolumeImplBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/class-use/FsVolumeImpl.BlockDirFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/AddBlockPoolException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.ReservedSpaceCalculatorPercentage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReservedSpaceCalculator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImplBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.BlockDirFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsVolumeSpi.ScanInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/fsdataset/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.RegisteredShm.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/SecureDataNodeStarter.SecureResources.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.ScanInfoVolumeReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/DataNodeUGIProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/DataNodeUGIProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/class-use/WebHdfsHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/DatanodeHttpServer.MapBasedFilterConfig.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/class-use/DatanodeHttpServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.MapBasedFilterConfig.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/web/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.VolumePair.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/BPServiceActorAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReportBadBlockAction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/LocalReplica.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/ReplicaHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.BlockMover.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.ReportCompiler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/RemoteEditLog.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlocksStorageMoveAttemptFinished.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlocksWithLocations.StripedBlockWithLocations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BalancerBandwidthCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockStorageMovementCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/RemoteEditLogManifest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/VolumeFailureSummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/NamespaceInfo.Capability.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/ReceivedDeletedBlockInfo.BlockStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageReceivedDeletedBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/ReceivedDeletedBlockInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/FencedException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/StorageBlockReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockStorageMovementCommand.BlockMovingInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/protocol/class-use/BlockRecoveryCommand.RecoveringStripedBlock.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.BlockUCState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/TokenVerifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/FileRegion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.NamenodeRole.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.BlockUCState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/TokenVerifier.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/FileRegion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.NamenodeRole.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/MetricsLoggerTask.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HostRestrictingAuthorizationFilter.HttpInteraction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.ReplicaState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/BlockAlias.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.StartupOption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HostRestrictingAuthorizationFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.NodeType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/HdfsServerConstants.RollingUpgradeStartupOption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/class-use/Storage.StorageState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.HttpInteraction.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.ReplicaState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/BlockAlias.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.StartupOption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HostRestrictingAuthorizationFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.NodeType.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/HdfsServerConstants.RollingUpgradeStartupOption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Reader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Reader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.ImmutableIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/class-use/BlockAliasMap.Writer.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Reader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.ImmutableIterator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBWriter.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.WriterOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.ReaderOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBReader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBWriter.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.WriterOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.ReaderOptions.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBReader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextWriter.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/LevelDBFileRegionAliasMap.LevelDBReader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextReader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextReader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/class-use/TextFileRegionAliasMap.TextWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextWriter.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/LevelDBFileRegionAliasMap.LevelDBReader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextReader.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextReader.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TextFileRegionAliasMap.TextWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Writer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/BlockAliasMap.Writer.Options.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/blockaliasmap/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/Storage.StorageState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/common/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerCluster.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerDataNode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolume.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerCluster.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerDataNode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerVolume.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/class-use/DiskBalancerVolumeSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/DiskBalancerVolumeSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/datamodel/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/HelpCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/ExecuteCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/HelpCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/ExecuteCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/PlanCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/Command.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/QueryCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/ReportCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/class-use/CancelCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/ReportCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/CancelCommand.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/command/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/DiskBalancerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/class-use/DiskBalancerException.Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/MoveStep.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/MoveStep.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/GreedyPlanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/PlannerFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/NodePlan.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/Planner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/class-use/Step.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/GreedyPlanner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/PlannerFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/NodePlan.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/Planner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/Step.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/planner/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/ClusterConnector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/JsonNodeConnector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/class-use/ConnectorFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ClusterConnector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/JsonNodeConnector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/ConnectorFactory.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/connectors/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/diskbalancer/DiskBalancerException.Result.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DBlockStriped.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DDatanode.StorageGroup.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DBlockStriped.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DDatanode.StorageGroup.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/MovedBlocks.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.StorageGroupMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.Source.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DBlock.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Matcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.PendingMove.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/MovedBlocks.Locations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/ExitStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/class-use/Dispatcher.DDatanode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.StorageGroupMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.Source.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DBlock.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Matcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.PendingMove.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/MovedBlocks.Locations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/ExitStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/Dispatcher.DDatanode.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/balancer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.CheckedFunction2.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMap.CheckedFunction2.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/class-use/InMemoryAliasMapProtocol.IterationResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMap.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/InMemoryAliasMapProtocol.IterationResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/aliasmap/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminBackoffMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockIdManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStorageInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.StoredReplicaState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockStatsMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/UnresolvedTopologyException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementStatusDefault.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.CachedBlocksList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.LeavingServiceStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoStriped.StorageAndBlockIndex.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminBackoffMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockIdManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeStorageInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/NumberReplicas.StoredReplicaState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminDefaultMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockStatsMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/UnresolvedTopologyException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockPlacementStatusDefault.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.CachedBlocksList.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.LeavingServiceStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminMonitorInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockInfoStriped.StorageAndBlockIndex.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/HostFileManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeDescriptor.CachedBlocksList.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/ProvidedStorageMap.ProvidedDescriptor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/SlowDiskTracker.DiskLatency.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockManagerFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/AvailableSpaceBlockPlacementPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockUnderConstructionFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/CombinedHostFileManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockStoragePolicySuite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeAdminMonitorBase.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/HostSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/DatanodeStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/CorruptReplicasMap.Reason.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/BlockPlacementPolicyWithNodeGroup.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/NumberReplicas.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/class-use/SlowPeerTracker.ReportForJson.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.CachedBlocksList.Type.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.ProvidedDescriptor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/SlowDiskTracker.DiskLatency.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerFaultInjector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/CombinedHostFileManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminMonitorBase.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/HostSet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStatistics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.Reason.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/NumberReplicas.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/SlowPeerTracker.ReportForJson.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/server/blockmanagement/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/DFSUtil.ConfiguredNNAddress.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/WebHdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/DFSUtil.ServiceComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/DFSUtil.ConfiguredNNAddress.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/SWebHdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/class-use/HdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/NamenodeProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/DatanodeProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/ReconfigurationProtocolServerSideUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/class-use/PBHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/ReconfigurationProtocolServerSideUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/PBHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/class-use/BlockPoolTokenSecretManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/BlockPoolTokenSecretManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/block/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.SecretManagerState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/class-use/DelegationTokenSecretManager.SecretManagerState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/security/token/delegation/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/OfflineEditsViewer.Flags.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/class-use/TeeOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/OfflineEditsViewer.Flags.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TeeOutputStream.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineEditsViewer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/DiskBalancerCLI.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/DiskBalancerCLI.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/DFSHAAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/AdminHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/GetConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/class-use/StoragePolicyAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/DFSHAAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/AdminHelper.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/GetConf.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageDelimitedTextWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/IgnoreSnapshotException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageDelimitedTextWriter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/IgnoreSnapshotException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/WebImageViewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageCorruptionDetector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/XmlImageVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/class-use/PBImageCorruption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/WebImageViewer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruptionDetector.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/XmlImageVisitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageCorruption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/offlineImageViewer/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/StoragePolicyAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/snapshot/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/tools/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/SWebHdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/AuthFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/AuthFilterInitializer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/ParamFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/AuthFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/AuthFilterInitializer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/ParamFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/class-use/JsonUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/JsonUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/TokenServiceParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/ExceptionHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/TokenKindParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenServiceParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/ExceptionHandler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/TokenKindParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UserProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/UriFsPathParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/class-use/NamenodeAddressParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/UserProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/UriFsPathParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/NamenodeAddressParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/resources/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/web/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosData.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosData.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.SegmentStateProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.StartLogSegmentRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosDataOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FormatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.RequestInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.HeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.AcceptRecoveryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournaledEditsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DiscardSegmentsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetEditLogManifestRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.IsFormattedResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoRollbackResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PersistedRecoveryPaxosData.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoFinalizeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/InterQJournalProtocolProtos.InterQJournalProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PrepareRecoveryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoPreUpgradeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.NewEpochResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.PurgeLogsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.CanRollBackRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalCTimeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.QJournalProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.GetJournalStateRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.JournalIdProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/class-use/QJournalProtocolProtos.DoUpgradeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.SegmentStateProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.StartLogSegmentRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosDataOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FormatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.RequestInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.HeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.AcceptRecoveryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournaledEditsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DiscardSegmentsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.FinalizeLogSegmentRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetEditLogManifestRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.IsFormattedResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoRollbackResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PersistedRecoveryPaxosData.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoFinalizeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/InterQJournalProtocolProtos.InterQJournalProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PrepareRecoveryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoPreUpgradeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.NewEpochResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.PurgeLogsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.CanRollBackRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalCTimeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.QJournalProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.GetJournalStateRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.JournalIdProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocol/QJournalProtocolProtos.DoUpgradeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/JournalNodeMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/JournalNodeMXBean.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/class-use/Journal.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/Journal.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/server/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/qjournal/client/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/HdfsDtFetcher.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/org/apache/hadoop/hdfs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/overview-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/allclasses-noframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/build/source/hadoop-hdfs-project/hadoop-hdfs/target/api/package-list\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/LibHdfs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/HDFSDiskbalancer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/HDFSRouterFederation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/hdfs-rbf-default.xml\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/configuration.xsl\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/routerfederation.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/serialized-form.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/index.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/stylesheet.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/index-all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/constant-values.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/help-doc.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/deprecated-list.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/allclasses-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/script.js\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/class-use/RouterProtocolProtos.RouterAdminProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.Interface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.Stub.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.BlockingInterface.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/RouterProtocolProtos.RouterAdminProtocolService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocol/proto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterHttpServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterStateManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUsage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUpdateService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterCacheAdmin.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/MountTableRefresherService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/PeriodicService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionNullException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/MountTableRefresherThread.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteLocationContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/Quota.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/FederationUtil.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterNamenodeProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NoNamenodesAvailableException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NameserviceManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterClientProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/IsRouterActiveServlet.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterWebHdfsMethods.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteMethod.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterAdminServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterPermissionChecker.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/SubClusterTimeoutException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/DFSRouter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcClient.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterServiceState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterHeartbeatService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/NamenodeHeartbeatService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RemoteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ErasureCoding.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterMetricsService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/ConnectionManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterUserProtocol.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterSafemodeService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterQuotaUsage.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/class-use/RouterRpcServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/class-use/RouterSecurityManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/router/security/token/class-use/ZKDelegationTokenSecretManagerImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/NamenodeBeanMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/FederationRPCPerformanceMonitor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/NullStateStoreMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/RBFMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/StateStoreMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/metrics/class-use/FederationRPCMetrics.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamespaceInfoResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/LeaveSafeModeRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamenodeRegistrationsRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnableNameserviceResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateNamenodeRegistrationRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshSuperUserGroupsConfigurationResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnableNameserviceRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDestinationResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RemoveMountTableEntryRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationsRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshSuperUserGroupsConfigurationRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetMountTableEntriesRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/DisableNameserviceRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDestinationRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetSafeModeResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RemoveMountTableEntryResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshMountTableEntriesRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnterSafeModeRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateMountTableEntryResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/LeaveSafeModeResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetSafeModeRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateMountTableEntryRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/NamenodeHeartbeatResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RouterHeartbeatRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationsResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RefreshMountTableEntriesResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamenodeRegistrationsResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetMountTableEntriesResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDisabledNameservicesResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/DisableNameserviceResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/NamenodeHeartbeatRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/EnterSafeModeResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetDisabledNameservicesRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/UpdateNamenodeRegistrationResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/AddMountTableEntryResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/RouterHeartbeatResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/AddMountTableEntryRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetNamespaceInfoRequest.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/class-use/GetRouterRegistrationResponse.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamenodeRegistrationsRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDestinationRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDisabledNameservicesRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/LeaveSafeModeResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnableNameserviceResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnterSafeModeResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamespaceInfoResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/FederationProtocolPBTranslator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RouterHeartbeatResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationsRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateNamenodeRegistrationRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/NamenodeHeartbeatResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDisabledNameservicesResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshSuperUserGroupsConfigurationRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshMountTableEntriesResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetSafeModeRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/NamenodeHeartbeatRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RouterHeartbeatRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateNamenodeRegistrationResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamenodeRegistrationsResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/DisableNameserviceResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetSafeModeResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetMountTableEntriesResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshSuperUserGroupsConfigurationResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnableNameserviceRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/AddMountTableEntryResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/EnterSafeModeRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RefreshMountTableEntriesRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateMountTableEntryResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetRouterRegistrationsResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/DisableNameserviceRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RemoveMountTableEntryRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetMountTableEntriesRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/RemoveMountTableEntryResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/AddMountTableEntryRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/UpdateMountTableEntryRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetDestinationResponsePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/GetNamespaceInfoRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/protocol/impl/pb/class-use/LeaveSafeModeRequestPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/CachedRecordStore.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreConnectionMonitorService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreUnavailableException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreCacheUpdateService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/class-use/StateStoreCache.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/StateStoreSerializer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/class-use/StateStoreDriver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreZooKeeperImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileSystemImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreBaseImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreSerializableImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreFileBaseImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/driver/impl/class-use/StateStoreSerializerPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/QueryResult.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/StateStoreVersion.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MountTable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MembershipState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/BaseRecord.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/DisabledNameservice.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/Query.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/RouterState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/class-use/MembershipStats.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/StateStoreVersionPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MembershipStatsPBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/DisabledNameservicePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/PBRecord.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MembershipStatePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/MountTablePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/store/records/impl/pb/class-use/RouterStatePBImpl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/NamenodePriorityComparator.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/PathLocation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RouterResolveException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamespaceInfo.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MembershipNamenodeResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RouterGenericManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/RemoteLocation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MountTableResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/NamenodeStatusReport.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamenodeContext.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MountTableManager.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/FederationNamenodeServiceState.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/class-use/MultipleDestinationMountTableResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/HashFirstResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/AvailableSpaceResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/DestinationOrder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/OrderedResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/RouterResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/LocalResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/HashResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/resolver/order/class-use/RandomResolver.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/server/federation/utils/class-use/ConsistentHashRing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/protocolPB/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshMountTableEntriesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisableNameserviceRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnterSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.DestOrder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.LeaveSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoveMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeHeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.MountTableRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.AddMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDestinationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RemoteLocationProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterHeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.RouterRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/class-use/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.StateStoreVersionRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisableNameserviceRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.FederationNamespaceInfoProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnterSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.DestOrder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.LeaveSafeModeResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetSafeModeRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetExpiredRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoveMountTableEntryResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.EnableNameserviceResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeHeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.DisabledNameserviceRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.MountTableRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetMountTableEntriesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.AddMountTableEntryRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipStatsRecordProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDestinationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RefreshSuperUserGroupsConfigurationRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamenodeRegistrationsRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatResponseProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.UpdateNamenodeRegistrationRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetDisabledNameservicesRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.NamenodeMembershipRecordProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetNamespaceInfoResponseProto.Builder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RemoteLocationProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterHeartbeatRequestProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.RouterRecordProtoOrBuilder.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/federation/protocol/proto/HdfsServerFederationProtos.GetRouterRegistrationsRequestProto.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/org/apache/hadoop/hdfs/tools/federation/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/overview-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/allclasses-noframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/build/source/hadoop-hdfs-project/hadoop-hdfs-rbf/target/api/package-list\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-hdfs-rbf/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-tools-dist/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-dynamometer-dist/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/hadoop-client/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-project-dist/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-nfs/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-runtime/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-client-modules/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-annotations/dependency-analysis.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/index.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/expanded.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/banner.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_warning_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/collapsed.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logo_apache.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/newwindow.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/h5.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/apache-maven-project-2.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/external.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/h3.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_success_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/maven-logo-2.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logo_maven.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/breadcrumbs.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/build-by-maven-white.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/maven-feather.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/logos/build-by-maven-black.png\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_info_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/bg.jpg\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/images/icon_error_sml.gif\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/project-reports.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/ServerSetup.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/UsingHttpTools.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/maven-base.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/maven-theme.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/site.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/css/print.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/serialized-form.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/index.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/stylesheet.css\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/index-all.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/constant-values.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/help-doc.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/deprecated-list.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/allclasses-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/script.js\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDeleteSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetAllStoragePolicies.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSCreateSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSExceptionProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetReplication.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.SourcesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.SnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServerWebServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetXAttr.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrValueParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.StartAfterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ReplicationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ECPolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetSnapshotDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OperationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.UnmaskedPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OldSnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/CheckUploadContentTypeFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.ModifiedTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSatisyStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.NoRedirectParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDeleteSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetAllStoragePolicies.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSCreateSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSExceptionProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetReplication.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.SourcesParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.SnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServerWebServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetXAttr.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrValueParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.StartAfterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ReplicationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ECPolicyParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetSnapshotDiff.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OperationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.UnmaskedPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OldSnapshotNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/CheckUploadContentTypeFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.ModifiedTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSatisyStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.NoRedirectParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.GroupParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRename.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetPermission.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.BlockSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListXAttrs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OverwriteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServerWebApp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSFileStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.FsActionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSOpen.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAclStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.FilterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetOwner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetServerDefaults.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.RecursiveParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAppend.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSFileChecksum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSReleaseFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrSetFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAllowSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.NewLengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSAccess.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OffsetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRenameSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSUnSetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSQuotaUsage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSModifyAclEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.DataParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDelete.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSHomeDir.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetTimes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSSetAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.AccessTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveDefaultAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.PolicyNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSTruncate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.PermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSCreate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSTrashRoot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSAuthenticationFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveAclEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSConcat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.OwnerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSDisallowSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSListStatusBatch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.DestinationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetXAttrs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSRemoveXAttr.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.LenParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.AclPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSContentSummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSMkdirs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSGetSnapshottableDirListing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/HttpFSParametersProvider.XAttrEncodingParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/class-use/FSOperations.FSUnsetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.GroupParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRename.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetPermission.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.BlockSizeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListXAttrs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OverwriteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServerWebApp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSServer.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSFileStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.FsActionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSOpen.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAclStatus.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.FilterParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetOwner.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetServerDefaults.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.RecursiveParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAppend.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSFileChecksum.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSReleaseFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrSetFlagParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAllowSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.NewLengthParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSAccess.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OffsetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRenameSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSUnSetErasureCodingPolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSQuotaUsage.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSModifyAclEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.DataParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDelete.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSHomeDir.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetTimes.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSSetAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.AccessTimeParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveDefaultAcl.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.PolicyNameParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSTruncate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.PermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSCreate.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSTrashRoot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSAuthenticationFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveAclEntries.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSConcat.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.OwnerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSDisallowSnapshot.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSListStatusBatch.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.DestinationParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetXAttrs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSRemoveXAttr.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.LenParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.AclPermissionParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSContentSummary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSMkdirs.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSGetSnapshottableDirListing.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/HttpFSParametersProvider.XAttrEncodingParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/server/FSOperations.FSUnsetStoragePolicy.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpsFSFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.Operation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/class-use/HttpFSFileSystem.FILE_TYPE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpsFSFileSystem.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.Operation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/HttpFSFileSystem.FILE_TYPE.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/fs/http/client/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/RunnableCallable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/RunnableCallable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/XException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/class-use/XException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/XException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/XException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/lang/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/ConfigurationUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/Check.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/ConfigurationUtils.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/class-use/Check.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/util/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Server.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Service.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/Server.Status.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/BaseService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServerException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/class-use/ServiceException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Service.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServerException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/Server.Status.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/BaseService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServerException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/ServiceException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/server/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/MDCFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/ServerWebApp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/HostnameFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/MDCFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/ServerWebApp.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/HostnameFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/class-use/FileSystemReleaseFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/FileSystemReleaseFilter.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/servlet/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Scheduler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/SchedulerService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/class-use/SchedulerService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/scheduler/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/class-use/InstrumentationService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/InstrumentationService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/instrumentation/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Groups.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccess.FileSystemExecutor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Scheduler.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Groups.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccess.FileSystemExecutor.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.Cron.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccessException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccessException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/Instrumentation.Variable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/class-use/FileSystemAccess.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.Cron.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/class-use/GroupsService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/GroupsService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/security/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/class-use/FileSystemAccessService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/FileSystemAccessService.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/hadoop/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccessException.ERROR.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccessException.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/Instrumentation.Variable.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/FileSystemAccess.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/service/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/EnumSetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/Parameters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/BooleanParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ShortParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/StringParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/JSONMapProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ExceptionProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/IntegerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/Param.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/EnumSetParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/Parameters.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/BooleanParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ShortParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/StringParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/JSONMapProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ExceptionProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/IntegerParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/Param.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/LongParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ParametersProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/JSONProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/EnumParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/ByteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/class-use/InputStreamEntity.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/LongParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ParametersProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/JSONProvider.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/EnumParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/ByteParam.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/InputStreamEntity.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-use.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/org/apache/hadoop/lib/wsrs/package-tree.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-summary.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/overview-frame.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/allclasses-noframe.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/apidocs/package-list\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/httpfs-default.html\n",
            "hadoop-3.3.0/share/doc/hadoop/hadoop-hdfs-httpfs/dependency-analysis.html\n",
            "hadoop-3.3.0/lib/\n",
            "hadoop-3.3.0/lib/native/\n",
            "hadoop-3.3.0/lib/native/libhadoop.a\n",
            "hadoop-3.3.0/lib/native/libhadooputils.a\n",
            "hadoop-3.3.0/lib/native/libhadoop.so.1.0.0\n",
            "hadoop-3.3.0/lib/native/examples/\n",
            "hadoop-3.3.0/lib/native/examples/wordcount-simple\n",
            "hadoop-3.3.0/lib/native/examples/pipes-sort\n",
            "hadoop-3.3.0/lib/native/examples/wordcount-nopipe\n",
            "hadoop-3.3.0/lib/native/examples/wordcount-part\n",
            "hadoop-3.3.0/lib/native/libnativetask.so\n",
            "hadoop-3.3.0/lib/native/libhadoop.so\n",
            "hadoop-3.3.0/lib/native/libhadooppipes.a\n",
            "hadoop-3.3.0/lib/native/libnativetask.so.1.0.0\n",
            "hadoop-3.3.0/lib/native/libhdfs.so.0.0.0\n",
            "hadoop-3.3.0/lib/native/libhdfs.a\n",
            "hadoop-3.3.0/lib/native/libhdfs.so\n",
            "hadoop-3.3.0/lib/native/libnativetask.a\n",
            "hadoop-3.3.0/etc/\n",
            "hadoop-3.3.0/etc/hadoop/\n",
            "hadoop-3.3.0/etc/hadoop/hadoop-env.cmd\n",
            "hadoop-3.3.0/etc/hadoop/workers\n",
            "hadoop-3.3.0/etc/hadoop/log4j.properties\n",
            "hadoop-3.3.0/etc/hadoop/yarnservice-log4j.properties\n",
            "hadoop-3.3.0/etc/hadoop/hadoop-policy.xml\n",
            "hadoop-3.3.0/etc/hadoop/kms-acls.xml\n",
            "hadoop-3.3.0/etc/hadoop/hdfs-rbf-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/configuration.xsl\n",
            "hadoop-3.3.0/etc/hadoop/container-executor.cfg\n",
            "hadoop-3.3.0/etc/hadoop/ssl-client.xml.example\n",
            "hadoop-3.3.0/etc/hadoop/httpfs-env.sh\n",
            "hadoop-3.3.0/etc/hadoop/hadoop-user-functions.sh.example\n",
            "hadoop-3.3.0/etc/hadoop/ssl-server.xml.example\n",
            "hadoop-3.3.0/etc/hadoop/shellprofile.d/\n",
            "hadoop-3.3.0/etc/hadoop/shellprofile.d/example.sh\n",
            "hadoop-3.3.0/etc/hadoop/hdfs-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/kms-env.sh\n",
            "hadoop-3.3.0/etc/hadoop/core-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/hadoop-env.sh\n",
            "hadoop-3.3.0/etc/hadoop/mapred-queues.xml.template\n",
            "hadoop-3.3.0/etc/hadoop/yarn-env.cmd\n",
            "hadoop-3.3.0/etc/hadoop/mapred-env.cmd\n",
            "hadoop-3.3.0/etc/hadoop/yarn-env.sh\n",
            "hadoop-3.3.0/etc/hadoop/httpfs-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/kms-log4j.properties\n",
            "hadoop-3.3.0/etc/hadoop/mapred-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/yarn-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/capacity-scheduler.xml\n",
            "hadoop-3.3.0/etc/hadoop/hadoop-metrics2.properties\n",
            "hadoop-3.3.0/etc/hadoop/user_ec_policies.xml.template\n",
            "hadoop-3.3.0/etc/hadoop/mapred-env.sh\n",
            "hadoop-3.3.0/etc/hadoop/kms-site.xml\n",
            "hadoop-3.3.0/etc/hadoop/httpfs-log4j.properties\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUI1SBy6yyMg"
      },
      "source": [
        "#copy  hadoop file to user/local\n",
        "!cp -r hadoop-3.3.0/ /usr/local/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzORlewJ6qRi"
      },
      "source": [
        "## Configuring Hadoop's Java Home and setting environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe1px84Ey2pX",
        "outputId": "a7fe5cec-ee81-4ed5-fc9c-03772a1ddbf6"
      },
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKam0zQIzUd4"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQFdqexy6l1q"
      },
      "source": [
        "## Running hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGDy2AQ_zXrH",
        "outputId": "4a8928ea-a340-4d7c-b022-a0c2f54bca08"
      },
      "source": [
        "!/usr/local/hadoop-3.3.0/bin/hadoop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9084E8vzFRvw",
        "outputId": "0da66273-0a69-4277-eb2f-5bfafa247efc"
      },
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpcTxuAEzr4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11728a4d-6be7-41e8-fbba-4e813f1c0f21"
      },
      "source": [
        "!mkdir ~/input\n",
        "!cp /usr/local/hadoop-3.3.0/etc/hadoop/*.xml ~/input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/input’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T3r4bAPzvPn",
        "outputId": "eba337da-6ac5-49f5-b7f6-c92b20178f42"
      },
      "source": [
        "!ls ~/input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "capacity-scheduler.xml\thdfs-rbf-site.xml  kms-acls.xml     yarn-site.xml\n",
            "core-site.xml\t\thdfs-site.xml\t   kms-site.xml\n",
            "hadoop-policy.xml\thttpfs-site.xml    mapred-site.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl66LdGezzs3"
      },
      "source": [
        "#!/usr/local/hadoop-3.3.0/bin/hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep ~/input ~/grep_example 'allowed[.]*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiRofg7ez8In",
        "outputId": "24a697c9-151a-4edc-a65f-8d7079cfaae7"
      },
      "source": [
        "!find / -name 'hadoop-streaming*.jar'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-sources.jar\n",
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-test-sources.jar\n",
            "/usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-sources.jar\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/sources/hadoop-streaming-3.3.0-test-sources.jar\n",
            "/content/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so51zAhZU1pa"
      },
      "source": [
        "## To run model.sh file "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5zylIf37NET"
      },
      "source": [
        "### For n=5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQuHOuuPiBgw",
        "outputId": "e3d0fc84-7c53-4365-b9d1-1a9f78c51f34"
      },
      "source": [
        "!sh model.sh   #to run shell file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count---------+1\n",
            "2021-05-07 06:40:21,766 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob5845468830867024026.jar tmpDir=null\n",
            "2021-05-07 06:40:22,641 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:40:22,829 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:40:22,830 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:40:22,853 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:23,017 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:40:23,040 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:40:23,350 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1322742916_0001\n",
            "2021-05-07 06:40:23,350 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:40:23,796 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1322742916_0001_d11eadca-3fef-4ecb-9bd2-b41a155323b5/centroids.txt\n",
            "2021-05-07 06:40:23,903 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:40:23,905 INFO mapreduce.Job: Running job: job_local1322742916_0001\n",
            "2021-05-07 06:40:23,911 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:40:23,913 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:40:23,918 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:23,918 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:23,964 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:40:23,968 INFO mapred.LocalJobRunner: Starting task: attempt_local1322742916_0001_m_000000_0\n",
            "2021-05-07 06:40:24,003 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:24,004 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:24,032 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:24,042 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:40:24,069 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:40:24,161 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:40:24,161 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:40:24,161 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:40:24,161 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:40:24,162 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:40:24,165 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:40:24,176 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:40:24,189 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:40:24,190 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:40:24,190 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:40:24,191 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:40:24,192 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:40:24,192 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:40:24,192 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:40:24,193 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:40:24,193 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:40:24,194 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:40:24,194 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:40:24,195 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:40:24,233 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,233 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,235 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,369 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:40:24,538 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:24,539 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:24,544 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:40:24,544 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:40:24,544 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:40:24,544 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:40:24,544 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:40:24,570 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:40:24,584 INFO mapred.Task: Task:attempt_local1322742916_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:24,587 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:40:24,587 INFO mapred.Task: Task 'attempt_local1322742916_0001_m_000000_0' done.\n",
            "2021-05-07 06:40:24,595 INFO mapred.Task: Final Counters for attempt_local1322742916_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658594\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:40:24,595 INFO mapred.LocalJobRunner: Finishing task: attempt_local1322742916_0001_m_000000_0\n",
            "2021-05-07 06:40:24,596 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:40:24,600 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:40:24,600 INFO mapred.LocalJobRunner: Starting task: attempt_local1322742916_0001_r_000000_0\n",
            "2021-05-07 06:40:24,616 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:24,616 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:24,616 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:24,619 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@188c0a2a\n",
            "2021-05-07 06:40:24,621 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:24,643 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:40:24,645 INFO reduce.EventFetcher: attempt_local1322742916_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:40:24,707 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1322742916_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:40:24,716 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1322742916_0001_m_000000_0\n",
            "2021-05-07 06:40:24,718 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:40:24,721 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:40:24,722 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:24,722 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:40:24,730 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:24,730 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:24,759 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:40:24,760 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:40:24,763 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:40:24,763 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:24,765 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:24,766 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:24,788 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:40:24,793 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:40:24,794 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:40:24,821 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,821 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,822 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,831 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:24,909 INFO mapreduce.Job: Job job_local1322742916_0001 running in uber mode : false\n",
            "2021-05-07 06:40:24,911 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:40:25,088 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:40:25,106 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:25,107 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:25,108 INFO mapred.Task: Task:attempt_local1322742916_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:25,109 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:25,109 INFO mapred.Task: Task attempt_local1322742916_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:40:25,111 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1322742916_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:40:25,117 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:40:25,117 INFO mapred.Task: Task 'attempt_local1322742916_0001_r_000000_0' done.\n",
            "2021-05-07 06:40:25,118 INFO mapred.Task: Final Counters for attempt_local1322742916_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:25,120 INFO mapred.LocalJobRunner: Finishing task: attempt_local1322742916_0001_r_000000_0\n",
            "2021-05-07 06:40:25,120 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:40:25,912 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:40:25,913 INFO mapreduce.Job: Job job_local1322742916_0001 completed successfully\n",
            "2021-05-07 06:40:25,925 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360249\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=708837376\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:25,925 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:40:29,675 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+2\n",
            "2021-05-07 06:40:30,941 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16630856104381883182.jar tmpDir=null\n",
            "2021-05-07 06:40:31,741 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:40:31,901 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:40:31,902 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:40:31,931 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:32,127 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:40:32,166 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:40:32,519 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local561734953_0001\n",
            "2021-05-07 06:40:32,519 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:40:32,966 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local561734953_0001_d5d2d2c2-3796-4cb7-bfcb-a25d5b93c01e/centroids.txt\n",
            "2021-05-07 06:40:33,057 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:40:33,058 INFO mapreduce.Job: Running job: job_local561734953_0001\n",
            "2021-05-07 06:40:33,065 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:40:33,067 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:40:33,072 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:33,072 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:33,117 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:40:33,121 INFO mapred.LocalJobRunner: Starting task: attempt_local561734953_0001_m_000000_0\n",
            "2021-05-07 06:40:33,150 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:33,151 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:33,173 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:33,182 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:40:33,200 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:40:33,273 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:40:33,274 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:40:33,274 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:40:33,274 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:40:33,274 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:40:33,277 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:40:33,297 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:40:33,306 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:40:33,307 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:40:33,307 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:40:33,308 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:40:33,308 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:40:33,308 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:40:33,309 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:40:33,309 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:40:33,309 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:40:33,310 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:40:33,310 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:40:33,311 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:40:33,343 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:33,343 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:33,345 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:33,491 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:40:33,654 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:33,655 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:33,659 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:40:33,659 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:40:33,659 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:40:33,659 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:40:33,659 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:40:33,685 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:40:33,699 INFO mapred.Task: Task:attempt_local561734953_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:33,702 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:40:33,703 INFO mapred.Task: Task 'attempt_local561734953_0001_m_000000_0' done.\n",
            "2021-05-07 06:40:33,715 INFO mapred.Task: Final Counters for attempt_local561734953_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655613\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:40:33,716 INFO mapred.LocalJobRunner: Finishing task: attempt_local561734953_0001_m_000000_0\n",
            "2021-05-07 06:40:33,716 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:40:33,724 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:40:33,725 INFO mapred.LocalJobRunner: Starting task: attempt_local561734953_0001_r_000000_0\n",
            "2021-05-07 06:40:33,741 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:33,741 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:33,742 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:33,747 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1359f6f6\n",
            "2021-05-07 06:40:33,749 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:33,779 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:40:33,793 INFO reduce.EventFetcher: attempt_local561734953_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:40:33,843 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local561734953_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:40:33,847 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local561734953_0001_m_000000_0\n",
            "2021-05-07 06:40:33,852 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:40:33,855 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:40:33,857 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:33,857 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:40:33,865 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:33,865 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:33,884 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:40:33,885 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:40:33,886 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:40:33,886 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:33,902 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:33,902 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:33,919 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:40:33,927 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:40:33,945 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:40:33,977 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:33,977 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:33,981 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:34,004 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:34,063 INFO mapreduce.Job: Job job_local561734953_0001 running in uber mode : false\n",
            "2021-05-07 06:40:34,065 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:40:34,224 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:40:34,246 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:34,246 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:34,247 INFO mapred.Task: Task:attempt_local561734953_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:34,251 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:34,252 INFO mapred.Task: Task attempt_local561734953_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:40:34,254 INFO output.FileOutputCommitter: Saved output of task 'attempt_local561734953_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:40:34,259 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:40:34,260 INFO mapred.Task: Task 'attempt_local561734953_0001_r_000000_0' done.\n",
            "2021-05-07 06:40:34,260 INFO mapred.Task: Final Counters for attempt_local561734953_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698674\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:34,261 INFO mapred.LocalJobRunner: Finishing task: attempt_local561734953_0001_r_000000_0\n",
            "2021-05-07 06:40:34,261 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:40:35,067 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:40:35,067 INFO mapreduce.Job: Job job_local561734953_0001 completed successfully\n",
            "2021-05-07 06:40:35,080 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=702545920\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:35,081 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:40:38,828 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+3\n",
            "2021-05-07 06:40:40,030 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob1809096876462785716.jar tmpDir=null\n",
            "2021-05-07 06:40:40,884 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:40:41,048 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:40:41,049 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:40:41,073 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:41,211 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:40:41,233 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:40:41,574 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local60624455_0001\n",
            "2021-05-07 06:40:41,575 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:40:42,011 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local60624455_0001_05f5cfa6-258b-4c2b-82bb-b795ffb6eadc/centroids.txt\n",
            "2021-05-07 06:40:42,121 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:40:42,123 INFO mapreduce.Job: Running job: job_local60624455_0001\n",
            "2021-05-07 06:40:42,130 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:40:42,132 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:40:42,137 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:42,137 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:42,193 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:40:42,200 INFO mapred.LocalJobRunner: Starting task: attempt_local60624455_0001_m_000000_0\n",
            "2021-05-07 06:40:42,238 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:42,240 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:42,275 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:42,301 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:40:42,332 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:40:42,402 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:40:42,402 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:40:42,402 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:40:42,402 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:40:42,403 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:40:42,406 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:40:42,415 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:40:42,431 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:40:42,431 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:40:42,434 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:40:42,435 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:40:42,435 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:40:42,436 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:40:42,437 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:40:42,437 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:40:42,437 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:40:42,438 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:40:42,438 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:40:42,439 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:40:42,476 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:42,477 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:42,478 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:42,614 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:40:42,789 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:42,789 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:42,792 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:40:42,792 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:40:42,792 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:40:42,792 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:40:42,792 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:40:42,822 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:40:42,838 INFO mapred.Task: Task:attempt_local60624455_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:42,842 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:40:42,844 INFO mapred.Task: Task 'attempt_local60624455_0001_m_000000_0' done.\n",
            "2021-05-07 06:40:42,856 INFO mapred.Task: Final Counters for attempt_local60624455_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=652628\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:40:42,857 INFO mapred.LocalJobRunner: Finishing task: attempt_local60624455_0001_m_000000_0\n",
            "2021-05-07 06:40:42,857 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:40:42,863 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:40:42,864 INFO mapred.LocalJobRunner: Starting task: attempt_local60624455_0001_r_000000_0\n",
            "2021-05-07 06:40:42,875 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:42,877 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:42,877 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:42,880 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f4de466\n",
            "2021-05-07 06:40:42,899 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:42,923 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:40:42,936 INFO reduce.EventFetcher: attempt_local60624455_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:40:42,976 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local60624455_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:40:42,992 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local60624455_0001_m_000000_0\n",
            "2021-05-07 06:40:42,993 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:40:42,999 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:40:43,000 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:43,000 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:40:43,008 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:43,009 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:43,034 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:40:43,035 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:40:43,036 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:40:43,036 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:43,037 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:43,038 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:43,049 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:40:43,055 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:40:43,057 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:40:43,089 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:43,089 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:43,090 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:43,107 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:43,128 INFO mapreduce.Job: Job job_local60624455_0001 running in uber mode : false\n",
            "2021-05-07 06:40:43,130 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:40:43,360 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:40:43,376 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:43,377 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:43,378 INFO mapred.Task: Task:attempt_local60624455_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:43,379 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:43,380 INFO mapred.Task: Task attempt_local60624455_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:40:43,383 INFO output.FileOutputCommitter: Saved output of task 'attempt_local60624455_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:40:43,390 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:40:43,390 INFO mapred.Task: Task 'attempt_local60624455_0001_r_000000_0' done.\n",
            "2021-05-07 06:40:43,391 INFO mapred.Task: Final Counters for attempt_local60624455_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=695689\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:43,391 INFO mapred.LocalJobRunner: Finishing task: attempt_local60624455_0001_r_000000_0\n",
            "2021-05-07 06:40:43,391 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:40:44,132 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:40:44,132 INFO mapreduce.Job: Job job_local60624455_0001 completed successfully\n",
            "2021-05-07 06:40:44,145 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1348317\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=662700032\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:44,145 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:40:47,893 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+4\n",
            "2021-05-07 06:40:49,202 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob1757636555049237010.jar tmpDir=null\n",
            "2021-05-07 06:40:50,061 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:40:50,251 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:40:50,251 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:40:50,273 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:50,410 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:40:50,432 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:40:50,769 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local254112167_0001\n",
            "2021-05-07 06:40:50,769 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:40:51,186 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local254112167_0001_6365135e-bbf4-4cc4-b735-1d713e321834/centroids.txt\n",
            "2021-05-07 06:40:51,309 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:40:51,311 INFO mapreduce.Job: Running job: job_local254112167_0001\n",
            "2021-05-07 06:40:51,318 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:40:51,321 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:40:51,332 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:51,332 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:51,394 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:40:51,397 INFO mapred.LocalJobRunner: Starting task: attempt_local254112167_0001_m_000000_0\n",
            "2021-05-07 06:40:51,428 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:51,431 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:51,460 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:51,473 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:40:51,490 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:40:51,574 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:40:51,574 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:40:51,574 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:40:51,574 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:40:51,574 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:40:51,579 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:40:51,592 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:40:51,607 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:40:51,607 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:40:51,608 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:40:51,608 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:40:51,609 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:40:51,610 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:40:51,611 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:40:51,611 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:40:51,611 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:40:51,612 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:40:51,612 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:40:51,613 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:40:51,641 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:51,641 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:51,645 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:51,795 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:40:51,982 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:51,983 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:51,986 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:40:51,986 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:40:51,986 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:40:51,986 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:40:51,986 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:40:52,012 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:40:52,033 INFO mapred.Task: Task:attempt_local254112167_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:52,041 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:40:52,042 INFO mapred.Task: Task 'attempt_local254112167_0001_m_000000_0' done.\n",
            "2021-05-07 06:40:52,052 INFO mapred.Task: Final Counters for attempt_local254112167_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655611\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=337641472\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:40:52,052 INFO mapred.LocalJobRunner: Finishing task: attempt_local254112167_0001_m_000000_0\n",
            "2021-05-07 06:40:52,052 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:40:52,056 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:40:52,056 INFO mapred.LocalJobRunner: Starting task: attempt_local254112167_0001_r_000000_0\n",
            "2021-05-07 06:40:52,063 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:40:52,063 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:40:52,064 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:40:52,067 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@205939cb\n",
            "2021-05-07 06:40:52,068 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:52,091 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:40:52,094 INFO reduce.EventFetcher: attempt_local254112167_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:40:52,133 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local254112167_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:40:52,137 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local254112167_0001_m_000000_0\n",
            "2021-05-07 06:40:52,141 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:40:52,144 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:40:52,151 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:52,151 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:40:52,158 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:52,159 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:52,170 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:40:52,171 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:40:52,173 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:40:52,174 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:40:52,178 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:40:52,179 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:52,195 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:40:52,202 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:40:52,205 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:40:52,233 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:52,233 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:52,234 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:52,251 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:40:52,317 INFO mapreduce.Job: Job job_local254112167_0001 running in uber mode : false\n",
            "2021-05-07 06:40:52,318 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:40:52,505 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:40:52,523 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:40:52,524 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:40:52,525 INFO mapred.Task: Task:attempt_local254112167_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:40:52,527 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:40:52,527 INFO mapred.Task: Task attempt_local254112167_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:40:52,529 INFO output.FileOutputCommitter: Saved output of task 'attempt_local254112167_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:40:52,531 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:40:52,531 INFO mapred.Task: Task 'attempt_local254112167_0001_r_000000_0' done.\n",
            "2021-05-07 06:40:52,532 INFO mapred.Task: Final Counters for attempt_local254112167_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698672\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=337641472\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:52,532 INFO mapred.LocalJobRunner: Finishing task: attempt_local254112167_0001_r_000000_0\n",
            "2021-05-07 06:40:52,532 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:40:53,320 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:40:53,320 INFO mapreduce.Job: Job job_local254112167_0001 completed successfully\n",
            "2021-05-07 06:40:53,331 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354283\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=675282944\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:40:53,332 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:40:56,916 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+5\n",
            "2021-05-07 06:40:58,285 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16818824416634016737.jar tmpDir=null\n",
            "2021-05-07 06:40:59,041 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:40:59,232 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:40:59,233 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:40:59,259 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:40:59,442 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:40:59,478 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:40:59,801 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2053998100_0001\n",
            "2021-05-07 06:40:59,801 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:00,210 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local2053998100_0001_c22909f7-81a9-4471-af10-fd6cc32cd991/centroids.txt\n",
            "2021-05-07 06:41:00,322 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:00,324 INFO mapreduce.Job: Running job: job_local2053998100_0001\n",
            "2021-05-07 06:41:00,330 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:00,332 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:00,337 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:00,337 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:00,388 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:00,392 INFO mapred.LocalJobRunner: Starting task: attempt_local2053998100_0001_m_000000_0\n",
            "2021-05-07 06:41:00,431 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:00,434 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:00,475 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:00,500 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:00,528 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:00,596 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:00,597 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:00,597 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:00,597 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:00,597 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:00,600 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:00,610 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:00,618 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:00,620 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:00,621 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:00,622 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:00,623 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:00,623 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:00,623 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:00,624 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:00,624 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:00,625 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:00,625 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:00,626 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:00,662 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:00,662 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:00,664 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:00,801 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:00,956 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:00,957 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:00,960 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:00,960 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:00,960 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:00,960 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:00,960 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:00,989 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:01,002 INFO mapred.Task: Task:attempt_local2053998100_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:01,005 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:01,005 INFO mapred.Task: Task 'attempt_local2053998100_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:01,014 INFO mapred.Task: Final Counters for attempt_local2053998100_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658596\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:01,014 INFO mapred.LocalJobRunner: Finishing task: attempt_local2053998100_0001_m_000000_0\n",
            "2021-05-07 06:41:01,014 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:01,018 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:01,024 INFO mapred.LocalJobRunner: Starting task: attempt_local2053998100_0001_r_000000_0\n",
            "2021-05-07 06:41:01,038 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:01,039 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:01,042 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:01,045 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7c884daf\n",
            "2021-05-07 06:41:01,049 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:01,078 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:01,099 INFO reduce.EventFetcher: attempt_local2053998100_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:01,172 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2053998100_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:01,182 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local2053998100_0001_m_000000_0\n",
            "2021-05-07 06:41:01,188 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:01,192 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:01,194 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:01,194 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:01,205 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:01,205 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:01,233 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:01,234 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:01,235 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:01,235 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:01,237 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:01,237 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:01,249 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:01,255 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:01,256 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:01,277 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:01,277 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:01,279 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:01,294 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:01,328 INFO mapreduce.Job: Job job_local2053998100_0001 running in uber mode : false\n",
            "2021-05-07 06:41:01,330 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:01,553 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:01,566 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:01,567 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:01,568 INFO mapred.Task: Task:attempt_local2053998100_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:01,569 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:01,570 INFO mapred.Task: Task attempt_local2053998100_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:01,572 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2053998100_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:01,576 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:01,576 INFO mapred.Task: Task 'attempt_local2053998100_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:01,577 INFO mapred.Task: Final Counters for attempt_local2053998100_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:01,581 INFO mapred.LocalJobRunner: Finishing task: attempt_local2053998100_0001_r_000000_0\n",
            "2021-05-07 06:41:01,581 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:02,331 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:02,332 INFO mapreduce.Job: Job job_local2053998100_0001 completed successfully\n",
            "2021-05-07 06:41:02,345 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360253\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=702545920\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:02,345 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:06,030 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+6\n",
            "2021-05-07 06:41:07,377 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob4044007671503685291.jar tmpDir=null\n",
            "2021-05-07 06:41:08,198 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:08,364 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:08,364 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:08,387 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:08,534 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:08,554 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:08,906 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1957517760_0001\n",
            "2021-05-07 06:41:08,906 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:09,369 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1957517760_0001_1bdcfb47-2d9b-4485-8318-60159bfb1add/centroids.txt\n",
            "2021-05-07 06:41:09,469 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:09,471 INFO mapreduce.Job: Running job: job_local1957517760_0001\n",
            "2021-05-07 06:41:09,478 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:09,480 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:09,485 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:09,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:09,540 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:09,544 INFO mapred.LocalJobRunner: Starting task: attempt_local1957517760_0001_m_000000_0\n",
            "2021-05-07 06:41:09,576 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:09,578 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:09,612 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:09,622 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:09,643 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:09,718 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:09,718 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:09,718 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:09,718 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:09,719 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:09,722 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:09,733 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:09,748 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:09,750 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:09,751 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:09,751 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:09,753 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:09,754 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:09,754 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:09,754 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:09,755 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:09,756 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:09,756 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:09,756 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:09,790 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:09,790 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:09,792 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:09,934 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:10,096 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:10,097 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:10,100 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:10,100 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:10,100 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:10,100 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:10,100 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:10,123 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:10,136 INFO mapred.Task: Task:attempt_local1957517760_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:10,139 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:10,139 INFO mapred.Task: Task 'attempt_local1957517760_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:10,151 INFO mapred.Task: Final Counters for attempt_local1957517760_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658594\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=360710144\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:10,151 INFO mapred.LocalJobRunner: Finishing task: attempt_local1957517760_0001_m_000000_0\n",
            "2021-05-07 06:41:10,152 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:10,156 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:10,156 INFO mapred.LocalJobRunner: Starting task: attempt_local1957517760_0001_r_000000_0\n",
            "2021-05-07 06:41:10,166 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:10,166 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:10,166 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:10,169 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@205939cb\n",
            "2021-05-07 06:41:10,174 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:10,202 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:10,217 INFO reduce.EventFetcher: attempt_local1957517760_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:10,262 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1957517760_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:10,274 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1957517760_0001_m_000000_0\n",
            "2021-05-07 06:41:10,276 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:10,281 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:10,282 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:10,282 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:10,292 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:10,292 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:10,316 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:10,318 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:10,319 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:10,319 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:10,320 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:10,321 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:10,330 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:10,336 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:10,338 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:10,376 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:10,376 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:10,377 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:10,384 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:10,476 INFO mapreduce.Job: Job job_local1957517760_0001 running in uber mode : false\n",
            "2021-05-07 06:41:10,477 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:10,635 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:10,654 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:10,655 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:10,656 INFO mapred.Task: Task:attempt_local1957517760_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:10,657 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:10,657 INFO mapred.Task: Task attempt_local1957517760_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:10,659 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1957517760_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:10,661 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:10,661 INFO mapred.Task: Task 'attempt_local1957517760_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:10,662 INFO mapred.Task: Final Counters for attempt_local1957517760_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=360710144\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:10,662 INFO mapred.LocalJobRunner: Finishing task: attempt_local1957517760_0001_r_000000_0\n",
            "2021-05-07 06:41:10,662 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:11,479 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:11,480 INFO mapreduce.Job: Job job_local1957517760_0001 completed successfully\n",
            "2021-05-07 06:41:11,491 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360249\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=721420288\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:11,491 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:15,032 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+7\n",
            "2021-05-07 06:41:16,327 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob17775819836451315786.jar tmpDir=null\n",
            "2021-05-07 06:41:17,228 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:17,401 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:17,401 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:17,424 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:17,559 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:17,580 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:17,887 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local893527761_0001\n",
            "2021-05-07 06:41:17,887 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:18,313 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local893527761_0001_ac6fca8c-240f-4b63-ad58-49a60ff49992/centroids.txt\n",
            "2021-05-07 06:41:18,443 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:18,445 INFO mapreduce.Job: Running job: job_local893527761_0001\n",
            "2021-05-07 06:41:18,452 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:18,454 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:18,460 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:18,460 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:18,537 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:18,543 INFO mapred.LocalJobRunner: Starting task: attempt_local893527761_0001_m_000000_0\n",
            "2021-05-07 06:41:18,607 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:18,608 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:18,639 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:18,653 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:18,678 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:18,757 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:18,757 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:18,757 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:18,757 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:18,757 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:18,761 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:18,781 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:18,793 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:18,799 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:18,800 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:18,800 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:18,801 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:18,801 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:18,801 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:18,802 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:18,802 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:18,803 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:18,806 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:18,807 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:18,846 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:18,846 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:18,848 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:18,999 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:19,158 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:19,159 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:19,162 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:19,162 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:19,162 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:19,162 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:19,162 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:19,193 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:19,207 INFO mapred.Task: Task:attempt_local893527761_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:19,211 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:19,211 INFO mapred.Task: Task 'attempt_local893527761_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:19,219 INFO mapred.Task: Final Counters for attempt_local893527761_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655613\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:19,219 INFO mapred.LocalJobRunner: Finishing task: attempt_local893527761_0001_m_000000_0\n",
            "2021-05-07 06:41:19,220 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:19,223 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:19,229 INFO mapred.LocalJobRunner: Starting task: attempt_local893527761_0001_r_000000_0\n",
            "2021-05-07 06:41:19,246 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:19,246 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:19,247 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:19,255 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c90e037\n",
            "2021-05-07 06:41:19,257 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:19,281 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:19,283 INFO reduce.EventFetcher: attempt_local893527761_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:19,344 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local893527761_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:19,352 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local893527761_0001_m_000000_0\n",
            "2021-05-07 06:41:19,357 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:19,362 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:19,364 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:19,364 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:19,371 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:19,371 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:19,393 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:19,394 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:19,395 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:19,395 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:19,396 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:19,397 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:19,415 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:19,421 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:19,423 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:19,449 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:19,449 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:19,451 INFO mapreduce.Job: Job job_local893527761_0001 running in uber mode : false\n",
            "2021-05-07 06:41:19,452 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:19,453 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:19,460 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:19,747 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:19,767 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:19,768 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:19,769 INFO mapred.Task: Task:attempt_local893527761_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:19,771 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:19,772 INFO mapred.Task: Task attempt_local893527761_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:19,774 INFO output.FileOutputCommitter: Saved output of task 'attempt_local893527761_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:19,782 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:19,782 INFO mapred.Task: Task 'attempt_local893527761_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:19,783 INFO mapred.Task: Final Counters for attempt_local893527761_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698674\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:19,783 INFO mapred.LocalJobRunner: Finishing task: attempt_local893527761_0001_r_000000_0\n",
            "2021-05-07 06:41:19,783 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:20,453 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:20,454 INFO mapreduce.Job: Job job_local893527761_0001 completed successfully\n",
            "2021-05-07 06:41:20,465 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=662700032\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:20,465 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:23,837 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+8\n",
            "2021-05-07 06:41:25,155 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob10338564050234746539.jar tmpDir=null\n",
            "2021-05-07 06:41:25,966 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:26,148 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:26,148 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:26,171 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:26,311 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:26,338 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:26,686 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local331198806_0001\n",
            "2021-05-07 06:41:26,686 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:27,127 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local331198806_0001_1a56c6b8-9e33-4f69-aa23-c4eb6f2ab6c1/centroids.txt\n",
            "2021-05-07 06:41:27,226 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:27,228 INFO mapreduce.Job: Running job: job_local331198806_0001\n",
            "2021-05-07 06:41:27,234 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:27,237 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:27,242 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:27,242 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:27,312 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:27,319 INFO mapred.LocalJobRunner: Starting task: attempt_local331198806_0001_m_000000_0\n",
            "2021-05-07 06:41:27,364 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:27,367 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:27,404 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:27,414 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:27,442 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:27,523 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:27,523 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:27,523 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:27,524 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:27,524 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:27,528 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:27,547 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:27,561 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:27,566 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:27,567 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:27,567 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:27,568 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:27,568 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:27,569 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:27,569 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:27,569 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:27,571 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:27,571 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:27,571 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:27,600 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:27,601 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:27,603 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:27,773 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:27,935 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:27,938 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:27,942 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:27,942 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:27,942 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:27,942 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:27,942 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:27,969 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:27,982 INFO mapred.Task: Task:attempt_local331198806_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:27,985 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:27,986 INFO mapred.Task: Task 'attempt_local331198806_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:27,995 INFO mapred.Task: Final Counters for attempt_local331198806_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655613\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:27,995 INFO mapred.LocalJobRunner: Finishing task: attempt_local331198806_0001_m_000000_0\n",
            "2021-05-07 06:41:27,995 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:27,999 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:28,001 INFO mapred.LocalJobRunner: Starting task: attempt_local331198806_0001_r_000000_0\n",
            "2021-05-07 06:41:28,020 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:28,021 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:28,030 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:28,032 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4b99d4b0\n",
            "2021-05-07 06:41:28,034 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:28,059 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:28,062 INFO reduce.EventFetcher: attempt_local331198806_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:28,110 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local331198806_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:28,119 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local331198806_0001_m_000000_0\n",
            "2021-05-07 06:41:28,123 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:28,127 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:28,128 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:28,128 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:28,139 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:28,139 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:28,152 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:28,153 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:28,154 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:28,154 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:28,155 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:28,156 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:28,169 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:28,176 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:28,179 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:28,206 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:28,206 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:28,207 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:28,214 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:28,233 INFO mapreduce.Job: Job job_local331198806_0001 running in uber mode : false\n",
            "2021-05-07 06:41:28,234 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:28,515 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:28,536 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:28,538 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:28,538 INFO mapred.Task: Task:attempt_local331198806_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:28,540 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:28,541 INFO mapred.Task: Task attempt_local331198806_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:28,544 INFO output.FileOutputCommitter: Saved output of task 'attempt_local331198806_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:28,546 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:28,546 INFO mapred.Task: Task 'attempt_local331198806_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:28,547 INFO mapred.Task: Final Counters for attempt_local331198806_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698674\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:28,547 INFO mapred.LocalJobRunner: Finishing task: attempt_local331198806_0001_r_000000_0\n",
            "2021-05-07 06:41:28,547 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:29,235 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:29,236 INFO mapreduce.Job: Job job_local331198806_0001 completed successfully\n",
            "2021-05-07 06:41:29,254 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=679477248\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:29,254 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:32,936 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+9\n",
            "2021-05-07 06:41:34,216 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob14187414320987630843.jar tmpDir=null\n",
            "2021-05-07 06:41:34,971 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:35,177 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:35,178 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:35,206 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:35,389 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:35,421 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:35,774 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1388449583_0001\n",
            "2021-05-07 06:41:35,774 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:36,242 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1388449583_0001_417fe169-bbd1-4a78-b3ef-f42bf42cbdb1/centroids.txt\n",
            "2021-05-07 06:41:36,384 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:36,386 INFO mapreduce.Job: Running job: job_local1388449583_0001\n",
            "2021-05-07 06:41:36,397 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:36,399 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:36,409 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:36,409 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:36,460 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:36,469 INFO mapred.LocalJobRunner: Starting task: attempt_local1388449583_0001_m_000000_0\n",
            "2021-05-07 06:41:36,513 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:36,514 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:36,559 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:36,573 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:36,593 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:36,656 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:36,657 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:36,657 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:36,657 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:36,657 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:36,659 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:36,668 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:36,679 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:36,680 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:36,681 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:36,682 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:36,683 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:36,683 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:36,683 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:36,683 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:36,684 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:36,685 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:36,685 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:36,685 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:36,721 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:36,722 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:36,723 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:36,886 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:37,053 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:37,054 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:37,057 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:37,057 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:37,058 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:37,058 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:37,058 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:37,080 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:37,108 INFO mapred.Task: Task:attempt_local1388449583_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:37,114 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:37,115 INFO mapred.Task: Task 'attempt_local1388449583_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:37,123 INFO mapred.Task: Final Counters for attempt_local1388449583_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658596\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:37,123 INFO mapred.LocalJobRunner: Finishing task: attempt_local1388449583_0001_m_000000_0\n",
            "2021-05-07 06:41:37,123 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:37,126 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:37,126 INFO mapred.LocalJobRunner: Starting task: attempt_local1388449583_0001_r_000000_0\n",
            "2021-05-07 06:41:37,135 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:37,135 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:37,136 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:37,139 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@889067f\n",
            "2021-05-07 06:41:37,144 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:37,167 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:37,170 INFO reduce.EventFetcher: attempt_local1388449583_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:37,214 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1388449583_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:37,223 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1388449583_0001_m_000000_0\n",
            "2021-05-07 06:41:37,224 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:37,229 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:37,231 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:37,231 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:37,238 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:37,239 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:37,250 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:37,251 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:37,251 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:37,251 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:37,252 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:37,253 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:37,267 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:37,274 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:37,276 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:37,301 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:37,301 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:37,303 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:37,326 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:37,394 INFO mapreduce.Job: Job job_local1388449583_0001 running in uber mode : false\n",
            "2021-05-07 06:41:37,395 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:37,579 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:37,601 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:37,602 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:37,603 INFO mapred.Task: Task:attempt_local1388449583_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:37,604 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:37,605 INFO mapred.Task: Task attempt_local1388449583_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:37,607 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1388449583_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:37,608 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:37,608 INFO mapred.Task: Task 'attempt_local1388449583_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:37,609 INFO mapred.Task: Final Counters for attempt_local1388449583_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=373293056\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:37,609 INFO mapred.LocalJobRunner: Finishing task: attempt_local1388449583_0001_r_000000_0\n",
            "2021-05-07 06:41:37,609 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:38,412 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:38,412 INFO mapreduce.Job: Job job_local1388449583_0001 completed successfully\n",
            "2021-05-07 06:41:38,423 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360253\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=746586112\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:38,430 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:42,020 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+10\n",
            "2021-05-07 06:41:43,330 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob15740861416400274780.jar tmpDir=null\n",
            "2021-05-07 06:41:44,282 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:44,477 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:44,477 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:44,499 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:44,670 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:44,700 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:45,048 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local263574425_0001\n",
            "2021-05-07 06:41:45,048 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:45,469 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local263574425_0001_e9c92afd-bef4-44bd-8237-bf5ce4160e2d/centroids.txt\n",
            "2021-05-07 06:41:45,572 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:45,573 INFO mapreduce.Job: Running job: job_local263574425_0001\n",
            "2021-05-07 06:41:45,580 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:45,583 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:45,587 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:45,588 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:45,654 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:45,658 INFO mapred.LocalJobRunner: Starting task: attempt_local263574425_0001_m_000000_0\n",
            "2021-05-07 06:41:45,692 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:45,694 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:45,733 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:45,757 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:45,789 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:45,865 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:45,865 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:45,865 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:45,865 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:45,865 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:45,869 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:45,881 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:45,890 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:45,891 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:45,891 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:45,892 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:45,892 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:45,893 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:45,893 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:45,893 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:45,894 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:45,895 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:45,895 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:45,895 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:45,924 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:45,924 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:45,926 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:46,090 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:46,258 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:46,258 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:46,261 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:46,261 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:46,261 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:46,261 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:46,261 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:46,284 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:46,297 INFO mapred.Task: Task:attempt_local263574425_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:46,303 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:46,304 INFO mapred.Task: Task 'attempt_local263574425_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:46,317 INFO mapred.Task: Final Counters for attempt_local263574425_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655613\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:46,317 INFO mapred.LocalJobRunner: Finishing task: attempt_local263574425_0001_m_000000_0\n",
            "2021-05-07 06:41:46,318 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:46,321 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:46,321 INFO mapred.LocalJobRunner: Starting task: attempt_local263574425_0001_r_000000_0\n",
            "2021-05-07 06:41:46,333 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:46,333 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:46,334 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:46,337 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c676960\n",
            "2021-05-07 06:41:46,339 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:46,377 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:46,385 INFO reduce.EventFetcher: attempt_local263574425_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:46,427 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local263574425_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:46,431 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local263574425_0001_m_000000_0\n",
            "2021-05-07 06:41:46,434 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:46,438 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:46,439 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:46,440 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:46,446 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:46,447 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:46,461 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:46,462 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:46,462 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:46,465 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:46,471 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:46,472 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:46,485 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:46,493 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:46,495 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:46,514 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:46,514 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:46,516 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:46,535 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:46,579 INFO mapreduce.Job: Job job_local263574425_0001 running in uber mode : false\n",
            "2021-05-07 06:41:46,581 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:46,832 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:46,856 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:46,857 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:46,858 INFO mapred.Task: Task:attempt_local263574425_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:46,861 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:46,862 INFO mapred.Task: Task attempt_local263574425_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:46,864 INFO output.FileOutputCommitter: Saved output of task 'attempt_local263574425_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:46,866 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:46,866 INFO mapred.Task: Task 'attempt_local263574425_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:46,867 INFO mapred.Task: Final Counters for attempt_local263574425_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698674\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:46,867 INFO mapred.LocalJobRunner: Finishing task: attempt_local263574425_0001_r_000000_0\n",
            "2021-05-07 06:41:46,867 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:47,583 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:47,583 INFO mapreduce.Job: Job job_local263574425_0001 completed successfully\n",
            "2021-05-07 06:41:47,597 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=685768704\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:47,597 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:41:51,377 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+11\n",
            "2021-05-07 06:41:52,619 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob12511896374435851640.jar tmpDir=null\n",
            "2021-05-07 06:41:53,510 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:41:53,758 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:41:53,758 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:41:53,779 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:53,910 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:41:53,930 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:41:54,241 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local622324447_0001\n",
            "2021-05-07 06:41:54,242 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:41:54,710 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local622324447_0001_d1d1a3d4-015a-46ec-8348-a315738e65aa/centroids.txt\n",
            "2021-05-07 06:41:54,832 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:41:54,834 INFO mapreduce.Job: Running job: job_local622324447_0001\n",
            "2021-05-07 06:41:54,842 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:41:54,846 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:41:54,856 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:54,857 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:54,918 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:41:54,921 INFO mapred.LocalJobRunner: Starting task: attempt_local622324447_0001_m_000000_0\n",
            "2021-05-07 06:41:54,989 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:54,991 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:55,026 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:55,037 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:41:55,060 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:41:55,132 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:41:55,132 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:41:55,132 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:41:55,132 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:41:55,133 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:41:55,135 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:41:55,144 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:41:55,155 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:41:55,156 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:41:55,157 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:41:55,157 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:41:55,158 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:41:55,158 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:41:55,159 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:41:55,159 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:41:55,160 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:41:55,161 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:41:55,161 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:41:55,162 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:41:55,199 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,199 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,201 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,345 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:41:55,523 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:55,524 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:55,527 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:41:55,528 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:41:55,528 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:41:55,528 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:41:55,528 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:41:55,553 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:41:55,565 INFO mapred.Task: Task:attempt_local622324447_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:55,570 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:41:55,570 INFO mapred.Task: Task 'attempt_local622324447_0001_m_000000_0' done.\n",
            "2021-05-07 06:41:55,582 INFO mapred.Task: Final Counters for attempt_local622324447_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=655613\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=338690048\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:41:55,582 INFO mapred.LocalJobRunner: Finishing task: attempt_local622324447_0001_m_000000_0\n",
            "2021-05-07 06:41:55,582 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:41:55,588 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:41:55,589 INFO mapred.LocalJobRunner: Starting task: attempt_local622324447_0001_r_000000_0\n",
            "2021-05-07 06:41:55,618 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:41:55,618 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:41:55,620 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:41:55,624 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5cd55517\n",
            "2021-05-07 06:41:55,631 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:41:55,657 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:41:55,660 INFO reduce.EventFetcher: attempt_local622324447_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:41:55,712 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local622324447_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:41:55,716 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local622324447_0001_m_000000_0\n",
            "2021-05-07 06:41:55,723 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:41:55,727 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:41:55,728 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:55,728 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:41:55,738 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:55,739 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:55,750 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:41:55,751 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:41:55,752 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:41:55,752 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:41:55,753 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:41:55,754 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:55,768 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:41:55,775 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:41:55,778 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:41:55,795 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,796 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,798 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,816 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:41:55,843 INFO mapreduce.Job: Job job_local622324447_0001 running in uber mode : false\n",
            "2021-05-07 06:41:55,844 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:41:56,111 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:41:56,128 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:41:56,128 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:41:56,129 INFO mapred.Task: Task:attempt_local622324447_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:41:56,131 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:41:56,131 INFO mapred.Task: Task attempt_local622324447_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:41:56,136 INFO output.FileOutputCommitter: Saved output of task 'attempt_local622324447_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:41:56,138 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:41:56,139 INFO mapred.Task: Task 'attempt_local622324447_0001_r_000000_0' done.\n",
            "2021-05-07 06:41:56,140 INFO mapred.Task: Final Counters for attempt_local622324447_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=698674\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=338690048\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:56,140 INFO mapred.LocalJobRunner: Finishing task: attempt_local622324447_0001_r_000000_0\n",
            "2021-05-07 06:41:56,140 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:41:56,846 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:41:56,846 INFO mapreduce.Job: Job job_local622324447_0001 completed successfully\n",
            "2021-05-07 06:41:56,858 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1354287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=677380096\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:41:56,858 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:42:00,646 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+12\n",
            "2021-05-07 06:42:02,021 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob4956200771914613945.jar tmpDir=null\n",
            "2021-05-07 06:42:02,864 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:03,012 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:03,012 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:03,035 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:03,176 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:03,195 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:03,495 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1539431743_0001\n",
            "2021-05-07 06:42:03,496 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:03,926 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1539431743_0001_a29ba0b4-2fdc-494d-a617-08cb520ceb40/centroids.txt\n",
            "2021-05-07 06:42:04,019 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:04,021 INFO mapreduce.Job: Running job: job_local1539431743_0001\n",
            "2021-05-07 06:42:04,029 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:04,031 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:04,036 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:04,036 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:04,086 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:04,090 INFO mapred.LocalJobRunner: Starting task: attempt_local1539431743_0001_m_000000_0\n",
            "2021-05-07 06:42:04,146 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:04,150 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:04,192 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:04,207 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:42:04,226 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:04,300 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:04,300 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:04,300 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:04,300 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:04,300 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:04,303 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:04,312 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:42:04,322 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:04,323 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:04,324 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:04,325 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:04,326 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:04,326 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:04,326 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:04,327 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:04,327 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:04,328 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:04,328 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:04,329 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:04,365 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,365 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,367 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,530 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:42:04,691 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:04,694 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:04,697 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:04,698 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:04,698 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:04,698 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:42:04,698 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:42:04,723 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:04,744 INFO mapred.Task: Task:attempt_local1539431743_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:04,750 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:42:04,750 INFO mapred.Task: Task 'attempt_local1539431743_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:04,772 INFO mapred.Task: Final Counters for attempt_local1539431743_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658594\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:42:04,772 INFO mapred.LocalJobRunner: Finishing task: attempt_local1539431743_0001_m_000000_0\n",
            "2021-05-07 06:42:04,772 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:04,777 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:04,778 INFO mapred.LocalJobRunner: Starting task: attempt_local1539431743_0001_r_000000_0\n",
            "2021-05-07 06:42:04,787 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:04,787 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:04,787 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:04,790 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@15011c8\n",
            "2021-05-07 06:42:04,796 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:04,821 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:04,824 INFO reduce.EventFetcher: attempt_local1539431743_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:04,872 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1539431743_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:42:04,880 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1539431743_0001_m_000000_0\n",
            "2021-05-07 06:42:04,884 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:42:04,890 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:04,891 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:04,892 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:04,900 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:04,900 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:04,909 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:04,910 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:42:04,911 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:04,911 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:04,912 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:04,913 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:04,927 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:42:04,934 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:04,937 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:04,962 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,962 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,964 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:04,971 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:05,027 INFO mapreduce.Job: Job job_local1539431743_0001 running in uber mode : false\n",
            "2021-05-07 06:42:05,029 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:42:05,245 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:42:05,267 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:05,267 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:05,269 INFO mapred.Task: Task:attempt_local1539431743_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:05,270 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:05,271 INFO mapred.Task: Task attempt_local1539431743_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:05,273 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1539431743_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:42:05,274 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:42:05,274 INFO mapred.Task: Task 'attempt_local1539431743_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:05,275 INFO mapred.Task: Final Counters for attempt_local1539431743_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701655\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:05,276 INFO mapred.LocalJobRunner: Finishing task: attempt_local1539431743_0001_r_000000_0\n",
            "2021-05-07 06:42:05,276 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:06,030 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:06,031 INFO mapreduce.Job: Job job_local1539431743_0001 completed successfully\n",
            "2021-05-07 06:42:06,044 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360249\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:06,044 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:42:09,781 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+13\n",
            "2021-05-07 06:42:11,151 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob11495634463105261047.jar tmpDir=null\n",
            "2021-05-07 06:42:12,026 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:12,270 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:12,271 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:12,302 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:12,507 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:12,544 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:12,872 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1883619258_0001\n",
            "2021-05-07 06:42:12,873 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:13,279 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1883619258_0001_ab4bf313-f730-4291-886a-0c2b6f585c1e/centroids.txt\n",
            "2021-05-07 06:42:13,395 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:13,397 INFO mapreduce.Job: Running job: job_local1883619258_0001\n",
            "2021-05-07 06:42:13,410 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:13,413 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:13,420 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:13,421 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:13,486 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:13,490 INFO mapred.LocalJobRunner: Starting task: attempt_local1883619258_0001_m_000000_0\n",
            "2021-05-07 06:42:13,535 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:13,538 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:13,573 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:13,587 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:42:13,613 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:13,676 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:13,676 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:13,676 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:13,676 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:13,676 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:13,680 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:13,689 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:42:13,698 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:13,699 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:13,700 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:13,701 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:13,702 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:13,702 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:13,702 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:13,703 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:13,703 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:13,709 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:13,709 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:13,710 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:13,744 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:13,744 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:13,746 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:13,908 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:42:14,063 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:14,064 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:14,067 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:14,067 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:14,067 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:14,067 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:42:14,067 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:42:14,097 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:14,113 INFO mapred.Task: Task:attempt_local1883619258_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:14,119 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:42:14,120 INFO mapred.Task: Task 'attempt_local1883619258_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:14,134 INFO mapred.Task: Final Counters for attempt_local1883619258_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658596\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=350224384\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:42:14,134 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883619258_0001_m_000000_0\n",
            "2021-05-07 06:42:14,134 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:14,138 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:14,138 INFO mapred.LocalJobRunner: Starting task: attempt_local1883619258_0001_r_000000_0\n",
            "2021-05-07 06:42:14,153 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:14,154 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:14,156 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:14,162 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3e792bf9\n",
            "2021-05-07 06:42:14,172 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:14,199 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:14,201 INFO reduce.EventFetcher: attempt_local1883619258_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:14,250 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1883619258_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:42:14,254 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1883619258_0001_m_000000_0\n",
            "2021-05-07 06:42:14,258 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:42:14,261 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:14,263 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:14,263 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:14,274 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:14,275 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:14,288 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:14,289 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:42:14,290 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:14,290 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:14,291 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:14,292 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:14,304 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:42:14,310 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:14,312 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:14,345 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:14,345 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:14,347 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:14,361 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:14,402 INFO mapreduce.Job: Job job_local1883619258_0001 running in uber mode : false\n",
            "2021-05-07 06:42:14,403 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:42:14,660 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:42:14,689 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:14,689 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:14,690 INFO mapred.Task: Task:attempt_local1883619258_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:14,692 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:14,692 INFO mapred.Task: Task attempt_local1883619258_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:14,695 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1883619258_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:42:14,698 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:42:14,698 INFO mapred.Task: Task 'attempt_local1883619258_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:14,699 INFO mapred.Task: Final Counters for attempt_local1883619258_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=350224384\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:14,701 INFO mapred.LocalJobRunner: Finishing task: attempt_local1883619258_0001_r_000000_0\n",
            "2021-05-07 06:42:14,701 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:15,404 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:15,405 INFO mapreduce.Job: Job job_local1883619258_0001 completed successfully\n",
            "2021-05-07 06:42:15,419 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360253\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=700448768\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:15,421 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:42:18,883 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+14\n",
            "2021-05-07 06:42:20,284 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob11776918090568977660.jar tmpDir=null\n",
            "2021-05-07 06:42:21,224 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:21,505 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:21,506 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:21,530 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:21,668 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:21,689 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:22,027 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1744609957_0001\n",
            "2021-05-07 06:42:22,027 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:22,500 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1744609957_0001_a791b689-11cb-431c-aaee-f358937367ac/centroids.txt\n",
            "2021-05-07 06:42:22,619 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:22,621 INFO mapreduce.Job: Running job: job_local1744609957_0001\n",
            "2021-05-07 06:42:22,629 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:22,632 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:22,642 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:22,642 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:22,699 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:22,704 INFO mapred.LocalJobRunner: Starting task: attempt_local1744609957_0001_m_000000_0\n",
            "2021-05-07 06:42:22,743 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:22,746 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:22,803 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:22,831 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:42:22,852 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:22,923 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:22,923 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:22,923 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:22,923 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:22,923 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:22,927 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:22,945 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:42:22,953 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:22,954 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:22,954 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:22,954 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:22,955 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:22,955 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:22,956 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:22,956 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:22,956 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:22,957 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:22,957 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:22,958 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:22,994 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:22,994 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:22,996 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:23,168 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:42:23,334 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:23,335 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:23,338 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:23,338 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:23,338 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:23,338 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:42:23,338 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:42:23,366 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:23,380 INFO mapred.Task: Task:attempt_local1744609957_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:23,383 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:42:23,383 INFO mapred.Task: Task 'attempt_local1744609957_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:23,394 INFO mapred.Task: Final Counters for attempt_local1744609957_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658596\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=369098752\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:42:23,395 INFO mapred.LocalJobRunner: Finishing task: attempt_local1744609957_0001_m_000000_0\n",
            "2021-05-07 06:42:23,395 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:23,399 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:23,402 INFO mapred.LocalJobRunner: Starting task: attempt_local1744609957_0001_r_000000_0\n",
            "2021-05-07 06:42:23,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:23,414 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:23,415 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:23,421 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5c45da0c\n",
            "2021-05-07 06:42:23,423 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:23,447 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:23,454 INFO reduce.EventFetcher: attempt_local1744609957_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:23,513 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1744609957_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:42:23,523 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1744609957_0001_m_000000_0\n",
            "2021-05-07 06:42:23,526 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:42:23,530 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:23,532 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:23,532 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:23,540 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:23,540 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:23,552 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:23,552 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:42:23,553 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:23,553 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:23,555 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:23,555 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:23,577 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:42:23,585 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:23,587 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:23,609 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:23,610 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:23,611 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:23,625 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:23,630 INFO mapreduce.Job: Job job_local1744609957_0001 running in uber mode : false\n",
            "2021-05-07 06:42:23,631 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:42:23,906 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:42:23,934 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:23,935 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:23,935 INFO mapred.Task: Task:attempt_local1744609957_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:23,937 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:23,937 INFO mapred.Task: Task attempt_local1744609957_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:23,939 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1744609957_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:42:23,941 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:42:23,941 INFO mapred.Task: Task 'attempt_local1744609957_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:23,942 INFO mapred.Task: Final Counters for attempt_local1744609957_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=369098752\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:23,942 INFO mapred.LocalJobRunner: Finishing task: attempt_local1744609957_0001_r_000000_0\n",
            "2021-05-07 06:42:23,943 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:24,633 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:24,633 INFO mapreduce.Job: Job job_local1744609957_0001 completed successfully\n",
            "2021-05-07 06:42:24,650 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360253\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=738197504\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:24,656 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:42:28,491 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+15\n",
            "2021-05-07 06:42:29,695 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob18108820763030592151.jar tmpDir=null\n",
            "2021-05-07 06:42:30,523 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:30,693 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:30,694 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:30,726 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:30,879 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:30,917 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:31,307 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1685465989_0001\n",
            "2021-05-07 06:42:31,307 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:31,712 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1685465989_0001_2eb812f9-296f-4b15-a283-b60b81c4a5c7/centroids.txt\n",
            "2021-05-07 06:42:31,813 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:31,815 INFO mapreduce.Job: Running job: job_local1685465989_0001\n",
            "2021-05-07 06:42:31,820 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:31,831 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:31,841 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:31,841 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:31,891 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:31,901 INFO mapred.LocalJobRunner: Starting task: attempt_local1685465989_0001_m_000000_0\n",
            "2021-05-07 06:42:31,947 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:31,950 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:32,001 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:32,025 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:42:32,046 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:32,114 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:32,114 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:32,114 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:32,114 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:32,114 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:32,119 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:32,133 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:42:32,146 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:32,147 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:32,147 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:32,148 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:32,149 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:32,149 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:32,150 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:32,151 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:32,152 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:32,157 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:32,157 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:32,157 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:32,194 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,197 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,199 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,348 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:42:32,501 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:32,502 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:32,506 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:32,506 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:32,506 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:32,506 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:42:32,506 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:42:32,542 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:32,554 INFO mapred.Task: Task:attempt_local1685465989_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:32,557 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:42:32,557 INFO mapred.Task: Task 'attempt_local1685465989_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:32,578 INFO mapred.Task: Final Counters for attempt_local1685465989_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36980\n",
            "\t\tFILE: Number of bytes written=658596\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:42:32,578 INFO mapred.LocalJobRunner: Finishing task: attempt_local1685465989_0001_m_000000_0\n",
            "2021-05-07 06:42:32,579 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:32,582 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:32,582 INFO mapred.LocalJobRunner: Starting task: attempt_local1685465989_0001_r_000000_0\n",
            "2021-05-07 06:42:32,592 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:32,592 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:32,593 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:32,595 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@758ddb80\n",
            "2021-05-07 06:42:32,598 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:32,633 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:32,636 INFO reduce.EventFetcher: attempt_local1685465989_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:32,694 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1685465989_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:42:32,702 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local1685465989_0001_m_000000_0\n",
            "2021-05-07 06:42:32,707 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:42:32,714 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:32,716 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:32,716 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:32,725 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:32,726 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:32,745 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:32,746 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:42:32,747 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:32,747 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:32,748 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:32,750 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:32,764 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:42:32,767 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:32,768 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:32,794 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,794 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,795 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,803 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:32,834 INFO mapreduce.Job: Job job_local1685465989_0001 running in uber mode : false\n",
            "2021-05-07 06:42:32,836 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:42:33,071 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:42:33,090 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:33,091 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:33,092 INFO mapred.Task: Task:attempt_local1685465989_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:33,093 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:33,093 INFO mapred.Task: Task attempt_local1685465989_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:33,095 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1685465989_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:42:33,096 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:42:33,096 INFO mapred.Task: Task 'attempt_local1685465989_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:33,097 INFO mapred.Task: Final Counters for attempt_local1685465989_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121220\n",
            "\t\tFILE: Number of bytes written=701657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:33,097 INFO mapred.LocalJobRunner: Finishing task: attempt_local1685465989_0001_r_000000_0\n",
            "2021-05-07 06:42:33,097 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:33,838 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:33,839 INFO mapreduce.Job: Job job_local1685465989_0001 completed successfully\n",
            "2021-05-07 06:42:33,852 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158200\n",
            "\t\tFILE: Number of bytes written=1360253\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=656408576\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=957\n",
            "2021-05-07 06:42:33,852 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:42:37,503 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "First Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 06:42:38,913 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids.txt] [] /tmp/streamjob2607884398092082329.jar tmpDir=null\n",
            "2021-05-07 06:42:39,746 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:39,951 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:39,951 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:39,976 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:40,143 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:40,185 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:40,572 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local326989034_0001\n",
            "2021-05-07 06:42:40,573 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:41,067 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local326989034_0001_8cd903a2-1ed1-462c-abd5-a66a97cdfbfa/centroids.txt\n",
            "2021-05-07 06:42:41,199 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:41,201 INFO mapreduce.Job: Running job: job_local326989034_0001\n",
            "2021-05-07 06:42:41,210 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:41,214 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:41,224 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:41,224 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:41,280 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:41,284 INFO mapred.LocalJobRunner: Starting task: attempt_local326989034_0001_m_000000_0\n",
            "2021-05-07 06:42:41,318 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:41,321 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:41,356 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:41,386 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+35343\n",
            "2021-05-07 06:42:41,416 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:41,483 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:41,483 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:41,483 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:41,483 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:41,483 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:41,488 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:41,498 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:42:41,517 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:41,518 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:41,519 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:41,519 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:41,520 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:41,520 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:41,521 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:41,521 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:41,524 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:41,525 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:41,525 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:41,526 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:41,564 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:41,564 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:41,566 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:41,732 INFO streaming.PipeMapRed: Records R/W=965/1\n",
            "2021-05-07 06:42:41,888 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:41,889 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:41,892 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:41,892 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:41,893 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:41,893 INFO mapred.MapTask: bufstart = 0; bufend = 38238; bufvoid = 104857600\n",
            "2021-05-07 06:42:41,893 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206680(104826720); length = 7717/6553600\n",
            "2021-05-07 06:42:41,924 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:41,937 INFO mapred.Task: Task:attempt_local326989034_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:41,945 INFO mapred.LocalJobRunner: Records R/W=965/1\n",
            "2021-05-07 06:42:41,945 INFO mapred.Task: Task 'attempt_local326989034_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:41,955 INFO mapred.Task: Final Counters for attempt_local326989034_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36937\n",
            "\t\tFILE: Number of bytes written=655590\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1930\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "2021-05-07 06:42:41,955 INFO mapred.LocalJobRunner: Finishing task: attempt_local326989034_0001_m_000000_0\n",
            "2021-05-07 06:42:41,956 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:41,967 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:41,968 INFO mapred.LocalJobRunner: Starting task: attempt_local326989034_0001_r_000000_0\n",
            "2021-05-07 06:42:41,980 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:41,985 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:41,985 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:41,991 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f1310ca\n",
            "2021-05-07 06:42:41,997 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:42,021 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:42,027 INFO reduce.EventFetcher: attempt_local326989034_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:42,085 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local326989034_0001_m_000000_0 decomp: 42100 len: 42104 to MEMORY\n",
            "2021-05-07 06:42:42,091 INFO reduce.InMemoryMapOutput: Read 42100 bytes from map-output for attempt_local326989034_0001_m_000000_0\n",
            "2021-05-07 06:42:42,094 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42100, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42100\n",
            "2021-05-07 06:42:42,099 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:42,100 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:42,100 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:42,110 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:42,110 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:42,130 INFO reduce.MergeManagerImpl: Merged 1 segments, 42100 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:42,130 INFO reduce.MergeManagerImpl: Merging 1 files, 42104 bytes from disk\n",
            "2021-05-07 06:42:42,131 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:42,131 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:42,132 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 42097 bytes\n",
            "2021-05-07 06:42:42,133 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:42,143 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, u_reducer.py]\n",
            "2021-05-07 06:42:42,147 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:42,148 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:42,174 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:42,174 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:42,175 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:42,194 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:42,209 INFO mapreduce.Job: Job job_local326989034_0001 running in uber mode : false\n",
            "2021-05-07 06:42:42,210 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:42:42,480 INFO streaming.PipeMapRed: Records R/W=1930/1\n",
            "2021-05-07 06:42:42,496 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:42,497 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:42,498 INFO mapred.Task: Task:attempt_local326989034_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:42,499 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:42,499 INFO mapred.Task: Task attempt_local326989034_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:42,501 INFO output.FileOutputCommitter: Saved output of task 'attempt_local326989034_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 06:42:42,502 INFO mapred.LocalJobRunner: Records R/W=1930/1 > reduce\n",
            "2021-05-07 06:42:42,502 INFO mapred.Task: Task 'attempt_local326989034_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:42,503 INFO mapred.Task: Final Counters for attempt_local326989034_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=121177\n",
            "\t\tFILE: Number of bytes written=698649\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=1930\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=955\n",
            "2021-05-07 06:42:42,503 INFO mapred.LocalJobRunner: Finishing task: attempt_local326989034_0001_r_000000_0\n",
            "2021-05-07 06:42:42,505 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:43,216 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:43,216 INFO mapreduce.Job: Job job_local326989034_0001 completed successfully\n",
            "2021-05-07 06:42:43,229 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=158114\n",
            "\t\tFILE: Number of bytes written=1354239\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=965\n",
            "\t\tMap output records=1930\n",
            "\t\tMap output bytes=38238\n",
            "\t\tMap output materialized bytes=42104\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=42104\n",
            "\t\tReduce input records=1930\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3860\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=715128832\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=35343\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=955\n",
            "2021-05-07 06:42:43,229 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 06:42:46,933 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Second Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 06:42:48,323 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids1.txt] [] /tmp/streamjob4625021088099339859.jar tmpDir=null\n",
            "2021-05-07 06:42:49,158 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:42:49,299 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:42:49,299 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:42:49,351 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:49,492 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:42:49,514 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:42:49,879 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1524324289_0001\n",
            "2021-05-07 06:42:49,880 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:42:50,390 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids1.txt as file:/tmp/hadoop-root/mapred/local/job_local1524324289_0001_7c304668-ca4a-4297-951a-000422ff1007/centroids1.txt\n",
            "2021-05-07 06:42:50,487 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:42:50,489 INFO mapreduce.Job: Running job: job_local1524324289_0001\n",
            "2021-05-07 06:42:50,495 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:42:50,497 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:42:50,502 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:50,502 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:50,550 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:42:50,553 INFO mapred.LocalJobRunner: Starting task: attempt_local1524324289_0001_m_000000_0\n",
            "2021-05-07 06:42:50,609 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:50,612 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:50,665 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:50,686 INFO mapred.MapTask: Processing split: file:/content/test.txt:0+697222\n",
            "2021-05-07 06:42:50,716 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:42:50,781 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:42:50,781 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:42:50,781 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:42:50,781 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:42:50,781 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:42:50,784 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:42:50,792 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_mapper.py]\n",
            "2021-05-07 06:42:50,804 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:42:50,805 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:42:50,806 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:42:50,806 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:42:50,807 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:42:50,808 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:42:50,808 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:42:50,808 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:42:50,809 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:42:50,810 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:42:50,810 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:42:50,810 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:42:50,846 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:50,846 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:50,848 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:50,858 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:51,493 INFO mapreduce.Job: Job job_local1524324289_0001 running in uber mode : false\n",
            "2021-05-07 06:42:51,495 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 06:42:51,717 INFO streaming.PipeMapRed: Records R/W=7404/1\n",
            "2021-05-07 06:42:52,291 INFO streaming.PipeMapRed: R/W/S=10000/4097/0 in:10000=10000/1 [rec/s] out:4097=4097/1 [rec/s]\n",
            "2021-05-07 06:42:54,654 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:54,655 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:54,658 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:42:54,658 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:42:54,658 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:42:54,658 INFO mapred.MapTask: bufstart = 0; bufend = 76140; bufvoid = 104857600\n",
            "2021-05-07 06:42:54,658 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26138260(104553040); length = 76137/6553600\n",
            "2021-05-07 06:42:54,738 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:42:54,759 INFO mapred.Task: Task:attempt_local1524324289_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:54,763 INFO mapred.LocalJobRunner: Records R/W=7404/1\n",
            "2021-05-07 06:42:54,763 INFO mapred.Task: Task 'attempt_local1524324289_0001_m_000000_0' done.\n",
            "2021-05-07 06:42:54,773 INFO mapred.Task: Final Counters for attempt_local1524324289_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=698819\n",
            "\t\tFILE: Number of bytes written=730693\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19035\n",
            "\t\tMap output records=19035\n",
            "\t\tMap output bytes=76140\n",
            "\t\tMap output materialized bytes=114216\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=19035\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=333447168\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=697222\n",
            "2021-05-07 06:42:54,773 INFO mapred.LocalJobRunner: Finishing task: attempt_local1524324289_0001_m_000000_0\n",
            "2021-05-07 06:42:54,773 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:42:54,777 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:42:54,777 INFO mapred.LocalJobRunner: Starting task: attempt_local1524324289_0001_r_000000_0\n",
            "2021-05-07 06:42:54,788 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:42:54,788 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:42:54,788 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:42:54,796 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a4438c5\n",
            "2021-05-07 06:42:54,800 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:42:54,822 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:42:54,824 INFO reduce.EventFetcher: attempt_local1524324289_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:42:54,883 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1524324289_0001_m_000000_0 decomp: 114212 len: 114216 to MEMORY\n",
            "2021-05-07 06:42:54,887 INFO reduce.InMemoryMapOutput: Read 114212 bytes from map-output for attempt_local1524324289_0001_m_000000_0\n",
            "2021-05-07 06:42:54,891 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 114212, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->114212\n",
            "2021-05-07 06:42:54,896 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:42:54,899 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:54,900 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:42:54,908 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:54,908 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 114208 bytes\n",
            "2021-05-07 06:42:54,967 INFO reduce.MergeManagerImpl: Merged 1 segments, 114212 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:42:54,968 INFO reduce.MergeManagerImpl: Merging 1 files, 114216 bytes from disk\n",
            "2021-05-07 06:42:54,968 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:42:54,968 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:42:54,974 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 114208 bytes\n",
            "2021-05-07 06:42:54,977 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:54,992 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_reducer.py]\n",
            "2021-05-07 06:42:54,995 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:42:54,996 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:42:55,019 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:55,020 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:55,021 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:55,038 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:55,064 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:42:55,343 INFO streaming.PipeMapRed: Records R/W=19035/1\n",
            "2021-05-07 06:42:55,386 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:42:55,387 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:42:55,388 INFO mapred.Task: Task:attempt_local1524324289_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:42:55,389 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:42:55,390 INFO mapred.Task: Task attempt_local1524324289_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:42:55,392 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1524324289_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 06:42:55,393 INFO mapred.LocalJobRunner: Records R/W=19035/1 > reduce\n",
            "2021-05-07 06:42:55,393 INFO mapred.Task: Task 'attempt_local1524324289_0001_r_000000_0' done.\n",
            "2021-05-07 06:42:55,394 INFO mapred.Task: Final Counters for attempt_local1524324289_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=927283\n",
            "\t\tFILE: Number of bytes written=883350\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21\n",
            "\t\tReduce shuffle bytes=114216\n",
            "\t\tReduce input records=19035\n",
            "\t\tReduce output records=21\n",
            "\t\tSpilled Records=19035\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=333447168\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=38441\n",
            "2021-05-07 06:42:55,394 INFO mapred.LocalJobRunner: Finishing task: attempt_local1524324289_0001_r_000000_0\n",
            "2021-05-07 06:42:55,394 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:42:55,500 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:42:55,501 INFO mapreduce.Job: Job job_local1524324289_0001 completed successfully\n",
            "2021-05-07 06:42:55,515 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1626102\n",
            "\t\tFILE: Number of bytes written=1614043\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19035\n",
            "\t\tMap output records=19035\n",
            "\t\tMap output bytes=76140\n",
            "\t\tMap output materialized bytes=114216\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=21\n",
            "\t\tReduce shuffle bytes=114216\n",
            "\t\tReduce input records=19035\n",
            "\t\tReduce output records=21\n",
            "\t\tSpilled Records=38070\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=666894336\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=697222\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=38441\n",
            "2021-05-07 06:42:55,519 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 06:42:59,295 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Third Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7yJ36FX7k4C"
      },
      "source": [
        "#evaluation \n",
        "\n",
        "import seaborn as sb\n",
        "import string\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "def parse_args():\n",
        "    '''Parse command line arguments'''\n",
        "    parser = argparse.ArgumentParser(description=\"Run classification evaluation\")\n",
        "    \n",
        "    parser.add_argument('--input', nargs='?', default='predictions.txt',help='Input File path')\n",
        "    parser.add_argument('--output', nargs='?', default='performance.png',help='Visualization plot name')\n",
        "    \n",
        "    return parser.parse_args()\n",
        "\n",
        "def get_performance(args):\n",
        "    fp = open(args.input,'r')\n",
        "\n",
        "    # Processing alphabets and preparing confusion matrix\n",
        "    cm = np.zeros((26, 26))\n",
        "    alphabets = string.ascii_uppercase[:26]\n",
        "    alphabet_dict = dict()\n",
        "    x_axis_labels = []\n",
        "    y_axis_labels = []\n",
        "    for i in range(26):\n",
        "        x_axis_labels.append(alphabets[i])\n",
        "        y_axis_labels.append(alphabets[i])\n",
        "        alphabet_dict[alphabets[i]] = i\n",
        "\n",
        "    # Filling up the confusion matrix\n",
        "    for i,line in enumerate(fp):\n",
        "        line_split = line.split('\\t')[0].split(',')\n",
        "        actual = line_split[0]\n",
        "        predictions = list(line_split[1:])\n",
        "        total_pred = len(predictions)\n",
        "        for p in predictions:\n",
        "          cm[alphabet_dict[actual]][alphabet_dict[p]] += 1.0\n",
        "\n",
        "    fp.close()\n",
        "\n",
        "    # Calculate accuracy with total sum of the array and trace of the array\n",
        "    print('Classification Accuracy: ' + str((np.trace(cm)/np.sum(cm))*100) + '%')\n",
        "    print(cm)\n",
        "    \n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cm = np.true_divide(cm, cm.sum(axis=1, keepdims=True))\n",
        "    cm[np.isnan(cm)] = 0\n",
        "\n",
        "    # Get heatmap of the confusion matrix\n",
        "    ax = sb.heatmap(cm, vmin=0, vmax=1, xticklabels=x_axis_labels, yticklabels=y_axis_labels).set_title('Confusion matrix')\n",
        "    fig = ax.get_figure()\n",
        "    fig.savefig(args.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jISXlPjlk2P1",
        "outputId": "872db781-fe63-42b4-fc83-41e135e531db"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    get_performance(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Accuracy: 25.27974783293932%\n",
            "[[478.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.   7.   0.\n",
            "    0.   0.   1.   0.   5.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 81.  89.   0. 130.  49.  46.   7.  43.  49.  60.  57.  65.  48.  52.\n",
            "    9.  69.  34. 110.  80.   0.   0.   0.   0.  51.   0.  60.]\n",
            " [  0.  64. 277.  64. 281.   5.  57.  55.  45.  43.  73. 113.   2.   8.\n",
            "    0.   3.   0.  43. 144.   0.   0.   0.   0.  81.   0. 296.]\n",
            " [  0.   0.  10.   0.   0. 447.   0.   1.   1.   1.   0.   0.   0.   2.\n",
            "    0. 325.   0.   0.   1. 176.   5.  47.   1.   3.  99.   1.]\n",
            " [ 39.  41.  69.  42.  34.   0.  94.  54.  50.   0.  51.   0.   0.  30.\n",
            "   88.  37.  94.  65.   0.   0.   0.   0.   0.  34.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   3.   0.   0. 260. 160.   0.  28.   0.   0.\n",
            "    0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
            " [ 20.  15.   0.  26.   0.   4.   0.   2.  32. 293.   0.  11.   1.  12.\n",
            "    0.  17.   0.   9.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 37.  77.  49.  86.  60.  61.  79.  92.  49.  43.  72.  57.  32.  51.\n",
            "   84. 125.  55.  66.  96.  34.  43.  28.   0.  71.  26.  79.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 322.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.  12.   0.   0.   0.   0.   5.   0.   0.   9.   0. 133.  70.\n",
            "    0.   0.   9.   3.   0.   0.  56.   0.  45.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0. 149.   0.   0.   0.   0. 156. 168.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  1.   7.  28. 199.   1.   1. 211.  80. 101.   2. 136.  10.  22.  96.\n",
            "  318.  34. 173. 110.   0.   4. 122.   0.   2. 122.   1.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 33. 210.  42. 155.  95.  55. 135. 104.  19.  36.  79.  54.  49.  77.\n",
            "  149.  57. 257. 172. 106.  58.  87.  26.  47.  87.  77.  55.]\n",
            " [ 28.  55.  43.  50.  55.  20.  38.  40.  30.  48.  53.  43.   8.  17.\n",
            "   21.   1.  41.  73.  56.  35.   5.  14.   2. 108.  16.  47.]\n",
            " [  0. 166.   2.  10.  65.   1.  64.   3.   2.   1.  25.   7.   0.   0.\n",
            "    9.   2.  50.  58. 145.   5.   0.   0.   0.  36.   0.  51.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0. 185.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  12.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0. 193.  15. 296.   3.   0. 191.   0.]\n",
            " [ 34.   4.  22.   1.   0.  22.  24.  14.   1.   1.  13.   3. 284. 122.\n",
            "   36.  74.  31.  12.   0.   5.  45.  90. 518.   3.  61.   0.]\n",
            " [  0.   0. 148.   2.  80.  60.  26.  56.   4.  22. 131.   1.  12.  39.\n",
            "    4.  17.   1.   0.  50.  68. 208.   2.   2.  95.  95.  46.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
            "    0.   0.   0.   0.   0. 181.   3. 223.  96.   0. 184.   0.]\n",
            " [  0.   0.   1.   0.  10.   0.   0.   0.  75.   0.   4.   0.   0.   0.\n",
            "    0.   0.   0.   0.  28.   0.   0.   0.   0.  58.   0.  63.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AzR3HUp8C0U"
      },
      "source": [
        "### For n=10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5hYgpgIk8du",
        "outputId": "c41b08ff-8cd7-4b40-9c86-28d9d35621ef"
      },
      "source": [
        "!sh model.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count---------+1\n",
            "2021-05-07 06:50:14,710 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob7030966371820598740.jar tmpDir=null\n",
            "2021-05-07 06:50:15,526 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:50:15,686 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:50:15,686 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:50:15,710 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:15,878 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:50:15,899 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:50:16,265 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1979508688_0001\n",
            "2021-05-07 06:50:16,265 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:50:16,734 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1979508688_0001_a8159625-3742-45be-8c32-f92620355f52/centroids.txt\n",
            "2021-05-07 06:50:16,831 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:50:16,833 INFO mapreduce.Job: Running job: job_local1979508688_0001\n",
            "2021-05-07 06:50:16,841 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:50:16,845 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:50:16,857 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:16,857 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:16,914 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:50:16,918 INFO mapred.LocalJobRunner: Starting task: attempt_local1979508688_0001_m_000000_0\n",
            "2021-05-07 06:50:16,979 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:16,984 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:17,032 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:17,057 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:50:17,077 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:50:17,145 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:50:17,145 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:50:17,145 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:50:17,145 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:50:17,146 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:50:17,149 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:50:17,158 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:50:17,175 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:50:17,177 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:50:17,178 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:50:17,181 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:50:17,182 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:50:17,183 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:50:17,184 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:50:17,184 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:50:17,186 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:50:17,187 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:50:17,187 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:50:17,188 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:50:17,231 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:17,231 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:17,234 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:17,261 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:17,406 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:50:17,736 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:17,736 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:17,739 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:50:17,740 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:50:17,740 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:50:17,740 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:50:17,740 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:50:17,779 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:50:17,801 INFO mapred.Task: Task:attempt_local1979508688_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:17,807 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:50:17,807 INFO mapred.Task: Task 'attempt_local1979508688_0001_m_000000_0' done.\n",
            "2021-05-07 06:50:17,813 INFO mapred.Task: Final Counters for attempt_local1979508688_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698933\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:50:17,813 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979508688_0001_m_000000_0\n",
            "2021-05-07 06:50:17,814 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:50:17,819 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:50:17,820 INFO mapred.LocalJobRunner: Starting task: attempt_local1979508688_0001_r_000000_0\n",
            "2021-05-07 06:50:17,841 INFO mapreduce.Job: Job job_local1979508688_0001 running in uber mode : false\n",
            "2021-05-07 06:50:17,846 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:50:17,849 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:17,849 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:17,849 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:17,852 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2dbbbc7a\n",
            "2021-05-07 06:50:17,854 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:17,887 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:50:17,889 INFO reduce.EventFetcher: attempt_local1979508688_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:50:17,945 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1979508688_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:50:17,955 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1979508688_0001_m_000000_0\n",
            "2021-05-07 06:50:17,960 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:50:17,961 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:50:17,964 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:17,964 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:50:17,972 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:17,972 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:17,985 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:50:17,986 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:50:17,987 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:50:17,987 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:17,988 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:17,989 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:17,998 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:50:18,003 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:50:18,005 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:50:18,036 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:18,036 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:18,037 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:18,044 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:18,310 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:50:18,326 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:18,327 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:18,328 INFO mapred.Task: Task:attempt_local1979508688_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:18,329 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:18,329 INFO mapred.Task: Task attempt_local1979508688_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:50:18,332 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1979508688_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:50:18,333 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:50:18,333 INFO mapred.Task: Task 'attempt_local1979508688_0001_r_000000_0' done.\n",
            "2021-05-07 06:50:18,334 INFO mapred.Task: Final Counters for attempt_local1979508688_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782345\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:18,334 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979508688_0001_r_000000_0\n",
            "2021-05-07 06:50:18,334 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:50:18,849 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:50:18,849 INFO mapreduce.Job: Job job_local1979508688_0001 completed successfully\n",
            "2021-05-07 06:50:18,861 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481278\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=694157312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:18,862 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:50:22,453 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+2\n",
            "2021-05-07 06:50:23,750 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16075774044142761721.jar tmpDir=null\n",
            "2021-05-07 06:50:24,550 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:50:24,737 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:50:24,737 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:50:24,762 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:24,928 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:50:24,961 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:50:25,326 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local371542726_0001\n",
            "2021-05-07 06:50:25,326 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:50:25,770 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local371542726_0001_195567c9-a4cd-4e81-9979-171ed17e3d63/centroids.txt\n",
            "2021-05-07 06:50:25,919 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:50:25,921 INFO mapreduce.Job: Running job: job_local371542726_0001\n",
            "2021-05-07 06:50:25,928 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:50:25,931 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:50:25,938 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:25,939 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:26,006 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:50:26,010 INFO mapred.LocalJobRunner: Starting task: attempt_local371542726_0001_m_000000_0\n",
            "2021-05-07 06:50:26,050 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:26,053 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:26,095 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:26,104 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:50:26,138 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:50:26,205 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:50:26,205 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:50:26,205 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:50:26,205 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:50:26,205 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:50:26,209 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:50:26,223 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:50:26,232 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:50:26,233 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:50:26,234 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:50:26,234 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:50:26,235 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:50:26,236 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:50:26,236 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:50:26,237 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:50:26,237 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:50:26,238 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:50:26,238 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:50:26,239 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:50:26,271 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:26,271 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:26,273 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:26,307 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:26,439 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:50:26,769 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:26,770 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:26,774 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:50:26,774 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:50:26,774 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:50:26,774 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:50:26,774 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:50:26,806 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:50:26,821 INFO mapred.Task: Task:attempt_local371542726_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:26,826 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:50:26,826 INFO mapred.Task: Task 'attempt_local371542726_0001_m_000000_0' done.\n",
            "2021-05-07 06:50:26,836 INFO mapred.Task: Final Counters for attempt_local371542726_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:50:26,836 INFO mapred.LocalJobRunner: Finishing task: attempt_local371542726_0001_m_000000_0\n",
            "2021-05-07 06:50:26,836 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:50:26,840 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:50:26,845 INFO mapred.LocalJobRunner: Starting task: attempt_local371542726_0001_r_000000_0\n",
            "2021-05-07 06:50:26,857 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:26,857 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:26,858 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:26,866 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5a0b1078\n",
            "2021-05-07 06:50:26,868 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:26,908 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:50:26,925 INFO reduce.EventFetcher: attempt_local371542726_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:50:26,927 INFO mapreduce.Job: Job job_local371542726_0001 running in uber mode : false\n",
            "2021-05-07 06:50:26,930 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:50:26,967 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local371542726_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:50:26,977 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local371542726_0001_m_000000_0\n",
            "2021-05-07 06:50:26,981 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:50:26,983 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:50:26,985 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:26,985 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:50:26,993 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:26,993 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:27,020 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:50:27,021 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:50:27,022 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:50:27,022 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:27,023 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:27,024 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:27,039 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:50:27,045 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:50:27,047 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:50:27,074 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:27,074 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:27,075 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:27,082 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:27,377 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:50:27,394 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:27,395 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:27,396 INFO mapred.Task: Task:attempt_local371542726_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:27,397 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:27,397 INFO mapred.Task: Task attempt_local371542726_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:50:27,398 INFO output.FileOutputCommitter: Saved output of task 'attempt_local371542726_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:50:27,402 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:50:27,402 INFO mapred.Task: Task 'attempt_local371542726_0001_r_000000_0' done.\n",
            "2021-05-07 06:50:27,403 INFO mapred.Task: Final Counters for attempt_local371542726_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779364\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:27,403 INFO mapred.LocalJobRunner: Finishing task: attempt_local371542726_0001_r_000000_0\n",
            "2021-05-07 06:50:27,403 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:50:27,931 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:50:27,932 INFO mapreduce.Job: Job job_local371542726_0001 completed successfully\n",
            "2021-05-07 06:50:27,944 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475316\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:27,944 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:50:31,690 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+3\n",
            "2021-05-07 06:50:33,112 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob8050613024162008933.jar tmpDir=null\n",
            "2021-05-07 06:50:34,011 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:50:34,206 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:50:34,206 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:50:34,229 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:34,411 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:50:34,451 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:50:34,792 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local315346624_0001\n",
            "2021-05-07 06:50:34,792 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:50:35,227 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local315346624_0001_f3113048-ee9e-479c-9c3e-8077c2065238/centroids.txt\n",
            "2021-05-07 06:50:35,365 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:50:35,368 INFO mapreduce.Job: Running job: job_local315346624_0001\n",
            "2021-05-07 06:50:35,372 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:50:35,376 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:50:35,382 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:35,383 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:35,441 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:50:35,453 INFO mapred.LocalJobRunner: Starting task: attempt_local315346624_0001_m_000000_0\n",
            "2021-05-07 06:50:35,495 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:35,495 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:35,525 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:35,539 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:50:35,562 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:50:35,629 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:50:35,629 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:50:35,629 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:50:35,629 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:50:35,629 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:50:35,634 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:50:35,645 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:50:35,656 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:50:35,657 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:50:35,658 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:50:35,658 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:50:35,660 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:50:35,660 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:50:35,660 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:50:35,661 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:50:35,661 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:50:35,662 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:50:35,663 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:50:35,663 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:50:35,699 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:35,700 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:35,702 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:35,725 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:35,865 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:50:36,197 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:36,198 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:36,201 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:50:36,201 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:50:36,203 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:50:36,203 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:50:36,203 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:50:36,232 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:50:36,246 INFO mapred.Task: Task:attempt_local315346624_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:36,257 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:50:36,257 INFO mapred.Task: Task 'attempt_local315346624_0001_m_000000_0' done.\n",
            "2021-05-07 06:50:36,267 INFO mapred.Task: Final Counters for attempt_local315346624_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695950\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=322961408\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:50:36,267 INFO mapred.LocalJobRunner: Finishing task: attempt_local315346624_0001_m_000000_0\n",
            "2021-05-07 06:50:36,268 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:50:36,272 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:50:36,272 INFO mapred.LocalJobRunner: Starting task: attempt_local315346624_0001_r_000000_0\n",
            "2021-05-07 06:50:36,285 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:36,286 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:36,287 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:36,298 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7a1a7250\n",
            "2021-05-07 06:50:36,300 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:36,328 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:50:36,330 INFO reduce.EventFetcher: attempt_local315346624_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:50:36,378 INFO mapreduce.Job: Job job_local315346624_0001 running in uber mode : false\n",
            "2021-05-07 06:50:36,381 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:50:36,395 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local315346624_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:50:36,403 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local315346624_0001_m_000000_0\n",
            "2021-05-07 06:50:36,405 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:50:36,407 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:50:36,409 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:36,409 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:50:36,416 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:36,417 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:36,431 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:50:36,432 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:50:36,433 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:50:36,433 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:36,434 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:36,435 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:36,458 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:50:36,465 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:50:36,467 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:50:36,500 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:36,500 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:36,501 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:36,508 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:36,804 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:50:36,813 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:36,814 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:36,816 INFO mapred.Task: Task:attempt_local315346624_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:36,817 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:36,817 INFO mapred.Task: Task attempt_local315346624_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:50:36,820 INFO output.FileOutputCommitter: Saved output of task 'attempt_local315346624_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:50:36,821 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:50:36,821 INFO mapred.Task: Task 'attempt_local315346624_0001_r_000000_0' done.\n",
            "2021-05-07 06:50:36,822 INFO mapred.Task: Final Counters for attempt_local315346624_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779362\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=25\n",
            "\t\tTotal committed heap usage (bytes)=322961408\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:36,822 INFO mapred.LocalJobRunner: Finishing task: attempt_local315346624_0001_r_000000_0\n",
            "2021-05-07 06:50:36,822 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:50:37,384 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:50:37,385 INFO mapreduce.Job: Job job_local315346624_0001 completed successfully\n",
            "2021-05-07 06:50:37,402 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475312\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=25\n",
            "\t\tTotal committed heap usage (bytes)=645922816\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:37,402 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:50:40,998 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+4\n",
            "2021-05-07 06:50:42,330 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob3723302454295341933.jar tmpDir=null\n",
            "2021-05-07 06:50:43,104 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:50:43,257 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:50:43,258 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:50:43,286 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:43,474 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:50:43,515 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:50:43,864 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local390609818_0001\n",
            "2021-05-07 06:50:43,864 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:50:44,303 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local390609818_0001_c7920467-7640-40eb-8475-12dc21a7cce7/centroids.txt\n",
            "2021-05-07 06:50:44,402 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:50:44,404 INFO mapreduce.Job: Running job: job_local390609818_0001\n",
            "2021-05-07 06:50:44,410 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:50:44,413 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:50:44,419 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:44,419 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:44,485 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:50:44,489 INFO mapred.LocalJobRunner: Starting task: attempt_local390609818_0001_m_000000_0\n",
            "2021-05-07 06:50:44,521 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:44,524 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:44,554 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:44,566 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:50:44,593 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:50:44,668 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:50:44,668 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:50:44,668 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:50:44,668 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:50:44,668 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:50:44,671 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:50:44,685 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:50:44,693 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:50:44,694 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:50:44,695 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:50:44,695 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:50:44,696 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:50:44,696 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:50:44,696 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:50:44,697 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:50:44,697 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:50:44,698 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:50:44,698 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:50:44,698 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:50:44,730 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:44,730 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:44,732 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:44,745 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:44,909 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:50:45,237 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:45,238 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:45,243 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:50:45,243 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:50:45,243 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:50:45,243 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:50:45,243 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:50:45,276 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:50:45,289 INFO mapred.Task: Task:attempt_local390609818_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:45,292 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:50:45,292 INFO mapred.Task: Task 'attempt_local390609818_0001_m_000000_0' done.\n",
            "2021-05-07 06:50:45,302 INFO mapred.Task: Final Counters for attempt_local390609818_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695950\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=369098752\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:50:45,302 INFO mapred.LocalJobRunner: Finishing task: attempt_local390609818_0001_m_000000_0\n",
            "2021-05-07 06:50:45,303 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:50:45,306 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:50:45,306 INFO mapred.LocalJobRunner: Starting task: attempt_local390609818_0001_r_000000_0\n",
            "2021-05-07 06:50:45,315 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:45,315 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:45,315 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:45,323 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@551c6def\n",
            "2021-05-07 06:50:45,325 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:45,344 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:50:45,351 INFO reduce.EventFetcher: attempt_local390609818_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:50:45,396 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local390609818_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:50:45,408 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local390609818_0001_m_000000_0\n",
            "2021-05-07 06:50:45,408 INFO mapreduce.Job: Job job_local390609818_0001 running in uber mode : false\n",
            "2021-05-07 06:50:45,417 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:50:45,418 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:50:45,419 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:50:45,424 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:45,424 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:50:45,432 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:45,432 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:45,447 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:50:45,448 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:50:45,449 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:50:45,449 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:45,450 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:45,451 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:45,464 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:50:45,472 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:50:45,474 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:50:45,500 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:45,500 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:45,501 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:45,514 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:45,795 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:50:45,812 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:45,813 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:45,814 INFO mapred.Task: Task:attempt_local390609818_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:45,815 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:45,816 INFO mapred.Task: Task attempt_local390609818_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:50:45,818 INFO output.FileOutputCommitter: Saved output of task 'attempt_local390609818_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:50:45,820 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:50:45,820 INFO mapred.Task: Task 'attempt_local390609818_0001_r_000000_0' done.\n",
            "2021-05-07 06:50:45,821 INFO mapred.Task: Final Counters for attempt_local390609818_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779362\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=369098752\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:45,821 INFO mapred.LocalJobRunner: Finishing task: attempt_local390609818_0001_r_000000_0\n",
            "2021-05-07 06:50:45,822 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:50:46,423 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:50:46,424 INFO mapreduce.Job: Job job_local390609818_0001 completed successfully\n",
            "2021-05-07 06:50:46,439 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475312\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=738197504\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:46,439 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:50:50,006 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+5\n",
            "2021-05-07 06:50:51,250 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob8467548385475334333.jar tmpDir=null\n",
            "2021-05-07 06:50:52,074 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:50:52,335 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:50:52,336 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:50:52,359 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:52,494 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:50:52,515 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:50:52,837 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local127802085_0001\n",
            "2021-05-07 06:50:52,837 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:50:53,246 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local127802085_0001_cf6cbe79-c5c4-42e0-bf7d-563e5ca0511d/centroids.txt\n",
            "2021-05-07 06:50:53,368 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:50:53,370 INFO mapreduce.Job: Running job: job_local127802085_0001\n",
            "2021-05-07 06:50:53,381 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:50:53,386 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:50:53,397 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:53,397 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:53,447 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:50:53,457 INFO mapred.LocalJobRunner: Starting task: attempt_local127802085_0001_m_000000_0\n",
            "2021-05-07 06:50:53,507 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:53,509 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:53,539 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:53,554 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:50:53,590 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:50:53,663 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:50:53,663 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:50:53,664 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:50:53,664 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:50:53,664 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:50:53,666 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:50:53,679 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:50:53,691 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:50:53,692 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:50:53,692 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:50:53,693 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:50:53,694 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:50:53,694 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:50:53,694 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:50:53,695 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:50:53,695 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:50:53,696 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:50:53,696 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:50:53,696 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:50:53,732 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:53,732 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:53,734 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:53,744 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:53,882 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:50:54,201 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:54,202 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:54,206 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:50:54,206 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:50:54,206 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:50:54,206 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:50:54,206 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:50:54,249 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:50:54,266 INFO mapred.Task: Task:attempt_local127802085_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:54,270 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:50:54,270 INFO mapred.Task: Task 'attempt_local127802085_0001_m_000000_0' done.\n",
            "2021-05-07 06:50:54,278 INFO mapred.Task: Final Counters for attempt_local127802085_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695950\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:50:54,278 INFO mapred.LocalJobRunner: Finishing task: attempt_local127802085_0001_m_000000_0\n",
            "2021-05-07 06:50:54,278 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:50:54,282 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:50:54,283 INFO mapred.LocalJobRunner: Starting task: attempt_local127802085_0001_r_000000_0\n",
            "2021-05-07 06:50:54,302 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:50:54,304 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:50:54,304 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:50:54,307 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d26a927\n",
            "2021-05-07 06:50:54,309 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:50:54,333 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:50:54,347 INFO reduce.EventFetcher: attempt_local127802085_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:50:54,375 INFO mapreduce.Job: Job job_local127802085_0001 running in uber mode : false\n",
            "2021-05-07 06:50:54,378 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:50:54,393 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local127802085_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:50:54,402 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local127802085_0001_m_000000_0\n",
            "2021-05-07 06:50:54,405 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:50:54,407 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:50:54,408 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:54,408 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:50:54,416 WARN io.ReadaheadPool: Failed readahead on ifile\n",
            "EBADF: Bad file descriptor\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:419)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:296)\n",
            "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:209)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "2021-05-07 06:50:54,419 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:54,420 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:54,440 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:50:54,441 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:50:54,442 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:50:54,442 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:50:54,444 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:50:54,446 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:54,455 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:50:54,460 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:50:54,461 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:50:54,488 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:54,488 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:54,489 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:54,503 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:50:54,804 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:50:54,820 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:50:54,821 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:50:54,822 INFO mapred.Task: Task:attempt_local127802085_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:50:54,823 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:50:54,823 INFO mapred.Task: Task attempt_local127802085_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:50:54,825 INFO output.FileOutputCommitter: Saved output of task 'attempt_local127802085_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:50:54,827 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:50:54,828 INFO mapred.Task: Task 'attempt_local127802085_0001_r_000000_0' done.\n",
            "2021-05-07 06:50:54,829 INFO mapred.Task: Final Counters for attempt_local127802085_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779362\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:54,829 INFO mapred.LocalJobRunner: Finishing task: attempt_local127802085_0001_r_000000_0\n",
            "2021-05-07 06:50:54,829 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:50:55,380 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:50:55,380 INFO mapreduce.Job: Job job_local127802085_0001 completed successfully\n",
            "2021-05-07 06:50:55,393 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475312\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:50:55,394 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:50:59,039 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+6\n",
            "2021-05-07 06:51:00,376 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob4622579639726455437.jar tmpDir=null\n",
            "2021-05-07 06:51:01,182 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:01,361 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:01,361 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:01,397 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:01,539 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:01,561 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:01,862 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1853594758_0001\n",
            "2021-05-07 06:51:01,862 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:02,313 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1853594758_0001_a9ca3439-f521-4df7-a8ff-8612da427961/centroids.txt\n",
            "2021-05-07 06:51:02,426 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:02,428 INFO mapreduce.Job: Running job: job_local1853594758_0001\n",
            "2021-05-07 06:51:02,438 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:02,442 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:02,452 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:02,452 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:02,505 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:02,509 INFO mapred.LocalJobRunner: Starting task: attempt_local1853594758_0001_m_000000_0\n",
            "2021-05-07 06:51:02,548 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:02,551 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:02,588 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:02,599 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:02,618 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:02,694 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:02,694 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:02,694 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:02,694 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:02,694 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:02,708 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:02,733 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:02,742 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:02,742 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:02,743 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:02,743 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:02,744 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:02,744 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:02,744 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:02,744 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:02,745 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:02,745 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:02,745 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:02,746 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:02,777 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:02,777 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:02,779 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:02,794 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:02,925 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:03,244 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:03,245 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:03,248 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:03,248 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:03,248 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:03,248 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:03,248 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:03,287 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:03,310 INFO mapred.Task: Task:attempt_local1853594758_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:03,316 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:03,316 INFO mapred.Task: Task 'attempt_local1853594758_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:03,331 INFO mapred.Task: Final Counters for attempt_local1853594758_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698933\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:03,332 INFO mapred.LocalJobRunner: Finishing task: attempt_local1853594758_0001_m_000000_0\n",
            "2021-05-07 06:51:03,332 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:03,342 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:03,342 INFO mapred.LocalJobRunner: Starting task: attempt_local1853594758_0001_r_000000_0\n",
            "2021-05-07 06:51:03,361 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:03,362 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:03,362 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:03,365 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d26a927\n",
            "2021-05-07 06:51:03,368 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:03,411 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:03,424 INFO reduce.EventFetcher: attempt_local1853594758_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:03,436 INFO mapreduce.Job: Job job_local1853594758_0001 running in uber mode : false\n",
            "2021-05-07 06:51:03,439 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:03,458 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1853594758_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:03,462 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1853594758_0001_m_000000_0\n",
            "2021-05-07 06:51:03,466 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:03,467 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:03,470 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:03,470 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:03,477 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:03,477 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:03,497 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:03,498 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:03,499 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:03,499 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:03,501 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:03,502 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:03,515 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:03,523 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:03,525 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:03,553 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:03,553 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:03,554 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:03,569 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:03,888 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:03,905 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:03,906 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:03,908 INFO mapred.Task: Task:attempt_local1853594758_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:03,910 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:03,910 INFO mapred.Task: Task attempt_local1853594758_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:03,913 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1853594758_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:03,914 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:03,914 INFO mapred.Task: Task 'attempt_local1853594758_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:03,915 INFO mapred.Task: Final Counters for attempt_local1853594758_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782345\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:03,916 INFO mapred.LocalJobRunner: Finishing task: attempt_local1853594758_0001_r_000000_0\n",
            "2021-05-07 06:51:03,916 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:04,440 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:04,440 INFO mapreduce.Job: Job job_local1853594758_0001 completed successfully\n",
            "2021-05-07 06:51:04,454 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481278\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=708837376\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:04,455 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:08,076 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+7\n",
            "2021-05-07 06:51:09,422 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob12143504649621394025.jar tmpDir=null\n",
            "2021-05-07 06:51:10,147 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:10,299 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:10,300 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:10,327 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:10,512 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:10,546 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:10,895 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1429010781_0001\n",
            "2021-05-07 06:51:10,895 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:11,362 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1429010781_0001_096e89b2-06e9-461e-b56b-dcb8b2d9a4ac/centroids.txt\n",
            "2021-05-07 06:51:11,472 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:11,474 INFO mapreduce.Job: Running job: job_local1429010781_0001\n",
            "2021-05-07 06:51:11,486 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:11,490 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:11,497 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:11,497 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:11,582 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:11,588 INFO mapred.LocalJobRunner: Starting task: attempt_local1429010781_0001_m_000000_0\n",
            "2021-05-07 06:51:11,620 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:11,622 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:11,662 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:11,677 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:11,713 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:11,780 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:11,780 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:11,780 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:11,780 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:11,780 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:11,783 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:11,805 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:11,813 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:11,814 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:11,815 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:11,815 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:11,816 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:11,816 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:11,817 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:11,817 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:11,818 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:11,819 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:11,819 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:11,820 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:11,855 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:11,855 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:11,857 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:11,871 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:12,010 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:12,328 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:12,329 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:12,332 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:12,332 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:12,332 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:12,332 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:12,332 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:12,378 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:12,390 INFO mapred.Task: Task:attempt_local1429010781_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:12,393 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:12,393 INFO mapred.Task: Task 'attempt_local1429010781_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:12,402 INFO mapred.Task: Final Counters for attempt_local1429010781_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698935\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:12,402 INFO mapred.LocalJobRunner: Finishing task: attempt_local1429010781_0001_m_000000_0\n",
            "2021-05-07 06:51:12,402 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:12,406 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:12,410 INFO mapred.LocalJobRunner: Starting task: attempt_local1429010781_0001_r_000000_0\n",
            "2021-05-07 06:51:12,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:12,435 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:12,436 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:12,439 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1359f6f6\n",
            "2021-05-07 06:51:12,449 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:12,465 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:12,467 INFO reduce.EventFetcher: attempt_local1429010781_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:12,481 INFO mapreduce.Job: Job job_local1429010781_0001 running in uber mode : false\n",
            "2021-05-07 06:51:12,484 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:12,509 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1429010781_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:12,513 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1429010781_0001_m_000000_0\n",
            "2021-05-07 06:51:12,517 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:12,518 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:12,523 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:12,523 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:12,530 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:12,531 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:12,557 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:12,557 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:12,558 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:12,558 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:12,559 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:12,560 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:12,581 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:12,591 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:12,592 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:12,605 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:12,605 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:12,606 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:12,614 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:12,908 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:12,927 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:12,927 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:12,929 INFO mapred.Task: Task:attempt_local1429010781_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:12,930 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:12,931 INFO mapred.Task: Task attempt_local1429010781_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:12,935 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1429010781_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:12,937 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:12,937 INFO mapred.Task: Task 'attempt_local1429010781_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:12,938 INFO mapred.Task: Final Counters for attempt_local1429010781_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782347\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:12,939 INFO mapred.LocalJobRunner: Finishing task: attempt_local1429010781_0001_r_000000_0\n",
            "2021-05-07 06:51:12,939 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:13,485 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:13,486 INFO mapreduce.Job: Job job_local1429010781_0001 completed successfully\n",
            "2021-05-07 06:51:13,498 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481282\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=725614592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:13,499 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:17,039 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+8\n",
            "2021-05-07 06:51:18,214 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob7364726789615447743.jar tmpDir=null\n",
            "2021-05-07 06:51:19,031 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:19,233 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:19,233 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:19,261 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:19,402 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:19,424 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:19,777 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1857735461_0001\n",
            "2021-05-07 06:51:19,777 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:20,193 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1857735461_0001_86cf5308-f830-459e-96f1-bee32ab65fe5/centroids.txt\n",
            "2021-05-07 06:51:20,308 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:20,310 INFO mapreduce.Job: Running job: job_local1857735461_0001\n",
            "2021-05-07 06:51:20,320 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:20,322 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:20,327 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:20,327 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:20,374 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:20,377 INFO mapred.LocalJobRunner: Starting task: attempt_local1857735461_0001_m_000000_0\n",
            "2021-05-07 06:51:20,407 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:20,408 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:20,447 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:20,461 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:20,486 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:20,559 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:20,559 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:20,559 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:20,559 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:20,559 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:20,562 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:20,577 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:20,586 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:20,587 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:20,587 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:20,588 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:20,589 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:20,589 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:20,589 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:20,590 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:20,590 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:20,591 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:20,591 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:20,592 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:20,623 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:20,623 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:20,625 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:20,637 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:20,789 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:21,099 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:21,100 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:21,103 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:21,103 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:21,104 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:21,104 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:21,104 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:21,147 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:21,165 INFO mapred.Task: Task:attempt_local1857735461_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:21,168 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:21,168 INFO mapred.Task: Task 'attempt_local1857735461_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:21,177 INFO mapred.Task: Final Counters for attempt_local1857735461_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698933\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:21,177 INFO mapred.LocalJobRunner: Finishing task: attempt_local1857735461_0001_m_000000_0\n",
            "2021-05-07 06:51:21,177 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:21,183 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:21,184 INFO mapred.LocalJobRunner: Starting task: attempt_local1857735461_0001_r_000000_0\n",
            "2021-05-07 06:51:21,194 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:21,194 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:21,194 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:21,197 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@205939cb\n",
            "2021-05-07 06:51:21,202 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:21,232 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:21,245 INFO reduce.EventFetcher: attempt_local1857735461_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:21,278 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1857735461_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:21,285 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1857735461_0001_m_000000_0\n",
            "2021-05-07 06:51:21,286 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:21,291 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:21,293 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:21,293 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:21,299 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:21,299 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:21,320 INFO mapreduce.Job: Job job_local1857735461_0001 running in uber mode : false\n",
            "2021-05-07 06:51:21,332 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:21,333 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:21,333 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:21,334 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:21,334 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:21,335 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:21,336 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:21,345 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:21,354 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:21,357 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:21,373 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:21,373 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:21,374 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:21,393 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:21,651 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:21,668 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:21,669 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:21,670 INFO mapred.Task: Task:attempt_local1857735461_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:21,671 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:21,671 INFO mapred.Task: Task attempt_local1857735461_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:21,679 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1857735461_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:21,680 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:21,681 INFO mapred.Task: Task 'attempt_local1857735461_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:21,682 INFO mapred.Task: Final Counters for attempt_local1857735461_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782345\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:21,682 INFO mapred.LocalJobRunner: Finishing task: attempt_local1857735461_0001_r_000000_0\n",
            "2021-05-07 06:51:21,682 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:22,334 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:22,335 INFO mapreduce.Job: Job job_local1857735461_0001 completed successfully\n",
            "2021-05-07 06:51:22,353 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481278\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=650117120\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:22,353 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:26,016 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+9\n",
            "2021-05-07 06:51:27,344 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob2526185074039163642.jar tmpDir=null\n",
            "2021-05-07 06:51:28,220 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:28,452 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:28,452 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:28,473 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:28,609 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:28,629 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:28,977 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1925529253_0001\n",
            "2021-05-07 06:51:28,977 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:29,434 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1925529253_0001_a6ce14d1-4776-482d-a244-bbc3a61e5187/centroids.txt\n",
            "2021-05-07 06:51:29,540 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:29,542 INFO mapreduce.Job: Running job: job_local1925529253_0001\n",
            "2021-05-07 06:51:29,551 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:29,554 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:29,559 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:29,559 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:29,609 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:29,613 INFO mapred.LocalJobRunner: Starting task: attempt_local1925529253_0001_m_000000_0\n",
            "2021-05-07 06:51:29,646 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:29,649 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:29,685 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:29,694 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:29,715 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:29,785 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:29,785 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:29,785 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:29,785 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:29,785 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:29,789 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:29,801 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:29,812 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:29,813 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:29,813 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:29,814 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:29,815 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:29,815 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:29,816 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:29,816 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:29,816 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:29,818 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:29,818 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:29,818 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:29,852 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:29,853 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:29,854 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:29,870 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:30,018 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:30,340 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:30,342 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:30,344 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:30,344 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:30,345 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:30,345 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:30,345 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:30,377 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:30,392 INFO mapred.Task: Task:attempt_local1925529253_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:30,395 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:30,396 INFO mapred.Task: Task 'attempt_local1925529253_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:30,409 INFO mapred.Task: Final Counters for attempt_local1925529253_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698933\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=334495744\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:30,409 INFO mapred.LocalJobRunner: Finishing task: attempt_local1925529253_0001_m_000000_0\n",
            "2021-05-07 06:51:30,410 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:30,420 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:30,427 INFO mapred.LocalJobRunner: Starting task: attempt_local1925529253_0001_r_000000_0\n",
            "2021-05-07 06:51:30,440 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:30,440 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:30,440 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:30,446 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d26a927\n",
            "2021-05-07 06:51:30,448 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:30,468 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:30,487 INFO reduce.EventFetcher: attempt_local1925529253_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:30,520 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1925529253_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:30,524 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1925529253_0001_m_000000_0\n",
            "2021-05-07 06:51:30,526 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:30,529 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:30,530 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:30,531 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:30,539 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:30,540 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:30,550 INFO mapreduce.Job: Job job_local1925529253_0001 running in uber mode : false\n",
            "2021-05-07 06:51:30,551 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:30,556 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:30,557 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:30,557 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:30,557 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:30,558 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:30,559 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:30,588 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:30,594 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:30,597 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:30,622 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:30,622 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:30,623 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:30,635 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:30,914 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:30,931 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:30,932 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:30,933 INFO mapred.Task: Task:attempt_local1925529253_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:30,935 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:30,935 INFO mapred.Task: Task attempt_local1925529253_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:30,937 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1925529253_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:30,938 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:30,938 INFO mapred.Task: Task 'attempt_local1925529253_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:30,939 INFO mapred.Task: Final Counters for attempt_local1925529253_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782345\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=334495744\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:30,939 INFO mapred.LocalJobRunner: Finishing task: attempt_local1925529253_0001_r_000000_0\n",
            "2021-05-07 06:51:30,939 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:31,553 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:31,553 INFO mapreduce.Job: Job job_local1925529253_0001 completed successfully\n",
            "2021-05-07 06:51:31,564 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481278\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=668991488\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:31,564 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:35,112 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+10\n",
            "2021-05-07 06:51:36,295 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob11183899267610652781.jar tmpDir=null\n",
            "2021-05-07 06:51:37,058 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:37,240 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:37,240 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:37,267 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:37,450 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:37,489 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:37,824 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local450507373_0001\n",
            "2021-05-07 06:51:37,824 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:38,286 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local450507373_0001_d5cd43be-93a7-4cab-b183-e5013e41b8bf/centroids.txt\n",
            "2021-05-07 06:51:38,409 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:38,411 INFO mapreduce.Job: Running job: job_local450507373_0001\n",
            "2021-05-07 06:51:38,418 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:38,420 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:38,430 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:38,430 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:38,490 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:38,497 INFO mapred.LocalJobRunner: Starting task: attempt_local450507373_0001_m_000000_0\n",
            "2021-05-07 06:51:38,564 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:38,567 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:38,612 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:38,621 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:38,639 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:38,722 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:38,722 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:38,722 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:38,722 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:38,722 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:38,725 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:38,738 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:38,747 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:38,747 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:38,747 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:38,748 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:38,749 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:38,749 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:38,749 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:38,749 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:38,750 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:38,750 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:38,751 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:38,751 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:38,784 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:38,784 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:38,786 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:38,795 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:38,940 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:39,278 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:39,278 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:39,281 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:39,281 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:39,282 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:39,282 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:39,282 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:39,313 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:39,336 INFO mapred.Task: Task:attempt_local450507373_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:39,343 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:39,344 INFO mapred.Task: Task 'attempt_local450507373_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:39,352 INFO mapred.Task: Final Counters for attempt_local450507373_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=374341632\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:39,352 INFO mapred.LocalJobRunner: Finishing task: attempt_local450507373_0001_m_000000_0\n",
            "2021-05-07 06:51:39,353 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:39,362 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:39,362 INFO mapred.LocalJobRunner: Starting task: attempt_local450507373_0001_r_000000_0\n",
            "2021-05-07 06:51:39,373 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:39,374 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:39,375 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:39,382 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72455177\n",
            "2021-05-07 06:51:39,388 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:39,416 INFO mapreduce.Job: Job job_local450507373_0001 running in uber mode : false\n",
            "2021-05-07 06:51:39,420 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:39,423 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:39,432 INFO reduce.EventFetcher: attempt_local450507373_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:39,484 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local450507373_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:39,493 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local450507373_0001_m_000000_0\n",
            "2021-05-07 06:51:39,498 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:39,500 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:39,504 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:39,504 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:39,511 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:39,511 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:39,526 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:39,527 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:39,533 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:39,533 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:39,534 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:39,535 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:39,550 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:39,557 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:39,559 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:39,586 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:39,586 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:39,587 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:39,594 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:39,856 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:39,873 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:39,874 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:39,874 INFO mapred.Task: Task:attempt_local450507373_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:39,875 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:39,876 INFO mapred.Task: Task attempt_local450507373_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:39,877 INFO output.FileOutputCommitter: Saved output of task 'attempt_local450507373_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:39,881 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:39,881 INFO mapred.Task: Task 'attempt_local450507373_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:39,882 INFO mapred.Task: Final Counters for attempt_local450507373_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779364\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=374341632\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:39,883 INFO mapred.LocalJobRunner: Finishing task: attempt_local450507373_0001_r_000000_0\n",
            "2021-05-07 06:51:39,883 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:40,423 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:40,424 INFO mapreduce.Job: Job job_local450507373_0001 completed successfully\n",
            "2021-05-07 06:51:40,438 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475316\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=748683264\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:40,439 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:44,243 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+11\n",
            "2021-05-07 06:51:45,597 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob17741446990810002792.jar tmpDir=null\n",
            "2021-05-07 06:51:46,464 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:46,657 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:46,658 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:46,684 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:46,823 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:46,844 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:47,231 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local744477020_0001\n",
            "2021-05-07 06:51:47,231 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:47,681 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local744477020_0001_d8ddb21d-61ce-4115-a26e-caa0f94e06bb/centroids.txt\n",
            "2021-05-07 06:51:47,823 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:47,825 INFO mapreduce.Job: Running job: job_local744477020_0001\n",
            "2021-05-07 06:51:47,833 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:47,835 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:47,845 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:47,845 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:47,898 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:47,904 INFO mapred.LocalJobRunner: Starting task: attempt_local744477020_0001_m_000000_0\n",
            "2021-05-07 06:51:47,941 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:47,943 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:47,974 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:47,983 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:48,004 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:48,075 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:48,075 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:48,075 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:48,075 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:48,075 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:48,078 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:48,087 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:48,100 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:48,101 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:48,102 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:48,103 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:48,104 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:48,104 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:48,104 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:48,105 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:48,105 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:48,107 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:48,107 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:48,107 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:48,139 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,140 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,142 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,151 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,309 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:48,630 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:48,630 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:48,633 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:48,633 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:48,633 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:48,634 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:48,634 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:48,670 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:48,700 INFO mapred.Task: Task:attempt_local744477020_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:48,707 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:48,707 INFO mapred.Task: Task 'attempt_local744477020_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:48,717 INFO mapred.Task: Final Counters for attempt_local744477020_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:48,718 INFO mapred.LocalJobRunner: Finishing task: attempt_local744477020_0001_m_000000_0\n",
            "2021-05-07 06:51:48,718 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:48,722 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:48,722 INFO mapred.LocalJobRunner: Starting task: attempt_local744477020_0001_r_000000_0\n",
            "2021-05-07 06:51:48,731 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:48,731 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:48,732 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:48,735 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@53d969bc\n",
            "2021-05-07 06:51:48,739 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:48,765 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:48,767 INFO reduce.EventFetcher: attempt_local744477020_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:48,822 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local744477020_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:48,830 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local744477020_0001_m_000000_0\n",
            "2021-05-07 06:51:48,834 INFO mapreduce.Job: Job job_local744477020_0001 running in uber mode : false\n",
            "2021-05-07 06:51:48,836 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:48,839 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:48,845 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:48,848 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:48,849 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:48,857 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:48,857 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:48,883 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:48,884 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:48,885 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:48,885 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:48,886 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:48,886 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:48,911 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:48,918 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:48,920 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:48,943 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,943 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,944 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:48,953 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:49,229 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:49,252 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:49,252 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:49,253 INFO mapred.Task: Task:attempt_local744477020_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:49,254 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:49,255 INFO mapred.Task: Task attempt_local744477020_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:49,257 INFO output.FileOutputCommitter: Saved output of task 'attempt_local744477020_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:49,259 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:49,259 INFO mapred.Task: Task 'attempt_local744477020_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:49,260 INFO mapred.Task: Final Counters for attempt_local744477020_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779364\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:49,260 INFO mapred.LocalJobRunner: Finishing task: attempt_local744477020_0001_r_000000_0\n",
            "2021-05-07 06:51:49,260 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:49,840 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:49,841 INFO mapreduce.Job: Job job_local744477020_0001 completed successfully\n",
            "2021-05-07 06:51:49,857 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475316\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=696254464\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:49,857 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:51:53,650 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+12\n",
            "2021-05-07 06:51:54,872 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob15655578337330355935.jar tmpDir=null\n",
            "2021-05-07 06:51:55,662 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:51:55,801 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:51:55,801 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:51:55,824 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:55,957 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:51:55,984 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:51:56,339 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1897581299_0001\n",
            "2021-05-07 06:51:56,339 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:51:56,840 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1897581299_0001_06707924-7097-4667-b6d8-8e3beef87dd1/centroids.txt\n",
            "2021-05-07 06:51:56,948 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:51:56,950 INFO mapreduce.Job: Running job: job_local1897581299_0001\n",
            "2021-05-07 06:51:56,957 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:51:56,960 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:51:56,970 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:56,970 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:57,021 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:51:57,025 INFO mapred.LocalJobRunner: Starting task: attempt_local1897581299_0001_m_000000_0\n",
            "2021-05-07 06:51:57,060 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:57,061 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:57,099 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:57,120 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:51:57,146 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:51:57,216 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:51:57,216 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:51:57,216 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:51:57,216 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:51:57,217 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:51:57,220 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:51:57,237 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:51:57,249 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:51:57,252 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:51:57,252 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:51:57,253 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:51:57,255 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:51:57,255 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:51:57,255 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:51:57,256 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:51:57,256 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:51:57,257 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:51:57,257 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:51:57,257 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:51:57,290 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:57,290 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:57,293 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:57,321 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:57,465 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:51:57,788 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:57,789 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:57,792 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:51:57,792 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:51:57,792 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:51:57,792 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:51:57,793 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:51:57,825 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:51:57,839 INFO mapred.Task: Task:attempt_local1897581299_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:57,842 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:51:57,842 INFO mapred.Task: Task 'attempt_local1897581299_0001_m_000000_0' done.\n",
            "2021-05-07 06:51:57,851 INFO mapred.Task: Final Counters for attempt_local1897581299_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698935\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:51:57,852 INFO mapred.LocalJobRunner: Finishing task: attempt_local1897581299_0001_m_000000_0\n",
            "2021-05-07 06:51:57,852 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:51:57,856 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:51:57,861 INFO mapred.LocalJobRunner: Starting task: attempt_local1897581299_0001_r_000000_0\n",
            "2021-05-07 06:51:57,873 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:51:57,873 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:51:57,873 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:51:57,882 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5c45da0c\n",
            "2021-05-07 06:51:57,888 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:51:57,916 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:51:57,932 INFO reduce.EventFetcher: attempt_local1897581299_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:51:57,955 INFO mapreduce.Job: Job job_local1897581299_0001 running in uber mode : false\n",
            "2021-05-07 06:51:57,958 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:51:57,969 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1897581299_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:51:57,977 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1897581299_0001_m_000000_0\n",
            "2021-05-07 06:51:57,978 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:51:57,980 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:51:57,981 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:57,982 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:51:57,988 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:57,989 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:58,004 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:51:58,005 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:51:58,006 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:51:58,006 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:51:58,007 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:51:58,008 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:58,020 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:51:58,025 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:51:58,028 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:51:58,074 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:58,074 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:58,077 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:58,090 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:51:58,354 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:51:58,370 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:51:58,371 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:51:58,372 INFO mapred.Task: Task:attempt_local1897581299_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:51:58,374 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:51:58,374 INFO mapred.Task: Task attempt_local1897581299_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:51:58,376 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1897581299_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:51:58,384 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:51:58,384 INFO mapred.Task: Task 'attempt_local1897581299_0001_r_000000_0' done.\n",
            "2021-05-07 06:51:58,385 INFO mapred.Task: Final Counters for attempt_local1897581299_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782347\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=351272960\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:58,385 INFO mapred.LocalJobRunner: Finishing task: attempt_local1897581299_0001_r_000000_0\n",
            "2021-05-07 06:51:58,385 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:51:58,959 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:51:58,960 INFO mapreduce.Job: Job job_local1897581299_0001 completed successfully\n",
            "2021-05-07 06:51:58,977 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481282\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=702545920\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:51:58,977 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:52:02,543 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+13\n",
            "2021-05-07 06:52:03,811 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob14472360820949456949.jar tmpDir=null\n",
            "2021-05-07 06:52:04,769 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:52:05,024 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:52:05,025 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:52:05,048 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:05,211 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:52:05,240 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:52:05,585 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local855258237_0001\n",
            "2021-05-07 06:52:05,585 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:52:06,021 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local855258237_0001_cf646848-dd91-4a16-8575-51ba5ccb9ebb/centroids.txt\n",
            "2021-05-07 06:52:06,124 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:52:06,127 INFO mapreduce.Job: Running job: job_local855258237_0001\n",
            "2021-05-07 06:52:06,136 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:52:06,140 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:52:06,150 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:06,151 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:06,239 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:52:06,244 INFO mapred.LocalJobRunner: Starting task: attempt_local855258237_0001_m_000000_0\n",
            "2021-05-07 06:52:06,301 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:06,302 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:06,351 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:06,382 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:52:06,413 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:52:06,474 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:52:06,474 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:52:06,475 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:52:06,475 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:52:06,475 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:52:06,477 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:52:06,486 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:52:06,497 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:52:06,499 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:52:06,501 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:52:06,502 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:52:06,503 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:52:06,503 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:52:06,503 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:52:06,504 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:52:06,504 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:52:06,505 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:52:06,505 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:52:06,506 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:52:06,536 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:06,537 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:06,539 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:06,563 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:06,710 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:52:07,029 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:07,030 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:07,034 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:52:07,034 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:52:07,034 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:52:07,034 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:52:07,034 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:52:07,068 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:52:07,081 INFO mapred.Task: Task:attempt_local855258237_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:07,084 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:52:07,084 INFO mapred.Task: Task 'attempt_local855258237_0001_m_000000_0' done.\n",
            "2021-05-07 06:52:07,093 INFO mapred.Task: Final Counters for attempt_local855258237_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:52:07,094 INFO mapred.LocalJobRunner: Finishing task: attempt_local855258237_0001_m_000000_0\n",
            "2021-05-07 06:52:07,094 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:52:07,098 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:52:07,099 INFO mapred.LocalJobRunner: Starting task: attempt_local855258237_0001_r_000000_0\n",
            "2021-05-07 06:52:07,110 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:07,110 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:07,111 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:07,117 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c7b5a77\n",
            "2021-05-07 06:52:07,119 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:07,137 INFO mapreduce.Job: Job job_local855258237_0001 running in uber mode : false\n",
            "2021-05-07 06:52:07,140 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:52:07,145 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:52:07,151 INFO reduce.EventFetcher: attempt_local855258237_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:52:07,193 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local855258237_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:52:07,197 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local855258237_0001_m_000000_0\n",
            "2021-05-07 06:52:07,201 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:52:07,203 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:52:07,208 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:07,208 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:52:07,215 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:07,216 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:07,238 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:52:07,239 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:52:07,240 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:52:07,240 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:07,241 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:07,242 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:07,255 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:52:07,260 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:52:07,262 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:52:07,293 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:07,294 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:07,296 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:07,318 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:07,600 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:52:07,614 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:07,615 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:07,616 INFO mapred.Task: Task:attempt_local855258237_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:07,618 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:07,618 INFO mapred.Task: Task attempt_local855258237_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:52:07,620 INFO output.FileOutputCommitter: Saved output of task 'attempt_local855258237_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:52:07,621 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:52:07,621 INFO mapred.Task: Task 'attempt_local855258237_0001_r_000000_0' done.\n",
            "2021-05-07 06:52:07,622 INFO mapred.Task: Final Counters for attempt_local855258237_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779364\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:07,623 INFO mapred.LocalJobRunner: Finishing task: attempt_local855258237_0001_r_000000_0\n",
            "2021-05-07 06:52:07,623 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:52:08,142 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:52:08,143 INFO mapreduce.Job: Job job_local855258237_0001 completed successfully\n",
            "2021-05-07 06:52:08,156 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475316\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=729808896\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:08,156 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:52:11,709 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+14\n",
            "2021-05-07 06:52:13,015 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob15176636556681987245.jar tmpDir=null\n",
            "2021-05-07 06:52:13,838 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:52:14,014 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:52:14,015 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:52:14,038 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:14,178 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:52:14,198 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:52:14,497 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1884526549_0001\n",
            "2021-05-07 06:52:14,497 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:52:14,913 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1884526549_0001_a51d1da1-4948-48ef-9e83-e51cb7738fa9/centroids.txt\n",
            "2021-05-07 06:52:15,013 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:52:15,015 INFO mapreduce.Job: Running job: job_local1884526549_0001\n",
            "2021-05-07 06:52:15,023 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:52:15,031 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:52:15,036 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:15,036 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:15,081 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:52:15,088 INFO mapred.LocalJobRunner: Starting task: attempt_local1884526549_0001_m_000000_0\n",
            "2021-05-07 06:52:15,121 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:15,125 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:15,165 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:15,174 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:52:15,191 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:52:15,264 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:52:15,264 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:52:15,264 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:52:15,265 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:52:15,265 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:52:15,268 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:52:15,292 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:52:15,304 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:52:15,310 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:52:15,311 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:52:15,311 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:52:15,312 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:52:15,313 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:52:15,314 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:52:15,315 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:52:15,316 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:52:15,317 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:52:15,317 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:52:15,317 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:52:15,360 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:15,360 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:15,362 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:15,374 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:15,513 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:52:15,841 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:15,842 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:15,845 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:52:15,845 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:52:15,845 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:52:15,845 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:52:15,845 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:52:15,880 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:52:15,903 INFO mapred.Task: Task:attempt_local1884526549_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:15,911 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:52:15,912 INFO mapred.Task: Task 'attempt_local1884526549_0001_m_000000_0' done.\n",
            "2021-05-07 06:52:15,921 INFO mapred.Task: Final Counters for attempt_local1884526549_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=698935\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:52:15,921 INFO mapred.LocalJobRunner: Finishing task: attempt_local1884526549_0001_m_000000_0\n",
            "2021-05-07 06:52:15,921 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:52:15,925 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:52:15,928 INFO mapred.LocalJobRunner: Starting task: attempt_local1884526549_0001_r_000000_0\n",
            "2021-05-07 06:52:15,939 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:15,939 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:15,940 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:15,945 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2cb75f1b\n",
            "2021-05-07 06:52:15,947 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:15,972 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:52:15,974 INFO reduce.EventFetcher: attempt_local1884526549_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:52:16,021 INFO mapreduce.Job: Job job_local1884526549_0001 running in uber mode : false\n",
            "2021-05-07 06:52:16,024 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:52:16,055 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1884526549_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:52:16,064 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1884526549_0001_m_000000_0\n",
            "2021-05-07 06:52:16,069 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:52:16,071 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:52:16,072 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:16,073 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:52:16,078 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:16,078 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:16,097 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:52:16,098 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:52:16,099 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:52:16,099 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:16,100 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:16,101 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:16,111 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:52:16,116 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:52:16,118 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:52:16,159 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:16,159 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:16,160 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:16,173 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:16,451 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:52:16,469 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:16,470 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:16,471 INFO mapred.Task: Task:attempt_local1884526549_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:16,472 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:16,472 INFO mapred.Task: Task attempt_local1884526549_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:52:16,474 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1884526549_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:52:16,475 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:52:16,475 INFO mapred.Task: Task 'attempt_local1884526549_0001_r_000000_0' done.\n",
            "2021-05-07 06:52:16,476 INFO mapred.Task: Final Counters for attempt_local1884526549_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=782347\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:16,476 INFO mapred.LocalJobRunner: Finishing task: attempt_local1884526549_0001_r_000000_0\n",
            "2021-05-07 06:52:16,477 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:52:17,025 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:52:17,026 INFO mapreduce.Job: Job job_local1884526549_0001 completed successfully\n",
            "2021-05-07 06:52:17,039 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1481282\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=679477248\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:17,039 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:52:20,764 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+15\n",
            "2021-05-07 06:52:22,059 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16127157963821496539.jar tmpDir=null\n",
            "2021-05-07 06:52:22,922 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:52:23,156 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:52:23,157 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:52:23,181 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:23,382 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:52:23,427 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:52:23,736 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local333160766_0001\n",
            "2021-05-07 06:52:23,737 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:52:24,147 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local333160766_0001_17b4fbf6-ec47-40ee-a047-29b23b33e848/centroids.txt\n",
            "2021-05-07 06:52:24,256 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:52:24,258 INFO mapreduce.Job: Running job: job_local333160766_0001\n",
            "2021-05-07 06:52:24,264 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:52:24,266 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:52:24,271 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:24,272 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:24,325 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:52:24,334 INFO mapred.LocalJobRunner: Starting task: attempt_local333160766_0001_m_000000_0\n",
            "2021-05-07 06:52:24,378 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:24,381 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:24,432 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:24,444 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:52:24,469 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:52:24,534 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:52:24,534 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:52:24,534 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:52:24,534 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:52:24,534 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:52:24,537 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:52:24,546 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:52:24,561 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:52:24,562 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:52:24,563 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:52:24,563 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:52:24,564 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:52:24,564 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:52:24,565 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:52:24,565 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:52:24,565 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:52:24,566 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:52:24,567 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:52:24,567 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:52:24,595 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:24,596 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:24,598 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:24,621 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:24,760 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:52:25,083 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:25,084 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:25,088 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:52:25,089 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:52:25,089 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:52:25,089 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:52:25,089 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:52:25,116 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:52:25,129 INFO mapred.Task: Task:attempt_local333160766_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:25,132 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:52:25,132 INFO mapred.Task: Task 'attempt_local333160766_0001_m_000000_0' done.\n",
            "2021-05-07 06:52:25,140 INFO mapred.Task: Final Counters for attempt_local333160766_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70837\n",
            "\t\tFILE: Number of bytes written=695952\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=355467264\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:52:25,141 INFO mapred.LocalJobRunner: Finishing task: attempt_local333160766_0001_m_000000_0\n",
            "2021-05-07 06:52:25,141 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:52:25,145 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:52:25,146 INFO mapred.LocalJobRunner: Starting task: attempt_local333160766_0001_r_000000_0\n",
            "2021-05-07 06:52:25,155 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:25,155 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:25,155 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:25,158 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7833e225\n",
            "2021-05-07 06:52:25,162 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:25,183 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:52:25,187 INFO reduce.EventFetcher: attempt_local333160766_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:52:25,223 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local333160766_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:52:25,227 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local333160766_0001_m_000000_0\n",
            "2021-05-07 06:52:25,231 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:52:25,236 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:52:25,237 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:25,237 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:52:25,246 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:25,246 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:25,262 INFO mapreduce.Job: Job job_local333160766_0001 running in uber mode : false\n",
            "2021-05-07 06:52:25,264 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:52:25,279 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:52:25,280 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:52:25,280 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:52:25,281 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:25,282 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:25,283 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:25,298 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 06:52:25,302 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:52:25,303 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:52:25,344 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:25,344 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:25,345 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:25,357 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:25,623 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:52:25,636 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:25,636 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:25,637 INFO mapred.Task: Task:attempt_local333160766_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:25,639 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:25,639 INFO mapred.Task: Task attempt_local333160766_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:52:25,641 INFO output.FileOutputCommitter: Saved output of task 'attempt_local333160766_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 06:52:25,642 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:52:25,642 INFO mapred.Task: Task 'attempt_local333160766_0001_r_000000_0' done.\n",
            "2021-05-07 06:52:25,643 INFO mapred.Task: Final Counters for attempt_local333160766_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235801\n",
            "\t\tFILE: Number of bytes written=779364\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=355467264\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:25,643 INFO mapred.LocalJobRunner: Finishing task: attempt_local333160766_0001_r_000000_0\n",
            "2021-05-07 06:52:25,644 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:52:26,265 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:52:26,266 INFO mapreduce.Job: Job job_local333160766_0001 completed successfully\n",
            "2021-05-07 06:52:26,279 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306638\n",
            "\t\tFILE: Number of bytes written=1475316\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=710934528\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=946\n",
            "2021-05-07 06:52:26,279 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 06:52:30,029 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "First Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 06:52:31,236 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids.txt] [] /tmp/streamjob4470602542323624840.jar tmpDir=null\n",
            "2021-05-07 06:52:32,021 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:52:32,182 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:52:32,183 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:52:32,208 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:32,390 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:52:32,424 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:52:32,783 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1930139086_0001\n",
            "2021-05-07 06:52:32,783 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:52:33,246 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1930139086_0001_d06cb3c9-10b7-4a4f-81b8-8b9da85c55bd/centroids.txt\n",
            "2021-05-07 06:52:33,442 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:52:33,444 INFO mapreduce.Job: Running job: job_local1930139086_0001\n",
            "2021-05-07 06:52:33,454 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:52:33,458 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:52:33,469 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:33,470 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:33,526 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:52:33,533 INFO mapred.LocalJobRunner: Starting task: attempt_local1930139086_0001_m_000000_0\n",
            "2021-05-07 06:52:33,577 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:33,580 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:33,606 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:33,617 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+69223\n",
            "2021-05-07 06:52:33,635 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:52:33,698 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:52:33,698 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:52:33,698 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:52:33,698 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:52:33,698 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:52:33,702 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:52:33,714 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 06:52:33,729 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:52:33,730 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:52:33,730 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:52:33,731 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:52:33,732 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:52:33,732 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:52:33,732 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:52:33,732 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:52:33,733 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:52:33,740 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:52:33,740 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:52:33,740 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:52:33,777 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:33,778 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:33,779 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:33,796 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:33,931 INFO streaming.PipeMapRed: Records R/W=1891/1\n",
            "2021-05-07 06:52:34,259 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:34,260 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:34,264 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:52:34,264 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:52:34,264 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:52:34,264 INFO mapred.MapTask: bufstart = 0; bufend = 74896; bufvoid = 104857600\n",
            "2021-05-07 06:52:34,264 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26199272(104797088); length = 15125/6553600\n",
            "2021-05-07 06:52:34,293 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:52:34,314 INFO mapred.Task: Task:attempt_local1930139086_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:34,318 INFO mapred.LocalJobRunner: Records R/W=1891/1\n",
            "2021-05-07 06:52:34,318 INFO mapred.Task: Task 'attempt_local1930139086_0001_m_000000_0' done.\n",
            "2021-05-07 06:52:34,330 INFO mapred.Task: Final Counters for attempt_local1930139086_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=70808\n",
            "\t\tFILE: Number of bytes written=698926\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=3782\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "2021-05-07 06:52:34,332 INFO mapred.LocalJobRunner: Finishing task: attempt_local1930139086_0001_m_000000_0\n",
            "2021-05-07 06:52:34,333 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:52:34,338 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:52:34,338 INFO mapred.LocalJobRunner: Starting task: attempt_local1930139086_0001_r_000000_0\n",
            "2021-05-07 06:52:34,354 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:34,354 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:34,354 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:34,357 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5cd55517\n",
            "2021-05-07 06:52:34,360 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:34,379 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:52:34,381 INFO reduce.EventFetcher: attempt_local1930139086_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:52:34,423 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1930139086_0001_m_000000_0 decomp: 82462 len: 82466 to MEMORY\n",
            "2021-05-07 06:52:34,427 INFO reduce.InMemoryMapOutput: Read 82462 bytes from map-output for attempt_local1930139086_0001_m_000000_0\n",
            "2021-05-07 06:52:34,431 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82462, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82462\n",
            "2021-05-07 06:52:34,435 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:52:34,437 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:34,437 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:52:34,446 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:34,446 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:34,452 INFO mapreduce.Job: Job job_local1930139086_0001 running in uber mode : false\n",
            "2021-05-07 06:52:34,453 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 06:52:34,462 INFO reduce.MergeManagerImpl: Merged 1 segments, 82462 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:52:34,462 INFO reduce.MergeManagerImpl: Merging 1 files, 82466 bytes from disk\n",
            "2021-05-07 06:52:34,463 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:52:34,463 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:34,464 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 82459 bytes\n",
            "2021-05-07 06:52:34,465 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:34,482 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, u_reducer.py]\n",
            "2021-05-07 06:52:34,485 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:52:34,486 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:52:34,511 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:34,511 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:34,512 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:34,520 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:34,836 INFO streaming.PipeMapRed: Records R/W=3782/1\n",
            "2021-05-07 06:52:34,852 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:34,853 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:34,854 INFO mapred.Task: Task:attempt_local1930139086_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:34,855 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:34,856 INFO mapred.Task: Task attempt_local1930139086_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:52:34,858 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1930139086_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 06:52:34,864 INFO mapred.LocalJobRunner: Records R/W=3782/1 > reduce\n",
            "2021-05-07 06:52:34,864 INFO mapred.Task: Task 'attempt_local1930139086_0001_r_000000_0' done.\n",
            "2021-05-07 06:52:34,865 INFO mapred.Task: Final Counters for attempt_local1930139086_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=235772\n",
            "\t\tFILE: Number of bytes written=782336\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=3782\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=944\n",
            "2021-05-07 06:52:34,866 INFO mapred.LocalJobRunner: Finishing task: attempt_local1930139086_0001_r_000000_0\n",
            "2021-05-07 06:52:34,866 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:52:35,455 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:52:35,456 INFO mapreduce.Job: Job job_local1930139086_0001 completed successfully\n",
            "2021-05-07 06:52:35,469 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=306580\n",
            "\t\tFILE: Number of bytes written=1481262\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1891\n",
            "\t\tMap output records=3782\n",
            "\t\tMap output bytes=74896\n",
            "\t\tMap output materialized bytes=82466\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=82466\n",
            "\t\tReduce input records=3782\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=7564\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=725614592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=69223\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=944\n",
            "2021-05-07 06:52:35,469 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 06:52:39,051 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Second Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 06:52:40,430 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids1.txt] [] /tmp/streamjob12002379087897731825.jar tmpDir=null\n",
            "2021-05-07 06:52:41,195 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 06:52:41,457 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 06:52:41,458 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 06:52:41,480 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:41,649 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 06:52:41,680 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 06:52:42,019 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1691694848_0001\n",
            "2021-05-07 06:52:42,019 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 06:52:42,480 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids1.txt as file:/tmp/hadoop-root/mapred/local/job_local1691694848_0001_b7456a24-8bb4-4642-b435-5c119773e0c8/centroids1.txt\n",
            "2021-05-07 06:52:42,575 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 06:52:42,577 INFO mapreduce.Job: Running job: job_local1691694848_0001\n",
            "2021-05-07 06:52:42,583 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 06:52:42,586 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 06:52:42,592 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:42,592 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:42,645 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 06:52:42,649 INFO mapred.LocalJobRunner: Starting task: attempt_local1691694848_0001_m_000000_0\n",
            "2021-05-07 06:52:42,708 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:42,708 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:42,762 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:42,770 INFO mapred.MapTask: Processing split: file:/content/test.txt:0+663342\n",
            "2021-05-07 06:52:42,788 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 06:52:42,851 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 06:52:42,851 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 06:52:42,851 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 06:52:42,851 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 06:52:42,851 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 06:52:42,855 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 06:52:42,865 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_mapper.py]\n",
            "2021-05-07 06:52:42,873 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 06:52:42,875 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 06:52:42,876 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 06:52:42,877 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 06:52:42,879 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 06:52:42,879 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 06:52:42,880 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 06:52:42,880 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 06:52:42,881 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 06:52:42,881 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 06:52:42,882 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 06:52:42,882 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 06:52:42,933 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:42,933 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:42,935 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:42,947 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:43,582 INFO mapreduce.Job: Job job_local1691694848_0001 running in uber mode : false\n",
            "2021-05-07 06:52:43,583 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 06:52:43,807 INFO streaming.PipeMapRed: Records R/W=7407/1\n",
            "2021-05-07 06:52:44,388 INFO streaming.PipeMapRed: R/W/S=10000/4097/0 in:10000=10000/1 [rec/s] out:4097=4097/1 [rec/s]\n",
            "2021-05-07 06:52:46,556 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:46,557 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:46,559 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 06:52:46,559 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 06:52:46,560 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 06:52:46,560 INFO mapred.MapTask: bufstart = 0; bufend = 72436; bufvoid = 104857600\n",
            "2021-05-07 06:52:46,560 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26141964(104567856); length = 72433/6553600\n",
            "2021-05-07 06:52:46,661 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 06:52:46,689 INFO mapred.Task: Task:attempt_local1691694848_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:46,692 INFO mapred.LocalJobRunner: Records R/W=7407/1\n",
            "2021-05-07 06:52:46,692 INFO mapred.Task: Task 'attempt_local1691694848_0001_m_000000_0' done.\n",
            "2021-05-07 06:52:46,703 INFO mapred.Task: Final Counters for attempt_local1691694848_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=664923\n",
            "\t\tFILE: Number of bytes written=725123\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=18109\n",
            "\t\tMap output records=18109\n",
            "\t\tMap output bytes=72436\n",
            "\t\tMap output materialized bytes=108660\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=18109\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=367001600\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=663342\n",
            "2021-05-07 06:52:46,703 INFO mapred.LocalJobRunner: Finishing task: attempt_local1691694848_0001_m_000000_0\n",
            "2021-05-07 06:52:46,703 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 06:52:46,712 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 06:52:46,714 INFO mapred.LocalJobRunner: Starting task: attempt_local1691694848_0001_r_000000_0\n",
            "2021-05-07 06:52:46,726 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 06:52:46,727 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 06:52:46,733 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 06:52:46,736 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@37db992c\n",
            "2021-05-07 06:52:46,741 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 06:52:46,768 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 06:52:46,776 INFO reduce.EventFetcher: attempt_local1691694848_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 06:52:46,869 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1691694848_0001_m_000000_0 decomp: 108656 len: 108660 to MEMORY\n",
            "2021-05-07 06:52:46,873 INFO reduce.InMemoryMapOutput: Read 108656 bytes from map-output for attempt_local1691694848_0001_m_000000_0\n",
            "2021-05-07 06:52:46,877 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 108656, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->108656\n",
            "2021-05-07 06:52:46,881 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 06:52:46,882 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:46,882 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 06:52:46,890 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:46,890 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108652 bytes\n",
            "2021-05-07 06:52:46,916 INFO reduce.MergeManagerImpl: Merged 1 segments, 108656 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 06:52:46,917 INFO reduce.MergeManagerImpl: Merging 1 files, 108660 bytes from disk\n",
            "2021-05-07 06:52:46,918 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 06:52:46,918 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 06:52:46,919 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 108652 bytes\n",
            "2021-05-07 06:52:46,920 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:46,933 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_reducer.py]\n",
            "2021-05-07 06:52:46,939 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 06:52:46,941 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 06:52:46,970 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:46,970 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:46,971 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:46,983 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:47,004 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 06:52:47,302 INFO streaming.PipeMapRed: Records R/W=18109/1\n",
            "2021-05-07 06:52:47,331 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 06:52:47,333 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 06:52:47,334 INFO mapred.Task: Task:attempt_local1691694848_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 06:52:47,335 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 06:52:47,337 INFO mapred.Task: Task attempt_local1691694848_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 06:52:47,339 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1691694848_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 06:52:47,343 INFO mapred.LocalJobRunner: Records R/W=18109/1 > reduce\n",
            "2021-05-07 06:52:47,343 INFO mapred.Task: Task 'attempt_local1691694848_0001_r_000000_0' done.\n",
            "2021-05-07 06:52:47,344 INFO mapred.Task: Final Counters for attempt_local1691694848_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=882275\n",
            "\t\tFILE: Number of bytes written=870353\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=20\n",
            "\t\tReduce shuffle bytes=108660\n",
            "\t\tReduce input records=18109\n",
            "\t\tReduce output records=20\n",
            "\t\tSpilled Records=18109\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=367001600\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=36570\n",
            "2021-05-07 06:52:47,345 INFO mapred.LocalJobRunner: Finishing task: attempt_local1691694848_0001_r_000000_0\n",
            "2021-05-07 06:52:47,345 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 06:52:47,588 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 06:52:47,589 INFO mapreduce.Job: Job job_local1691694848_0001 completed successfully\n",
            "2021-05-07 06:52:47,600 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1547198\n",
            "\t\tFILE: Number of bytes written=1595476\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=18109\n",
            "\t\tMap output records=18109\n",
            "\t\tMap output bytes=72436\n",
            "\t\tMap output materialized bytes=108660\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=20\n",
            "\t\tReduce shuffle bytes=108660\n",
            "\t\tReduce input records=18109\n",
            "\t\tReduce output records=20\n",
            "\t\tSpilled Records=36218\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=734003200\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=663342\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=36570\n",
            "2021-05-07 06:52:47,601 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 06:52:51,219 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Third Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfUuN4mZ8Wfo"
      },
      "source": [
        "#evaluation \n",
        "\n",
        "import seaborn as sb\n",
        "import string\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "def parse_args():\n",
        "    '''Parse command line arguments'''\n",
        "    parser = argparse.ArgumentParser(description=\"Run classification evaluation\")\n",
        "    \n",
        "    parser.add_argument('--input', nargs='?', default='predictions.txt',help='Input File path')\n",
        "    parser.add_argument('--output', nargs='?', default='performance.png',help='Visualization plot name')\n",
        "    \n",
        "    return parser.parse_args()\n",
        "\n",
        "def get_performance(args):\n",
        "    fp = open(args.input,'r')\n",
        "\n",
        "    # Processing alphabets and preparing confusion matrix\n",
        "    cm = np.zeros((26, 26))\n",
        "    alphabets = string.ascii_uppercase[:26]\n",
        "    alphabet_dict = dict()\n",
        "    x_axis_labels = []\n",
        "    y_axis_labels = []\n",
        "    for i in range(26):\n",
        "        x_axis_labels.append(alphabets[i])\n",
        "        y_axis_labels.append(alphabets[i])\n",
        "        alphabet_dict[alphabets[i]] = i\n",
        "\n",
        "    # Filling up the confusion matrix\n",
        "    for i,line in enumerate(fp):\n",
        "        line_split = line.split('\\t')[0].split(',')\n",
        "        actual = line_split[0]\n",
        "        predictions = list(line_split[1:])\n",
        "        total_pred = len(predictions)\n",
        "        for p in predictions:\n",
        "          cm[alphabet_dict[actual]][alphabet_dict[p]] += 1.0\n",
        "\n",
        "    fp.close()\n",
        "\n",
        "    # Calculate accuracy with total sum of the array and trace of the array\n",
        "    print('Classification Accuracy: ' + str((np.trace(cm)/np.sum(cm))*100) + '%')\n",
        "    print(cm)\n",
        "    \n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cm = np.true_divide(cm, cm.sum(axis=1, keepdims=True))\n",
        "    cm[np.isnan(cm)] = 0\n",
        "\n",
        "    # Get heatmap of the confusion matrix\n",
        "    ax = sb.heatmap(cm, vmin=0, vmax=1, xticklabels=x_axis_labels, yticklabels=y_axis_labels).set_title('Confusion matrix')\n",
        "    fig = ax.get_figure()\n",
        "    fig.savefig(args.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GrLb-810lmg7",
        "outputId": "dc05543c-52b6-4366-fd84-6d82b146032f"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    get_performance(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Accuracy: 28.10204870506378%\n",
            "[[422.   0.   0.   1.   0.   0.   1.   1.   0.  27.   1.  14.   2.   3.\n",
            "    0.   0.   2.   0.   3.   0.   0.   0.   0.   0.   0.   8.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0. 321.   0. 149.   0.  55.   1.   4.   0.  19.   1.   0.   1.\n",
            "    0.   0.   0.   0.   1.   0.   2.   0.   0.   0.   0.   1.]\n",
            " [ 43.  30.   0.  54.   0.   0.  32.  39.  48.   0.  31.   0.  44.  63.\n",
            "   82.  14.  81.  58.   0.   0.  17.   0.  20.  31.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 15.  11.  34.  20.  13. 236.   8.  32.  22.  15.  16.   1.   2.  91.\n",
            "    1. 156.   0.   7.  21. 189. 104. 126.   2.  16. 171.   9.]\n",
            " [  0. 125.  15. 129. 134.   4. 138. 114.  15.   3. 106.  31.   1.  10.\n",
            "   63.   5.  37. 111.  79.   3.  45.   0.   0. 107.   0.  72.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0. 234.  19.   0.  16.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 23.   8.   1.  33.  65.  11.   0.  26. 166. 434.  45. 139.   0.   8.\n",
            "    0.   7.   0.  24.  83.   5.   0.   0.   0. 100.   0. 117.]\n",
            " [  0.   0.   0.   1.   0.   0.   1.  38.   0.   0. 104.   0.  56.  52.\n",
            "    0.  11.   2.  25.   0.   0.   2.   0.   4.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 294.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 47.  71.  39.  37.  55.  53.  82.  66.   9.  19.  34.  34. 411.  54.\n",
            "   43.  71. 107.  64.  57.  22.  58.  63. 227.  34. 118.  28.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  2.   2.  13.  11.   0.   0.  21.  21.   0.   2.   9.   2.   2.  46.\n",
            "  113.  16. 100.   0.   0.   1.  54.   5.   7.  85.   1.   0.]\n",
            " [  0.   0.   0.   0.   0. 153.   0.   0.  21.  32.   0.   0.   0.   8.\n",
            "    0. 367.   0.   0.   1.  68.   0.   3.   0.   0.   0.   1.]\n",
            " [133.  76.  30.  76.  36.  19.  85.  57.  42.  55.  69.  67.  51.  31.\n",
            "  164.  25. 151.  70.  71.   9.  34.   0.   0.  51.   3.  37.]\n",
            " [ 23. 141.  42. 218.  42.  31.  78.  96.  27.  48.  77.  23.  18. 104.\n",
            "  123.  29.  94. 187.  31.  21.  66.   9.   6.  60.  10.  21.]\n",
            " [  4. 141.   7. 124.  21.   8.  63.  13.  10.  10.   4.  19.   1.   2.\n",
            "   61.  15. 110. 137. 103.   8.   4.   0.   0.   7.   6.  21.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0. 200.   0.  37.   0.   0. 120.   0.]\n",
            " [  0.   0.  87.   2.  47.  76.   6. 158.   1.   4. 124.   7. 125. 189.\n",
            "   11.   7.   0.   0.  44. 115. 331.  87. 109.  90. 149.  21.]\n",
            " [  0.   0.   1.   0.   0. 108.   0.   0.   0.   0.   1.   0.   0.   4.\n",
            "    0.   2.   1.   0.   0.  38.  14. 357.   2.   0. 129.   0.]\n",
            " [  1.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   3.  44.\n",
            "    0.   1.   0.   0.   0.   0.   3.   6. 305.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  1.  90.  78.  21. 138.   0. 130.   0.  85.   6.  28.  43.   0.   0.\n",
            "   22.   1.  23.   3. 185.  42.   0.   0.   0. 130.   6. 327.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayUO8VH68hAd"
      },
      "source": [
        "### For n=25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIMkgQ2ZzCOI",
        "outputId": "4fcec88f-dbf8-4811-b736-b7da218971f2"
      },
      "source": [
        "!sh model.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count---------+1\n",
            "2021-05-07 07:48:13,263 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob6487055911722700774.jar tmpDir=null\n",
            "2021-05-07 07:48:14,127 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:48:14,263 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:48:14,263 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:48:14,295 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:14,430 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:48:14,451 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:48:14,769 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1091870309_0001\n",
            "2021-05-07 07:48:14,769 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:48:15,196 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1091870309_0001_4761747e-164c-4589-a8b2-efbd194e048a/centroids.txt\n",
            "2021-05-07 07:48:15,304 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:48:15,306 INFO mapreduce.Job: Running job: job_local1091870309_0001\n",
            "2021-05-07 07:48:15,312 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:48:15,315 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:48:15,324 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:15,324 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:15,379 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:48:15,382 INFO mapred.LocalJobRunner: Starting task: attempt_local1091870309_0001_m_000000_0\n",
            "2021-05-07 07:48:15,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:15,416 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:15,464 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:15,484 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:48:15,511 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:48:15,596 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:48:15,597 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:48:15,597 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:48:15,597 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:48:15,597 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:48:15,601 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:48:15,610 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:48:15,620 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:48:15,621 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:48:15,622 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:48:15,622 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:48:15,623 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:48:15,623 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:48:15,624 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:48:15,624 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:48:15,625 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:48:15,626 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:48:15,626 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:48:15,626 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:48:15,660 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:15,660 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:15,662 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:15,673 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:15,817 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:48:16,311 INFO mapreduce.Job: Job job_local1091870309_0001 running in uber mode : false\n",
            "2021-05-07 07:48:16,312 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:48:16,612 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:16,614 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:16,617 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:48:16,617 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:48:16,617 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:48:16,617 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:48:16,617 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:48:16,666 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:48:16,687 INFO mapred.Task: Task:attempt_local1091870309_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:16,693 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:48:16,693 INFO mapred.Task: Task 'attempt_local1091870309_0001_m_000000_0' done.\n",
            "2021-05-07 07:48:16,702 INFO mapred.Task: Final Counters for attempt_local1091870309_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808233\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=374341632\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:48:16,702 INFO mapred.LocalJobRunner: Finishing task: attempt_local1091870309_0001_m_000000_0\n",
            "2021-05-07 07:48:16,703 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:48:16,711 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:48:16,711 INFO mapred.LocalJobRunner: Starting task: attempt_local1091870309_0001_r_000000_0\n",
            "2021-05-07 07:48:16,721 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:16,721 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:16,722 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:16,727 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@703afa19\n",
            "2021-05-07 07:48:16,729 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:16,751 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:48:16,764 INFO reduce.EventFetcher: attempt_local1091870309_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:48:16,801 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1091870309_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:48:16,809 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1091870309_0001_m_000000_0\n",
            "2021-05-07 07:48:16,811 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:48:16,819 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:48:16,820 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:16,821 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:48:16,827 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:16,828 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:16,851 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:48:16,852 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:48:16,853 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:48:16,853 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:16,854 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:16,855 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:16,865 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:48:16,873 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:48:16,875 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:48:16,915 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:16,915 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:16,916 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:16,934 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:17,233 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:48:17,257 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:17,257 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:17,258 INFO mapred.Task: Task:attempt_local1091870309_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:17,260 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:17,260 INFO mapred.Task: Task attempt_local1091870309_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:48:17,262 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1091870309_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:48:17,263 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:48:17,264 INFO mapred.Task: Task 'attempt_local1091870309_0001_r_000000_0' done.\n",
            "2021-05-07 07:48:17,264 INFO mapred.Task: Final Counters for attempt_local1091870309_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000932\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=374341632\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:17,265 INFO mapred.LocalJobRunner: Finishing task: attempt_local1091870309_0001_r_000000_0\n",
            "2021-05-07 07:48:17,265 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:48:17,315 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:48:17,316 INFO mapreduce.Job: Job job_local1091870309_0001 completed successfully\n",
            "2021-05-07 07:48:17,329 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=748683264\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:17,335 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:48:20,997 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+2\n",
            "2021-05-07 07:48:22,229 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob630094428505264281.jar tmpDir=null\n",
            "2021-05-07 07:48:23,062 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:48:23,286 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:48:23,287 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:48:23,318 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:23,467 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:48:23,487 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:48:23,818 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1407264144_0001\n",
            "2021-05-07 07:48:23,819 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:48:24,232 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1407264144_0001_ab04eabe-d732-475e-9b35-ce51420938d3/centroids.txt\n",
            "2021-05-07 07:48:24,370 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:48:24,372 INFO mapreduce.Job: Running job: job_local1407264144_0001\n",
            "2021-05-07 07:48:24,378 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:48:24,380 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:48:24,385 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:24,385 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:24,437 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:48:24,448 INFO mapred.LocalJobRunner: Starting task: attempt_local1407264144_0001_m_000000_0\n",
            "2021-05-07 07:48:24,486 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:24,486 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:24,520 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:24,534 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:48:24,561 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:48:24,639 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:48:24,639 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:48:24,639 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:48:24,639 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:48:24,639 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:48:24,644 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:48:24,659 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:48:24,671 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:48:24,672 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:48:24,673 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:48:24,674 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:48:24,675 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:48:24,675 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:48:24,675 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:48:24,676 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:48:24,676 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:48:24,677 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:48:24,678 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:48:24,678 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:48:24,714 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:24,715 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:24,717 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:24,750 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:24,885 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:48:25,377 INFO mapreduce.Job: Job job_local1407264144_0001 running in uber mode : false\n",
            "2021-05-07 07:48:25,378 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:48:25,666 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:25,668 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:25,673 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:48:25,673 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:48:25,673 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:48:25,673 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:48:25,673 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:48:25,740 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:48:25,761 INFO mapred.Task: Task:attempt_local1407264144_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:25,764 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:48:25,764 INFO mapred.Task: Task 'attempt_local1407264144_0001_m_000000_0' done.\n",
            "2021-05-07 07:48:25,773 INFO mapred.Task: Final Counters for attempt_local1407264144_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808231\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=321912832\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:48:25,773 INFO mapred.LocalJobRunner: Finishing task: attempt_local1407264144_0001_m_000000_0\n",
            "2021-05-07 07:48:25,773 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:48:25,783 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:48:25,785 INFO mapred.LocalJobRunner: Starting task: attempt_local1407264144_0001_r_000000_0\n",
            "2021-05-07 07:48:25,797 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:25,797 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:25,797 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:25,806 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@172d79a4\n",
            "2021-05-07 07:48:25,813 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:25,836 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:48:25,850 INFO reduce.EventFetcher: attempt_local1407264144_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:48:25,891 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1407264144_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:48:25,900 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1407264144_0001_m_000000_0\n",
            "2021-05-07 07:48:25,903 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:48:25,908 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:48:25,909 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:25,909 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:48:25,916 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:25,916 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:25,942 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:48:25,943 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:48:25,944 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:48:25,944 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:25,945 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:25,946 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:25,971 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:48:25,976 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:48:25,979 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:48:26,008 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:26,009 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:26,011 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:26,018 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:26,339 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:48:26,365 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:26,365 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:26,367 INFO mapred.Task: Task:attempt_local1407264144_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:26,371 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:26,371 INFO mapred.Task: Task attempt_local1407264144_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:48:26,373 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1407264144_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:48:26,374 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:48:26,374 INFO mapred.Task: Task 'attempt_local1407264144_0001_r_000000_0' done.\n",
            "2021-05-07 07:48:26,375 INFO mapred.Task: Final Counters for attempt_local1407264144_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000930\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=321912832\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:26,375 INFO mapred.LocalJobRunner: Finishing task: attempt_local1407264144_0001_r_000000_0\n",
            "2021-05-07 07:48:26,376 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:48:26,380 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:48:26,381 INFO mapreduce.Job: Job job_local1407264144_0001 completed successfully\n",
            "2021-05-07 07:48:26,403 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809161\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=643825664\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:26,407 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:48:30,115 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+3\n",
            "2021-05-07 07:48:31,377 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob10798364375839047095.jar tmpDir=null\n",
            "2021-05-07 07:48:32,197 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:48:32,347 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:48:32,348 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:48:32,372 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:32,512 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:48:32,532 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:48:32,876 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local275975533_0001\n",
            "2021-05-07 07:48:32,879 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:48:33,313 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local275975533_0001_14f752a5-9870-495e-856d-8323dc12b6ac/centroids.txt\n",
            "2021-05-07 07:48:33,437 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:48:33,438 INFO mapreduce.Job: Running job: job_local275975533_0001\n",
            "2021-05-07 07:48:33,445 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:48:33,448 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:48:33,453 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:33,453 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:33,499 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:48:33,506 INFO mapred.LocalJobRunner: Starting task: attempt_local275975533_0001_m_000000_0\n",
            "2021-05-07 07:48:33,539 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:33,542 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:33,594 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:33,612 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:48:33,630 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:48:33,703 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:48:33,703 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:48:33,703 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:48:33,703 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:48:33,703 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:48:33,708 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:48:33,720 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:48:33,732 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:48:33,732 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:48:33,733 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:48:33,734 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:48:33,735 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:48:33,735 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:48:33,735 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:48:33,736 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:48:33,736 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:48:33,737 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:48:33,737 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:48:33,738 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:48:33,787 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:33,787 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:33,789 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:33,813 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:33,973 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:48:34,444 INFO mapreduce.Job: Job job_local275975533_0001 running in uber mode : false\n",
            "2021-05-07 07:48:34,445 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:48:34,752 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:34,757 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:34,762 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:48:34,762 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:48:34,762 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:48:34,763 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:48:34,763 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:48:34,815 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:48:34,828 INFO mapred.Task: Task:attempt_local275975533_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:34,832 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:48:34,832 INFO mapred.Task: Task 'attempt_local275975533_0001_m_000000_0' done.\n",
            "2021-05-07 07:48:34,841 INFO mapred.Task: Final Counters for attempt_local275975533_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=805252\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=352321536\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:48:34,841 INFO mapred.LocalJobRunner: Finishing task: attempt_local275975533_0001_m_000000_0\n",
            "2021-05-07 07:48:34,842 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:48:34,847 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:48:34,849 INFO mapred.LocalJobRunner: Starting task: attempt_local275975533_0001_r_000000_0\n",
            "2021-05-07 07:48:34,863 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:34,863 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:34,863 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:34,873 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@703afa19\n",
            "2021-05-07 07:48:34,876 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:34,911 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:48:34,917 INFO reduce.EventFetcher: attempt_local275975533_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:48:34,957 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local275975533_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:48:34,962 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local275975533_0001_m_000000_0\n",
            "2021-05-07 07:48:34,967 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:48:34,971 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:48:34,974 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:34,974 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:48:34,980 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:34,981 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:35,000 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:48:35,001 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:48:35,002 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:48:35,002 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:35,003 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:35,004 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:35,016 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:48:35,023 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:48:35,025 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:48:35,052 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:35,052 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:35,053 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:35,064 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:35,377 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:48:35,396 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:35,399 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:35,400 INFO mapred.Task: Task:attempt_local275975533_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:35,402 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:35,402 INFO mapred.Task: Task attempt_local275975533_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:48:35,406 INFO output.FileOutputCommitter: Saved output of task 'attempt_local275975533_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:48:35,411 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:48:35,411 INFO mapred.Task: Task 'attempt_local275975533_0001_r_000000_0' done.\n",
            "2021-05-07 07:48:35,412 INFO mapred.Task: Final Counters for attempt_local275975533_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=997951\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=352321536\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:35,412 INFO mapred.LocalJobRunner: Finishing task: attempt_local275975533_0001_r_000000_0\n",
            "2021-05-07 07:48:35,413 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:48:35,447 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:48:35,448 INFO mapreduce.Job: Job job_local275975533_0001 completed successfully\n",
            "2021-05-07 07:48:35,469 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1803203\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=704643072\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:35,476 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:48:39,089 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+4\n",
            "2021-05-07 07:48:40,375 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob4175065631897227328.jar tmpDir=null\n",
            "2021-05-07 07:48:41,192 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:48:41,409 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:48:41,409 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:48:41,433 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:41,561 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:48:41,581 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:48:41,917 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2067936554_0001\n",
            "2021-05-07 07:48:41,917 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:48:42,372 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local2067936554_0001_74b3a794-81db-4841-9e07-c8a26c2fe983/centroids.txt\n",
            "2021-05-07 07:48:42,505 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:48:42,507 INFO mapreduce.Job: Running job: job_local2067936554_0001\n",
            "2021-05-07 07:48:42,513 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:48:42,516 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:48:42,526 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:42,526 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:42,584 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:48:42,589 INFO mapred.LocalJobRunner: Starting task: attempt_local2067936554_0001_m_000000_0\n",
            "2021-05-07 07:48:42,628 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:42,629 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:42,689 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:42,710 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:48:42,740 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:48:42,815 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:48:42,816 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:48:42,816 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:48:42,816 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:48:42,816 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:48:42,820 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:48:42,832 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:48:42,842 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:48:42,843 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:48:42,844 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:48:42,844 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:48:42,846 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:48:42,846 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:48:42,846 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:48:42,847 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:48:42,847 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:48:42,848 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:48:42,848 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:48:42,849 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:48:42,882 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:42,882 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:42,884 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:42,895 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:43,043 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:48:43,512 INFO mapreduce.Job: Job job_local2067936554_0001 running in uber mode : false\n",
            "2021-05-07 07:48:43,513 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:48:43,837 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:43,837 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:43,841 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:48:43,841 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:48:43,841 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:48:43,841 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:48:43,841 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:48:43,909 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:48:43,926 INFO mapred.Task: Task:attempt_local2067936554_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:43,931 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:48:43,932 INFO mapred.Task: Task 'attempt_local2067936554_0001_m_000000_0' done.\n",
            "2021-05-07 07:48:43,941 INFO mapred.Task: Final Counters for attempt_local2067936554_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808233\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=330301440\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:48:43,941 INFO mapred.LocalJobRunner: Finishing task: attempt_local2067936554_0001_m_000000_0\n",
            "2021-05-07 07:48:43,941 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:48:43,948 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:48:43,951 INFO mapred.LocalJobRunner: Starting task: attempt_local2067936554_0001_r_000000_0\n",
            "2021-05-07 07:48:43,962 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:43,962 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:43,963 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:43,968 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4d1ef1af\n",
            "2021-05-07 07:48:43,970 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:43,995 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:48:44,004 INFO reduce.EventFetcher: attempt_local2067936554_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:48:44,042 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2067936554_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:48:44,046 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local2067936554_0001_m_000000_0\n",
            "2021-05-07 07:48:44,049 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:48:44,052 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:48:44,053 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:44,054 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:48:44,070 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:44,070 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:44,121 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:48:44,123 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:48:44,124 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:48:44,124 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:44,125 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:44,126 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:44,138 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:48:44,146 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:48:44,148 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:48:44,189 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:44,189 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:44,193 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:44,210 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:44,516 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:48:44,603 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:48:44,627 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:44,627 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:44,628 INFO mapred.Task: Task:attempt_local2067936554_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:44,630 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:44,630 INFO mapred.Task: Task attempt_local2067936554_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:48:44,632 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2067936554_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:48:44,635 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:48:44,635 INFO mapred.Task: Task 'attempt_local2067936554_0001_r_000000_0' done.\n",
            "2021-05-07 07:48:44,636 INFO mapred.Task: Final Counters for attempt_local2067936554_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000932\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=330301440\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:44,637 INFO mapred.LocalJobRunner: Finishing task: attempt_local2067936554_0001_r_000000_0\n",
            "2021-05-07 07:48:44,637 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:48:45,517 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:48:45,518 INFO mapreduce.Job: Job job_local2067936554_0001 completed successfully\n",
            "2021-05-07 07:48:45,533 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=660602880\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:45,533 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:48:49,388 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+5\n",
            "2021-05-07 07:48:50,762 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob5473387918106622819.jar tmpDir=null\n",
            "2021-05-07 07:48:51,615 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:48:51,811 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:48:51,812 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:48:51,838 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:52,023 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:48:52,053 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:48:52,405 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1995541043_0001\n",
            "2021-05-07 07:48:52,406 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:48:52,844 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1995541043_0001_b80d31e7-3b28-4d1b-b209-487ecfbe57c6/centroids.txt\n",
            "2021-05-07 07:48:52,973 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:48:52,975 INFO mapreduce.Job: Running job: job_local1995541043_0001\n",
            "2021-05-07 07:48:52,983 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:48:52,987 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:48:52,997 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:52,997 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:53,049 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:48:53,059 INFO mapred.LocalJobRunner: Starting task: attempt_local1995541043_0001_m_000000_0\n",
            "2021-05-07 07:48:53,097 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:53,100 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:53,154 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:53,176 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:48:53,199 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:48:53,273 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:48:53,274 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:48:53,274 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:48:53,274 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:48:53,274 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:48:53,278 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:48:53,285 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:48:53,306 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:48:53,309 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:48:53,310 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:48:53,311 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:48:53,312 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:48:53,312 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:48:53,312 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:48:53,314 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:48:53,314 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:48:53,315 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:48:53,315 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:48:53,316 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:48:53,359 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:53,359 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:53,362 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:53,387 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:53,532 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:48:53,982 INFO mapreduce.Job: Job job_local1995541043_0001 running in uber mode : false\n",
            "2021-05-07 07:48:53,983 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:48:54,317 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:54,320 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:54,324 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:48:54,324 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:48:54,324 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:48:54,324 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:48:54,324 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:48:54,375 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:48:54,388 INFO mapred.Task: Task:attempt_local1995541043_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:54,394 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:48:54,394 INFO mapred.Task: Task 'attempt_local1995541043_0001_m_000000_0' done.\n",
            "2021-05-07 07:48:54,404 INFO mapred.Task: Final Counters for attempt_local1995541043_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808233\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:48:54,404 INFO mapred.LocalJobRunner: Finishing task: attempt_local1995541043_0001_m_000000_0\n",
            "2021-05-07 07:48:54,404 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:48:54,408 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:48:54,413 INFO mapred.LocalJobRunner: Starting task: attempt_local1995541043_0001_r_000000_0\n",
            "2021-05-07 07:48:54,426 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:48:54,426 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:48:54,427 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:48:54,429 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5117927a\n",
            "2021-05-07 07:48:54,437 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:48:54,460 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:48:54,480 INFO reduce.EventFetcher: attempt_local1995541043_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:48:54,537 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1995541043_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:48:54,546 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1995541043_0001_m_000000_0\n",
            "2021-05-07 07:48:54,549 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:48:54,554 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:48:54,555 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:54,556 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:48:54,565 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:54,565 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:54,620 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:48:54,621 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:48:54,622 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:48:54,622 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:48:54,623 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:48:54,624 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:54,635 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:48:54,639 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:48:54,641 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:48:54,665 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:54,666 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:54,667 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:54,677 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:48:54,985 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:48:54,997 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:48:55,023 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:48:55,024 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:48:55,025 INFO mapred.Task: Task:attempt_local1995541043_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:48:55,026 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:48:55,027 INFO mapred.Task: Task attempt_local1995541043_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:48:55,029 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1995541043_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:48:55,030 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:48:55,030 INFO mapred.Task: Task 'attempt_local1995541043_0001_r_000000_0' done.\n",
            "2021-05-07 07:48:55,032 INFO mapred.Task: Final Counters for attempt_local1995541043_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000932\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=354418688\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:55,032 INFO mapred.LocalJobRunner: Finishing task: attempt_local1995541043_0001_r_000000_0\n",
            "2021-05-07 07:48:55,033 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:48:55,986 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:48:55,987 INFO mapreduce.Job: Job job_local1995541043_0001 completed successfully\n",
            "2021-05-07 07:48:56,000 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=708837376\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:48:56,003 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:48:59,676 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+6\n",
            "2021-05-07 07:49:01,052 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob5092630493183168461.jar tmpDir=null\n",
            "2021-05-07 07:49:01,950 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:02,122 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:02,122 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:02,143 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:02,346 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:02,390 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:02,742 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local495554128_0001\n",
            "2021-05-07 07:49:02,742 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:03,203 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local495554128_0001_2fd12167-9a9a-4229-8d05-5154b7f4c85c/centroids.txt\n",
            "2021-05-07 07:49:03,351 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:03,353 INFO mapreduce.Job: Running job: job_local495554128_0001\n",
            "2021-05-07 07:49:03,361 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:03,364 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:03,377 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:03,377 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:03,458 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:03,472 INFO mapred.LocalJobRunner: Starting task: attempt_local495554128_0001_m_000000_0\n",
            "2021-05-07 07:49:03,514 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:03,517 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:03,572 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:03,588 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:03,606 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:03,679 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:03,679 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:03,679 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:03,679 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:03,679 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:03,683 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:03,694 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:03,711 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:03,712 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:03,713 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:03,713 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:03,714 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:03,714 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:03,715 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:03,715 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:03,715 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:03,719 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:03,720 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:03,720 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:03,767 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:03,767 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:03,778 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:03,799 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:03,944 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:04,359 INFO mapreduce.Job: Job job_local495554128_0001 running in uber mode : false\n",
            "2021-05-07 07:49:04,361 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:04,739 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:04,742 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:04,747 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:04,747 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:04,747 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:04,747 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:04,747 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:04,802 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:04,825 INFO mapred.Task: Task:attempt_local495554128_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:04,830 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:04,830 INFO mapred.Task: Task 'attempt_local495554128_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:04,839 INFO mapred.Task: Final Counters for attempt_local495554128_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=805250\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=350224384\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:04,840 INFO mapred.LocalJobRunner: Finishing task: attempt_local495554128_0001_m_000000_0\n",
            "2021-05-07 07:49:04,840 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:04,843 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:04,846 INFO mapred.LocalJobRunner: Starting task: attempt_local495554128_0001_r_000000_0\n",
            "2021-05-07 07:49:04,857 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:04,857 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:04,859 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:04,866 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@592cf008\n",
            "2021-05-07 07:49:04,868 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:04,887 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:04,890 INFO reduce.EventFetcher: attempt_local495554128_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:04,928 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local495554128_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:04,932 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local495554128_0001_m_000000_0\n",
            "2021-05-07 07:49:04,933 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:04,938 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:04,940 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:04,940 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:04,947 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:04,948 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:04,992 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:04,993 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:04,994 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:04,994 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:04,995 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:04,996 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:05,012 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:05,017 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:05,018 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:05,047 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:05,047 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:05,049 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:05,059 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:05,365 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:49:05,389 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:05,415 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:05,416 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:05,419 INFO mapred.Task: Task:attempt_local495554128_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:05,423 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:05,423 INFO mapred.Task: Task attempt_local495554128_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:05,426 INFO output.FileOutputCommitter: Saved output of task 'attempt_local495554128_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:05,427 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:05,428 INFO mapred.Task: Task 'attempt_local495554128_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:05,430 INFO mapred.Task: Final Counters for attempt_local495554128_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=997949\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=350224384\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:05,431 INFO mapred.LocalJobRunner: Finishing task: attempt_local495554128_0001_r_000000_0\n",
            "2021-05-07 07:49:05,431 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:06,366 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:06,366 INFO mapreduce.Job: Job job_local495554128_0001 completed successfully\n",
            "2021-05-07 07:49:06,378 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1803199\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=700448768\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:06,378 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:49:10,048 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+7\n",
            "2021-05-07 07:49:11,403 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob9145482641840905632.jar tmpDir=null\n",
            "2021-05-07 07:49:12,260 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:12,465 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:12,465 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:12,492 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:12,693 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:12,730 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:13,089 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local708745668_0001\n",
            "2021-05-07 07:49:13,089 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:13,543 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local708745668_0001_8d70de0d-4736-42da-9e6a-6bf5998605bb/centroids.txt\n",
            "2021-05-07 07:49:13,673 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:13,675 INFO mapreduce.Job: Running job: job_local708745668_0001\n",
            "2021-05-07 07:49:13,686 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:13,690 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:13,700 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:13,700 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:13,760 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:13,765 INFO mapred.LocalJobRunner: Starting task: attempt_local708745668_0001_m_000000_0\n",
            "2021-05-07 07:49:13,823 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:13,824 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:13,878 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:13,889 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:13,908 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:13,972 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:13,972 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:13,972 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:13,972 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:13,972 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:13,975 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:13,984 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:13,993 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:13,994 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:13,995 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:13,995 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:13,996 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:13,996 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:13,997 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:13,997 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:13,998 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:13,999 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:13,999 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:14,000 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:14,038 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:14,039 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:14,040 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:14,057 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:14,199 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:14,680 INFO mapreduce.Job: Job job_local708745668_0001 running in uber mode : false\n",
            "2021-05-07 07:49:14,681 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:15,034 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:15,035 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:15,040 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:15,040 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:15,040 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:15,040 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:15,040 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:15,089 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:15,102 INFO mapred.Task: Task:attempt_local708745668_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:15,105 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:15,105 INFO mapred.Task: Task 'attempt_local708745668_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:15,114 INFO mapred.Task: Final Counters for attempt_local708745668_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=805250\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:15,115 INFO mapred.LocalJobRunner: Finishing task: attempt_local708745668_0001_m_000000_0\n",
            "2021-05-07 07:49:15,115 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:15,119 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:15,121 INFO mapred.LocalJobRunner: Starting task: attempt_local708745668_0001_r_000000_0\n",
            "2021-05-07 07:49:15,141 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:15,142 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:15,147 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:15,154 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@143b705\n",
            "2021-05-07 07:49:15,157 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:15,177 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:15,179 INFO reduce.EventFetcher: attempt_local708745668_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:15,231 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local708745668_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:15,235 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local708745668_0001_m_000000_0\n",
            "2021-05-07 07:49:15,239 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:15,246 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:15,247 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:15,249 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:15,258 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:15,258 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:15,292 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:15,293 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:15,294 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:15,294 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:15,295 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:15,296 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:15,306 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:15,312 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:15,314 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:15,348 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:15,348 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:15,349 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:15,356 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:15,684 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:49:15,702 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:15,727 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:15,729 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:15,730 INFO mapred.Task: Task:attempt_local708745668_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:15,731 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:15,731 INFO mapred.Task: Task attempt_local708745668_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:15,733 INFO output.FileOutputCommitter: Saved output of task 'attempt_local708745668_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:15,734 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:15,734 INFO mapred.Task: Task 'attempt_local708745668_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:15,736 INFO mapred.Task: Final Counters for attempt_local708745668_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=997949\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:15,737 INFO mapred.LocalJobRunner: Finishing task: attempt_local708745668_0001_r_000000_0\n",
            "2021-05-07 07:49:15,737 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:16,685 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:16,686 INFO mapreduce.Job: Job job_local708745668_0001 completed successfully\n",
            "2021-05-07 07:49:16,700 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1803199\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=685768704\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:16,701 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:49:20,287 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+8\n",
            "2021-05-07 07:49:21,556 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob9159354684707847009.jar tmpDir=null\n",
            "2021-05-07 07:49:22,393 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:22,575 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:22,576 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:22,598 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:22,751 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:22,771 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:23,139 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1728406357_0001\n",
            "2021-05-07 07:49:23,139 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:23,582 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1728406357_0001_6210d808-c7a0-498a-a6a8-0cecb02641f1/centroids.txt\n",
            "2021-05-07 07:49:23,701 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:23,703 INFO mapreduce.Job: Running job: job_local1728406357_0001\n",
            "2021-05-07 07:49:23,715 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:23,717 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:23,728 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:23,728 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:23,776 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:23,780 INFO mapred.LocalJobRunner: Starting task: attempt_local1728406357_0001_m_000000_0\n",
            "2021-05-07 07:49:23,827 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:23,829 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:23,873 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:23,891 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:23,912 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:23,978 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:23,978 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:23,978 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:23,978 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:23,978 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:23,982 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:23,993 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:24,001 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:24,002 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:24,003 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:24,003 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:24,004 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:24,004 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:24,004 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:24,005 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:24,005 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:24,006 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:24,006 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:24,007 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:24,038 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:24,038 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:24,040 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:24,052 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:24,224 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:24,713 INFO mapreduce.Job: Job job_local1728406357_0001 running in uber mode : false\n",
            "2021-05-07 07:49:24,715 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:25,000 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:25,001 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:25,014 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:25,015 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:25,015 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:25,015 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:25,015 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:25,069 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:25,082 INFO mapred.Task: Task:attempt_local1728406357_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:25,085 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:25,085 INFO mapred.Task: Task 'attempt_local1728406357_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:25,094 INFO mapred.Task: Final Counters for attempt_local1728406357_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808233\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:25,094 INFO mapred.LocalJobRunner: Finishing task: attempt_local1728406357_0001_m_000000_0\n",
            "2021-05-07 07:49:25,095 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:25,099 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:25,102 INFO mapred.LocalJobRunner: Starting task: attempt_local1728406357_0001_r_000000_0\n",
            "2021-05-07 07:49:25,114 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:25,114 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:25,114 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:25,120 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1ad1bcff\n",
            "2021-05-07 07:49:25,122 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:25,146 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:25,156 INFO reduce.EventFetcher: attempt_local1728406357_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:25,195 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1728406357_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:25,200 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1728406357_0001_m_000000_0\n",
            "2021-05-07 07:49:25,203 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:25,207 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:25,208 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:25,209 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:25,216 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:25,216 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:25,255 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:25,257 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:25,258 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:25,258 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:25,259 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:25,260 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:25,276 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:25,281 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:25,283 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:25,331 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:25,331 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:25,332 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:25,353 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:25,696 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:25,697 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:25,700 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:25,702 INFO mapred.Task: Task:attempt_local1728406357_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:25,704 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:25,704 INFO mapred.Task: Task attempt_local1728406357_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:25,708 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1728406357_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:25,713 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:25,713 INFO mapred.Task: Task 'attempt_local1728406357_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:25,714 INFO mapred.Task: Final Counters for attempt_local1728406357_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000932\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=27\n",
            "\t\tTotal committed heap usage (bytes)=343932928\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:25,715 INFO mapred.LocalJobRunner: Finishing task: attempt_local1728406357_0001_r_000000_0\n",
            "2021-05-07 07:49:25,715 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:25,717 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:26,718 INFO mapreduce.Job: Job job_local1728406357_0001 completed successfully\n",
            "2021-05-07 07:49:26,734 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=27\n",
            "\t\tTotal committed heap usage (bytes)=687865856\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:26,734 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:49:30,384 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+9\n",
            "2021-05-07 07:49:31,717 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob11485216439541629903.jar tmpDir=null\n",
            "2021-05-07 07:49:32,620 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:32,821 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:32,821 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:32,847 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:33,029 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:33,067 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:33,412 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1769560648_0001\n",
            "2021-05-07 07:49:33,412 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:33,856 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1769560648_0001_5b142276-6c82-4c34-a742-6035326e0c2a/centroids.txt\n",
            "2021-05-07 07:49:34,010 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:34,012 INFO mapreduce.Job: Running job: job_local1769560648_0001\n",
            "2021-05-07 07:49:34,019 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:34,022 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:34,026 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:34,027 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:34,077 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:34,085 INFO mapred.LocalJobRunner: Starting task: attempt_local1769560648_0001_m_000000_0\n",
            "2021-05-07 07:49:34,122 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:34,124 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:34,173 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:34,182 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:34,202 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:34,278 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:34,278 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:34,278 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:34,278 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:34,278 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:34,282 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:34,298 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:34,307 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:34,307 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:34,308 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:34,308 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:34,309 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:34,309 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:34,309 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:34,309 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:34,310 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:34,310 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:34,310 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:34,311 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:34,343 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:34,346 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:34,348 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:34,362 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:34,507 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:35,021 INFO mapreduce.Job: Job job_local1769560648_0001 running in uber mode : false\n",
            "2021-05-07 07:49:35,022 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:35,296 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:35,299 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:35,303 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:35,303 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:35,303 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:35,303 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:35,303 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:35,359 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:35,382 INFO mapred.Task: Task:attempt_local1769560648_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:35,388 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:35,388 INFO mapred.Task: Task 'attempt_local1769560648_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:35,398 INFO mapred.Task: Final Counters for attempt_local1769560648_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=346030080\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:35,398 INFO mapred.LocalJobRunner: Finishing task: attempt_local1769560648_0001_m_000000_0\n",
            "2021-05-07 07:49:35,399 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:35,406 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:35,408 INFO mapred.LocalJobRunner: Starting task: attempt_local1769560648_0001_r_000000_0\n",
            "2021-05-07 07:49:35,419 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:35,419 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:35,421 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:35,426 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@400c50f0\n",
            "2021-05-07 07:49:35,432 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:35,455 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:35,457 INFO reduce.EventFetcher: attempt_local1769560648_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:35,493 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1769560648_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:35,497 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1769560648_0001_m_000000_0\n",
            "2021-05-07 07:49:35,504 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:35,509 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:35,510 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:35,510 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:35,517 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:35,519 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:35,587 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:35,588 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:35,589 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:35,589 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:35,590 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:35,591 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:35,605 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:35,612 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:35,615 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:35,643 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:35,643 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:35,644 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:35,655 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:36,000 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:36,024 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:36,025 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:49:36,026 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:36,026 INFO mapred.Task: Task:attempt_local1769560648_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:36,028 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:36,028 INFO mapred.Task: Task attempt_local1769560648_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:36,030 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1769560648_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:36,031 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:36,031 INFO mapred.Task: Task 'attempt_local1769560648_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:36,034 INFO mapred.Task: Final Counters for attempt_local1769560648_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000934\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=346030080\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:36,034 INFO mapred.LocalJobRunner: Finishing task: attempt_local1769560648_0001_r_000000_0\n",
            "2021-05-07 07:49:36,034 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:37,026 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:37,027 INFO mapreduce.Job: Job job_local1769560648_0001 completed successfully\n",
            "2021-05-07 07:49:37,038 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809169\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=692060160\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:37,041 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:49:40,748 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+10\n",
            "2021-05-07 07:49:42,018 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob383424240287377801.jar tmpDir=null\n",
            "2021-05-07 07:49:42,891 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:43,127 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:43,127 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:43,149 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:43,334 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:43,373 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:43,738 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1781355094_0001\n",
            "2021-05-07 07:49:43,738 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:44,201 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1781355094_0001_b70e267e-4f6f-4837-b37b-c2a5f22aaafa/centroids.txt\n",
            "2021-05-07 07:49:44,345 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:44,347 INFO mapreduce.Job: Running job: job_local1781355094_0001\n",
            "2021-05-07 07:49:44,355 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:44,359 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:44,369 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:44,369 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:44,428 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:44,434 INFO mapred.LocalJobRunner: Starting task: attempt_local1781355094_0001_m_000000_0\n",
            "2021-05-07 07:49:44,466 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:44,469 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:44,519 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:44,532 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:44,559 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:44,630 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:44,630 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:44,630 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:44,630 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:44,630 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:44,634 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:44,643 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:44,655 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:44,656 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:44,656 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:44,657 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:44,658 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:44,658 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:44,658 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:44,659 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:44,659 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:44,660 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:44,661 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:44,661 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:44,697 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:44,698 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:44,699 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:44,733 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:44,871 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:45,353 INFO mapreduce.Job: Job job_local1781355094_0001 running in uber mode : false\n",
            "2021-05-07 07:49:45,354 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:45,661 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:45,663 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:45,665 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:45,666 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:45,666 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:45,666 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:45,666 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:45,714 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:45,728 INFO mapred.Task: Task:attempt_local1781355094_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:45,731 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:45,731 INFO mapred.Task: Task 'attempt_local1781355094_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:45,740 INFO mapred.Task: Final Counters for attempt_local1781355094_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808231\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=332398592\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:45,740 INFO mapred.LocalJobRunner: Finishing task: attempt_local1781355094_0001_m_000000_0\n",
            "2021-05-07 07:49:45,740 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:45,745 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:45,747 INFO mapred.LocalJobRunner: Starting task: attempt_local1781355094_0001_r_000000_0\n",
            "2021-05-07 07:49:45,757 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:45,757 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:45,758 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:45,761 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33331c3b\n",
            "2021-05-07 07:49:45,764 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:45,792 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:45,799 INFO reduce.EventFetcher: attempt_local1781355094_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:45,846 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1781355094_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:45,860 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1781355094_0001_m_000000_0\n",
            "2021-05-07 07:49:45,864 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:45,873 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:45,878 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:45,878 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:45,885 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:45,885 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:45,925 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:45,926 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:45,934 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:45,934 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:45,936 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:45,937 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:45,946 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:45,952 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:45,953 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:45,975 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:45,975 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:45,978 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:45,993 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:46,323 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:46,347 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:46,348 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:46,349 INFO mapred.Task: Task:attempt_local1781355094_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:46,351 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:46,351 INFO mapred.Task: Task attempt_local1781355094_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:46,353 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1781355094_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:46,354 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:46,354 INFO mapred.Task: Task 'attempt_local1781355094_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:46,355 INFO mapred.Task: Final Counters for attempt_local1781355094_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000930\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=332398592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:46,355 INFO mapred.LocalJobRunner: Finishing task: attempt_local1781355094_0001_r_000000_0\n",
            "2021-05-07 07:49:46,355 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:46,358 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:46,359 INFO mapreduce.Job: Job job_local1781355094_0001 completed successfully\n",
            "2021-05-07 07:49:46,395 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809161\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=664797184\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:46,400 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:49:50,220 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+11\n",
            "2021-05-07 07:49:51,438 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob3784540985498612089.jar tmpDir=null\n",
            "2021-05-07 07:49:52,331 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:49:52,480 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:49:52,480 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:49:52,502 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:52,674 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:49:52,714 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:49:53,077 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local283491028_0001\n",
            "2021-05-07 07:49:53,078 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:49:53,577 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local283491028_0001_dadc1e25-0c67-41a2-8c80-db3a19a80838/centroids.txt\n",
            "2021-05-07 07:49:53,727 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:49:53,729 INFO mapreduce.Job: Running job: job_local283491028_0001\n",
            "2021-05-07 07:49:53,736 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:49:53,739 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:49:53,749 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:53,749 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:53,803 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:49:53,812 INFO mapred.LocalJobRunner: Starting task: attempt_local283491028_0001_m_000000_0\n",
            "2021-05-07 07:49:53,864 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:53,871 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:53,906 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:53,917 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:49:53,937 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:49:53,997 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:49:53,997 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:49:53,997 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:49:53,997 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:49:53,998 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:49:54,000 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:49:54,011 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:49:54,031 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:49:54,032 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:49:54,033 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:49:54,037 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:49:54,037 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:49:54,039 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:49:54,039 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:49:54,040 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:49:54,041 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:49:54,042 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:49:54,043 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:49:54,043 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:49:54,078 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:54,078 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:54,080 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:54,103 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:54,263 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:49:54,735 INFO mapreduce.Job: Job job_local283491028_0001 running in uber mode : false\n",
            "2021-05-07 07:49:54,736 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:49:55,054 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:55,057 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:55,061 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:49:55,061 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:49:55,061 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:49:55,061 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:49:55,062 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:49:55,125 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:49:55,147 INFO mapred.Task: Task:attempt_local283491028_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:55,154 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:49:55,154 INFO mapred.Task: Task 'attempt_local283491028_0001_m_000000_0' done.\n",
            "2021-05-07 07:49:55,164 INFO mapred.Task: Final Counters for attempt_local283491028_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=805250\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:49:55,164 INFO mapred.LocalJobRunner: Finishing task: attempt_local283491028_0001_m_000000_0\n",
            "2021-05-07 07:49:55,165 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:49:55,171 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:49:55,174 INFO mapred.LocalJobRunner: Starting task: attempt_local283491028_0001_r_000000_0\n",
            "2021-05-07 07:49:55,183 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:49:55,183 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:49:55,183 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:49:55,189 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3d49cc53\n",
            "2021-05-07 07:49:55,191 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:49:55,214 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:49:55,227 INFO reduce.EventFetcher: attempt_local283491028_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:49:55,268 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local283491028_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:49:55,279 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local283491028_0001_m_000000_0\n",
            "2021-05-07 07:49:55,284 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:49:55,288 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:49:55,291 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:55,291 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:49:55,300 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:55,301 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:55,331 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:49:55,332 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:49:55,333 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:49:55,333 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:49:55,334 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:49:55,335 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:55,354 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:49:55,359 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:49:55,361 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:49:55,391 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:55,391 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:55,392 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:55,399 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:49:55,734 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:49:55,739 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:49:55,768 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:49:55,769 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:49:55,770 INFO mapred.Task: Task:attempt_local283491028_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:49:55,771 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:49:55,771 INFO mapred.Task: Task attempt_local283491028_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:49:55,774 INFO output.FileOutputCommitter: Saved output of task 'attempt_local283491028_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:49:55,776 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:49:55,776 INFO mapred.Task: Task 'attempt_local283491028_0001_r_000000_0' done.\n",
            "2021-05-07 07:49:55,777 INFO mapred.Task: Final Counters for attempt_local283491028_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=997949\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:55,777 INFO mapred.LocalJobRunner: Finishing task: attempt_local283491028_0001_r_000000_0\n",
            "2021-05-07 07:49:55,778 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:49:56,740 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:49:56,741 INFO mapreduce.Job: Job job_local283491028_0001 completed successfully\n",
            "2021-05-07 07:49:56,753 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1803199\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=694157312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:49:56,753 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:50:00,427 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+12\n",
            "2021-05-07 07:50:01,661 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob2777767264884386823.jar tmpDir=null\n",
            "2021-05-07 07:50:02,411 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:02,614 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:02,615 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:02,639 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:02,833 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:02,868 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:03,242 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1008924518_0001\n",
            "2021-05-07 07:50:03,242 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:03,712 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1008924518_0001_478a7dea-8ca8-4f90-8607-07b65fd351c8/centroids.txt\n",
            "2021-05-07 07:50:03,849 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:03,851 INFO mapreduce.Job: Running job: job_local1008924518_0001\n",
            "2021-05-07 07:50:03,861 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:03,863 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:03,868 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:03,868 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:03,916 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:03,919 INFO mapred.LocalJobRunner: Starting task: attempt_local1008924518_0001_m_000000_0\n",
            "2021-05-07 07:50:03,955 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:03,958 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:03,990 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:04,008 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:50:04,049 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:04,116 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:04,116 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:04,116 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:04,116 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:04,116 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:04,119 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:04,128 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:50:04,140 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:04,144 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:04,145 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:04,145 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:04,146 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:04,146 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:04,146 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:04,147 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:04,152 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:04,153 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:04,154 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:04,154 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:04,198 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:04,198 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:04,200 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:04,225 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:04,388 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:50:04,859 INFO mapreduce.Job: Job job_local1008924518_0001 running in uber mode : false\n",
            "2021-05-07 07:50:04,861 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:05,180 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:05,181 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:05,186 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:50:05,186 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:50:05,186 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:50:05,186 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:50:05,186 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:50:05,243 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:50:05,260 INFO mapred.Task: Task:attempt_local1008924518_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:05,266 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:50:05,266 INFO mapred.Task: Task 'attempt_local1008924518_0001_m_000000_0' done.\n",
            "2021-05-07 07:50:05,275 INFO mapred.Task: Final Counters for attempt_local1008924518_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808233\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=314572800\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:50:05,275 INFO mapred.LocalJobRunner: Finishing task: attempt_local1008924518_0001_m_000000_0\n",
            "2021-05-07 07:50:05,275 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:50:05,282 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:50:05,287 INFO mapred.LocalJobRunner: Starting task: attempt_local1008924518_0001_r_000000_0\n",
            "2021-05-07 07:50:05,307 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:05,307 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:05,308 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:05,311 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@37b1557c\n",
            "2021-05-07 07:50:05,321 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:05,356 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:50:05,367 INFO reduce.EventFetcher: attempt_local1008924518_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:50:05,433 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1008924518_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:50:05,437 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1008924518_0001_m_000000_0\n",
            "2021-05-07 07:50:05,442 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:50:05,445 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:50:05,448 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:05,448 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:50:05,456 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:05,457 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:05,486 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:50:05,488 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:50:05,489 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:50:05,489 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:05,490 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:05,491 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:05,502 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:50:05,508 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:50:05,510 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:50:05,540 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:05,540 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:05,541 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:05,556 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:05,863 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:50:05,919 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:50:05,949 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:05,950 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:05,951 INFO mapred.Task: Task:attempt_local1008924518_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:05,954 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:05,954 INFO mapred.Task: Task attempt_local1008924518_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:50:05,956 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1008924518_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:50:05,960 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:50:05,960 INFO mapred.Task: Task 'attempt_local1008924518_0001_r_000000_0' done.\n",
            "2021-05-07 07:50:05,961 INFO mapred.Task: Final Counters for attempt_local1008924518_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000932\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=314572800\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:05,962 INFO mapred.LocalJobRunner: Finishing task: attempt_local1008924518_0001_r_000000_0\n",
            "2021-05-07 07:50:05,963 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:50:06,864 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:50:06,865 INFO mapreduce.Job: Job job_local1008924518_0001 completed successfully\n",
            "2021-05-07 07:50:06,874 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809165\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=629145600\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:06,875 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:50:10,542 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+13\n",
            "2021-05-07 07:50:11,818 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob17450377030447016137.jar tmpDir=null\n",
            "2021-05-07 07:50:12,636 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:12,841 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:12,841 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:12,864 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:13,005 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:13,039 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:13,354 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1988554101_0001\n",
            "2021-05-07 07:50:13,355 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:13,844 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1988554101_0001_4c6258e6-637e-40c8-8c64-8f44f1f45a8c/centroids.txt\n",
            "2021-05-07 07:50:13,978 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:13,980 INFO mapreduce.Job: Running job: job_local1988554101_0001\n",
            "2021-05-07 07:50:13,987 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:13,991 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:14,000 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:14,001 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:14,049 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:14,053 INFO mapred.LocalJobRunner: Starting task: attempt_local1988554101_0001_m_000000_0\n",
            "2021-05-07 07:50:14,091 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:14,094 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:14,142 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:14,162 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:50:14,181 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:14,243 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:14,243 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:14,243 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:14,243 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:14,243 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:14,246 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:14,255 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:50:14,270 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:14,275 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:14,277 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:14,277 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:14,278 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:14,278 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:14,278 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:14,279 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:14,279 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:14,280 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:14,280 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:14,281 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:14,326 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:14,327 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:14,329 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:14,343 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:14,491 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:50:14,986 INFO mapreduce.Job: Job job_local1988554101_0001 running in uber mode : false\n",
            "2021-05-07 07:50:14,987 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:15,285 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:15,287 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:15,293 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:50:15,294 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:50:15,294 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:50:15,294 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:50:15,294 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:50:15,351 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:50:15,365 INFO mapred.Task: Task:attempt_local1988554101_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:15,368 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:50:15,369 INFO mapred.Task: Task 'attempt_local1988554101_0001_m_000000_0' done.\n",
            "2021-05-07 07:50:15,379 INFO mapred.Task: Final Counters for attempt_local1988554101_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:50:15,379 INFO mapred.LocalJobRunner: Finishing task: attempt_local1988554101_0001_m_000000_0\n",
            "2021-05-07 07:50:15,379 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:50:15,384 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:50:15,388 INFO mapred.LocalJobRunner: Starting task: attempt_local1988554101_0001_r_000000_0\n",
            "2021-05-07 07:50:15,410 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:15,410 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:15,410 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:15,416 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7d5d06ee\n",
            "2021-05-07 07:50:15,417 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:15,441 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:50:15,443 INFO reduce.EventFetcher: attempt_local1988554101_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:50:15,486 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1988554101_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:50:15,490 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1988554101_0001_m_000000_0\n",
            "2021-05-07 07:50:15,494 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:50:15,497 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:50:15,502 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:15,503 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:50:15,510 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:15,510 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:15,547 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:50:15,548 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:50:15,549 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:50:15,549 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:15,550 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:15,551 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:15,564 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:50:15,571 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:50:15,583 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:50:15,607 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:15,607 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:15,609 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:15,627 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:15,964 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:50:15,988 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:15,989 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:50:15,990 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:15,991 INFO mapred.Task: Task:attempt_local1988554101_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:15,993 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:15,993 INFO mapred.Task: Task attempt_local1988554101_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:50:15,995 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1988554101_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:50:15,999 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:50:15,999 INFO mapred.Task: Task 'attempt_local1988554101_0001_r_000000_0' done.\n",
            "2021-05-07 07:50:16,000 INFO mapred.Task: Final Counters for attempt_local1988554101_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000934\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:16,000 INFO mapred.LocalJobRunner: Finishing task: attempt_local1988554101_0001_r_000000_0\n",
            "2021-05-07 07:50:16,001 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:50:16,991 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:50:16,991 INFO mapreduce.Job: Job job_local1988554101_0001 completed successfully\n",
            "2021-05-07 07:50:17,022 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809169\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=736100352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:17,022 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:50:20,804 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+14\n",
            "2021-05-07 07:50:22,101 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob15106893381709160134.jar tmpDir=null\n",
            "2021-05-07 07:50:22,903 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:23,104 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:23,104 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:23,128 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:23,301 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:23,345 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:23,717 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1199433597_0001\n",
            "2021-05-07 07:50:23,717 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:24,132 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1199433597_0001_1a074533-b718-4bd6-94bc-6d058b0fd034/centroids.txt\n",
            "2021-05-07 07:50:24,279 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:24,281 INFO mapreduce.Job: Running job: job_local1199433597_0001\n",
            "2021-05-07 07:50:24,289 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:24,293 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:24,303 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:24,303 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:24,356 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:24,366 INFO mapred.LocalJobRunner: Starting task: attempt_local1199433597_0001_m_000000_0\n",
            "2021-05-07 07:50:24,405 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:24,406 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:24,443 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:24,458 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:50:24,480 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:24,548 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:24,548 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:24,548 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:24,548 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:24,549 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:24,551 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:24,561 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:50:24,572 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:24,574 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:24,574 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:24,575 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:24,576 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:24,576 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:24,576 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:24,577 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:24,577 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:24,578 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:24,578 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:24,579 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:24,614 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:24,614 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:24,616 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:24,632 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:24,826 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:50:25,288 INFO mapreduce.Job: Job job_local1199433597_0001 running in uber mode : false\n",
            "2021-05-07 07:50:25,290 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:25,609 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:25,613 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:25,616 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:50:25,617 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:50:25,617 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:50:25,617 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:50:25,617 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:50:25,672 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:50:25,686 INFO mapred.Task: Task:attempt_local1199433597_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:25,688 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:50:25,688 INFO mapred.Task: Task 'attempt_local1199433597_0001_m_000000_0' done.\n",
            "2021-05-07 07:50:25,700 INFO mapred.Task: Final Counters for attempt_local1199433597_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:50:25,700 INFO mapred.LocalJobRunner: Finishing task: attempt_local1199433597_0001_m_000000_0\n",
            "2021-05-07 07:50:25,700 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:50:25,706 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:50:25,709 INFO mapred.LocalJobRunner: Starting task: attempt_local1199433597_0001_r_000000_0\n",
            "2021-05-07 07:50:25,719 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:25,720 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:25,720 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:25,726 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41e296ec\n",
            "2021-05-07 07:50:25,728 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:25,754 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:50:25,760 INFO reduce.EventFetcher: attempt_local1199433597_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:50:25,813 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1199433597_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:50:25,823 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1199433597_0001_m_000000_0\n",
            "2021-05-07 07:50:25,827 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:50:25,832 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:50:25,834 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:25,834 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:50:25,846 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:25,846 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:25,874 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:50:25,875 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:50:25,875 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:50:25,876 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:25,876 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:25,877 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:25,893 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:50:25,898 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:50:25,899 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:50:25,927 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:25,927 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:25,928 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:25,951 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:26,265 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:50:26,287 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:26,287 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:26,289 INFO mapred.Task: Task:attempt_local1199433597_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:26,290 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:26,290 INFO mapred.Task: Task attempt_local1199433597_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:50:26,292 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:50:26,293 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1199433597_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:50:26,294 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:50:26,294 INFO mapred.Task: Task 'attempt_local1199433597_0001_r_000000_0' done.\n",
            "2021-05-07 07:50:26,295 INFO mapred.Task: Final Counters for attempt_local1199433597_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000934\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=347078656\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:26,296 INFO mapred.LocalJobRunner: Finishing task: attempt_local1199433597_0001_r_000000_0\n",
            "2021-05-07 07:50:26,296 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:50:27,293 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:50:27,294 INFO mapreduce.Job: Job job_local1199433597_0001 completed successfully\n",
            "2021-05-07 07:50:27,307 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809169\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=694157312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:27,308 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:50:31,222 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+15\n",
            "2021-05-07 07:50:32,649 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob18275526128194149948.jar tmpDir=null\n",
            "2021-05-07 07:50:33,561 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:33,751 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:33,751 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:33,777 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:33,931 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:33,956 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:34,301 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1376192634_0001\n",
            "2021-05-07 07:50:34,301 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:34,795 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1376192634_0001_2279f399-1c94-434b-a92a-9e3bce4e56bf/centroids.txt\n",
            "2021-05-07 07:50:34,936 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:34,938 INFO mapreduce.Job: Running job: job_local1376192634_0001\n",
            "2021-05-07 07:50:34,945 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:34,948 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:34,958 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:34,958 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:35,030 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:35,036 INFO mapred.LocalJobRunner: Starting task: attempt_local1376192634_0001_m_000000_0\n",
            "2021-05-07 07:50:35,068 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:35,071 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:35,118 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:35,132 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:50:35,152 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:35,225 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:35,225 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:35,225 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:35,225 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:35,225 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:35,230 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:35,248 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:50:35,258 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:35,259 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:35,259 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:35,260 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:35,261 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:35,261 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:35,261 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:35,262 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:35,262 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:35,263 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:35,264 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:35,264 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:35,302 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:35,303 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:35,304 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:35,325 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:35,485 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:50:35,944 INFO mapreduce.Job: Job job_local1376192634_0001 running in uber mode : false\n",
            "2021-05-07 07:50:35,946 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:36,321 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:36,322 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:36,331 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:50:36,331 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:50:36,331 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:50:36,331 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:50:36,331 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:50:36,399 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:50:36,427 INFO mapred.Task: Task:attempt_local1376192634_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:36,437 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:50:36,437 INFO mapred.Task: Task 'attempt_local1376192634_0001_m_000000_0' done.\n",
            "2021-05-07 07:50:36,447 INFO mapred.Task: Final Counters for attempt_local1376192634_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162616\n",
            "\t\tFILE: Number of bytes written=808235\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:50:36,447 INFO mapred.LocalJobRunner: Finishing task: attempt_local1376192634_0001_m_000000_0\n",
            "2021-05-07 07:50:36,447 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:50:36,454 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:50:36,454 INFO mapred.LocalJobRunner: Starting task: attempt_local1376192634_0001_r_000000_0\n",
            "2021-05-07 07:50:36,467 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:36,467 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:36,475 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:36,480 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@43d366fb\n",
            "2021-05-07 07:50:36,483 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:36,509 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:50:36,519 INFO reduce.EventFetcher: attempt_local1376192634_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:50:36,578 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1376192634_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:50:36,589 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1376192634_0001_m_000000_0\n",
            "2021-05-07 07:50:36,593 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:50:36,599 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:50:36,613 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:36,614 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:50:36,624 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:36,624 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:36,673 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:50:36,674 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:50:36,675 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:50:36,675 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:36,678 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:36,679 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:36,689 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 07:50:36,697 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:50:36,699 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:50:36,730 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:36,731 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:36,733 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:36,753 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:36,949 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:50:37,147 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:50:37,187 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:37,189 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:37,190 INFO mapred.Task: Task:attempt_local1376192634_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:37,192 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:37,192 INFO mapred.Task: Task attempt_local1376192634_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:50:37,195 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1376192634_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 07:50:37,197 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:50:37,197 INFO mapred.Task: Task 'attempt_local1376192634_0001_r_000000_0' done.\n",
            "2021-05-07 07:50:37,198 INFO mapred.Task: Final Counters for attempt_local1376192634_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546142\n",
            "\t\tFILE: Number of bytes written=1000934\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=325058560\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:37,198 INFO mapred.LocalJobRunner: Finishing task: attempt_local1376192634_0001_r_000000_0\n",
            "2021-05-07 07:50:37,198 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:50:37,950 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:50:37,951 INFO mapreduce.Job: Job job_local1376192634_0001 completed successfully\n",
            "2021-05-07 07:50:37,966 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708758\n",
            "\t\tFILE: Number of bytes written=1809169\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=650117120\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=952\n",
            "2021-05-07 07:50:37,966 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 07:50:42,022 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "First Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 07:50:43,357 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids.txt] [] /tmp/streamjob10224607167433774226.jar tmpDir=null\n",
            "2021-05-07 07:50:44,324 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:44,482 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:44,483 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:44,514 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:44,672 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:44,724 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:45,147 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1910961706_0001\n",
            "2021-05-07 07:50:45,147 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:45,646 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1910961706_0001_bbf753f1-5722-46cb-b6be-ec6026cce7b2/centroids.txt\n",
            "2021-05-07 07:50:45,761 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:45,763 INFO mapreduce.Job: Running job: job_local1910961706_0001\n",
            "2021-05-07 07:50:45,771 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:45,774 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:45,779 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:45,780 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:45,837 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:45,842 INFO mapred.LocalJobRunner: Starting task: attempt_local1910961706_0001_m_000000_0\n",
            "2021-05-07 07:50:45,891 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:45,892 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:45,922 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:45,939 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+160983\n",
            "2021-05-07 07:50:45,965 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:46,044 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:46,044 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:46,044 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:46,045 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:46,045 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:46,048 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:46,058 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 07:50:46,077 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:46,077 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:46,078 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:46,078 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:46,079 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:46,079 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:46,080 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:46,084 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:46,085 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:46,086 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:46,086 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:46,086 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:46,143 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:46,144 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:46,146 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:46,168 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:46,356 INFO streaming.PipeMapRed: Records R/W=3679/1\n",
            "2021-05-07 07:50:46,769 INFO mapreduce.Job: Job job_local1910961706_0001 running in uber mode : false\n",
            "2021-05-07 07:50:46,771 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:47,201 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:47,202 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:47,207 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:50:47,207 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:50:47,207 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:50:47,207 INFO mapred.MapTask: bufstart = 0; bufend = 174165; bufvoid = 104857600\n",
            "2021-05-07 07:50:47,207 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26179248(104716992); length = 35149/6553600\n",
            "2021-05-07 07:50:47,267 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:50:47,285 INFO mapred.Task: Task:attempt_local1910961706_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:47,289 INFO mapred.LocalJobRunner: Records R/W=3679/1\n",
            "2021-05-07 07:50:47,289 INFO mapred.Task: Task 'attempt_local1910961706_0001_m_000000_0' done.\n",
            "2021-05-07 07:50:47,300 INFO mapred.Task: Final Counters for attempt_local1910961706_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=162574\n",
            "\t\tFILE: Number of bytes written=808215\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=8788\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=327155712\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "2021-05-07 07:50:47,300 INFO mapred.LocalJobRunner: Finishing task: attempt_local1910961706_0001_m_000000_0\n",
            "2021-05-07 07:50:47,300 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:50:47,305 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:50:47,307 INFO mapred.LocalJobRunner: Starting task: attempt_local1910961706_0001_r_000000_0\n",
            "2021-05-07 07:50:47,330 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:47,335 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:47,337 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:47,346 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@bea31e\n",
            "2021-05-07 07:50:47,350 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:47,386 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:50:47,390 INFO reduce.EventFetcher: attempt_local1910961706_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:50:47,460 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1910961706_0001_m_000000_0 decomp: 191743 len: 191747 to MEMORY\n",
            "2021-05-07 07:50:47,465 INFO reduce.InMemoryMapOutput: Read 191743 bytes from map-output for attempt_local1910961706_0001_m_000000_0\n",
            "2021-05-07 07:50:47,471 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 191743, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->191743\n",
            "2021-05-07 07:50:47,486 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:50:47,487 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:47,487 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:50:47,496 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:47,496 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:47,525 INFO reduce.MergeManagerImpl: Merged 1 segments, 191743 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:50:47,526 INFO reduce.MergeManagerImpl: Merging 1 files, 191747 bytes from disk\n",
            "2021-05-07 07:50:47,527 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:50:47,527 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:50:47,528 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 191740 bytes\n",
            "2021-05-07 07:50:47,530 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:47,544 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, u_reducer.py]\n",
            "2021-05-07 07:50:47,549 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:50:47,560 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:50:47,589 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:47,590 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:47,595 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:47,613 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:47,775 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:50:48,038 INFO streaming.PipeMapRed: Records R/W=8788/1\n",
            "2021-05-07 07:50:48,085 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:50:48,092 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:50:48,094 INFO mapred.Task: Task:attempt_local1910961706_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:50:48,095 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:50:48,096 INFO mapred.Task: Task attempt_local1910961706_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:50:48,098 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1910961706_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 07:50:48,099 INFO mapred.LocalJobRunner: Records R/W=8788/1 > reduce\n",
            "2021-05-07 07:50:48,100 INFO mapred.Task: Task 'attempt_local1910961706_0001_r_000000_0' done.\n",
            "2021-05-07 07:50:48,101 INFO mapred.Task: Final Counters for attempt_local1910961706_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=546100\n",
            "\t\tFILE: Number of bytes written=1000911\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=8788\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=327155712\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=949\n",
            "2021-05-07 07:50:48,101 INFO mapred.LocalJobRunner: Finishing task: attempt_local1910961706_0001_r_000000_0\n",
            "2021-05-07 07:50:48,101 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:50:48,776 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:50:48,777 INFO mapreduce.Job: Job job_local1910961706_0001 completed successfully\n",
            "2021-05-07 07:50:48,790 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=708674\n",
            "\t\tFILE: Number of bytes written=1809126\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=4394\n",
            "\t\tMap output records=8788\n",
            "\t\tMap output bytes=174165\n",
            "\t\tMap output materialized bytes=191747\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=191747\n",
            "\t\tReduce input records=8788\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=17576\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=654311424\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=160983\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=949\n",
            "2021-05-07 07:50:48,790 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 07:50:52,996 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Second Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 07:50:54,526 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids1.txt] [] /tmp/streamjob6781145133350012692.jar tmpDir=null\n",
            "2021-05-07 07:50:55,470 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 07:50:55,703 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 07:50:55,703 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 07:50:55,731 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:50:55,924 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 07:50:55,976 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 07:50:56,363 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1680296910_0001\n",
            "2021-05-07 07:50:56,363 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 07:50:56,835 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids1.txt as file:/tmp/hadoop-root/mapred/local/job_local1680296910_0001_b2a11ad8-e754-42f9-bb61-3a935d7cc7c4/centroids1.txt\n",
            "2021-05-07 07:50:56,960 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 07:50:56,963 INFO mapreduce.Job: Running job: job_local1680296910_0001\n",
            "2021-05-07 07:50:56,972 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 07:50:56,975 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 07:50:56,981 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:56,981 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:57,065 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 07:50:57,070 INFO mapred.LocalJobRunner: Starting task: attempt_local1680296910_0001_m_000000_0\n",
            "2021-05-07 07:50:57,113 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:50:57,116 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:50:57,153 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:50:57,172 INFO mapred.MapTask: Processing split: file:/content/test.txt:0+571582\n",
            "2021-05-07 07:50:57,196 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 07:50:57,277 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 07:50:57,277 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 07:50:57,278 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 07:50:57,278 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 07:50:57,278 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 07:50:57,282 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 07:50:57,292 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_mapper.py]\n",
            "2021-05-07 07:50:57,302 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 07:50:57,303 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 07:50:57,303 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 07:50:57,304 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 07:50:57,305 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 07:50:57,305 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 07:50:57,305 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 07:50:57,306 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 07:50:57,306 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 07:50:57,307 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 07:50:57,307 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 07:50:57,308 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 07:50:57,366 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:57,366 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:57,368 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:57,404 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:50:57,971 INFO mapreduce.Job: Job job_local1680296910_0001 running in uber mode : false\n",
            "2021-05-07 07:50:57,973 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 07:50:58,254 INFO streaming.PipeMapRed: Records R/W=7423/1\n",
            "2021-05-07 07:50:58,842 INFO streaming.PipeMapRed: R/W/S=10000/4097/0 in:10000=10000/1 [rec/s] out:4097=4097/1 [rec/s]\n",
            "2021-05-07 07:51:00,720 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:51:00,721 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:51:00,726 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 07:51:00,727 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 07:51:00,727 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 07:51:00,727 INFO mapred.MapTask: bufstart = 0; bufend = 62424; bufvoid = 104857600\n",
            "2021-05-07 07:51:00,727 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26151976(104607904); length = 62421/6553600\n",
            "2021-05-07 07:51:00,799 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 07:51:00,819 INFO mapred.Task: Task:attempt_local1680296910_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:51:00,823 INFO mapred.LocalJobRunner: Records R/W=7423/1\n",
            "2021-05-07 07:51:00,824 INFO mapred.Task: Task 'attempt_local1680296910_0001_m_000000_0' done.\n",
            "2021-05-07 07:51:00,834 INFO mapred.Task: Final Counters for attempt_local1680296910_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=573173\n",
            "\t\tFILE: Number of bytes written=710113\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=15606\n",
            "\t\tMap output records=15606\n",
            "\t\tMap output bytes=62424\n",
            "\t\tMap output materialized bytes=93642\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=15606\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=571582\n",
            "2021-05-07 07:51:00,834 INFO mapred.LocalJobRunner: Finishing task: attempt_local1680296910_0001_m_000000_0\n",
            "2021-05-07 07:51:00,834 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 07:51:00,841 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 07:51:00,844 INFO mapred.LocalJobRunner: Starting task: attempt_local1680296910_0001_r_000000_0\n",
            "2021-05-07 07:51:00,855 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 07:51:00,855 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 07:51:00,856 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 07:51:00,862 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b19cc67\n",
            "2021-05-07 07:51:00,864 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 07:51:00,896 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 07:51:00,914 INFO reduce.EventFetcher: attempt_local1680296910_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 07:51:00,981 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 07:51:00,990 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1680296910_0001_m_000000_0 decomp: 93638 len: 93642 to MEMORY\n",
            "2021-05-07 07:51:00,996 INFO reduce.InMemoryMapOutput: Read 93638 bytes from map-output for attempt_local1680296910_0001_m_000000_0\n",
            "2021-05-07 07:51:01,003 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 93638, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->93638\n",
            "2021-05-07 07:51:01,006 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 07:51:01,008 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:51:01,009 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 07:51:01,021 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:51:01,022 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93634 bytes\n",
            "2021-05-07 07:51:01,063 INFO reduce.MergeManagerImpl: Merged 1 segments, 93638 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 07:51:01,064 INFO reduce.MergeManagerImpl: Merging 1 files, 93642 bytes from disk\n",
            "2021-05-07 07:51:01,065 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 07:51:01,065 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 07:51:01,066 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 93634 bytes\n",
            "2021-05-07 07:51:01,067 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:51:01,084 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_reducer.py]\n",
            "2021-05-07 07:51:01,091 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 07:51:01,093 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 07:51:01,124 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:51:01,124 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:51:01,127 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:51:01,147 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:51:01,190 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 07:51:01,463 INFO streaming.PipeMapRed: Records R/W=15606/1\n",
            "2021-05-07 07:51:01,503 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 07:51:01,504 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 07:51:01,506 INFO mapred.Task: Task:attempt_local1680296910_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 07:51:01,507 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 07:51:01,507 INFO mapred.Task: Task attempt_local1680296910_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 07:51:01,508 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1680296910_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 07:51:01,511 INFO mapred.LocalJobRunner: Records R/W=15606/1 > reduce\n",
            "2021-05-07 07:51:01,511 INFO mapred.Task: Task 'attempt_local1680296910_0001_r_000000_0' done.\n",
            "2021-05-07 07:51:01,512 INFO mapred.Task: Final Counters for attempt_local1680296910_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=760489\n",
            "\t\tFILE: Number of bytes written=835280\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=19\n",
            "\t\tReduce shuffle bytes=93642\n",
            "\t\tReduce input records=15606\n",
            "\t\tReduce output records=19\n",
            "\t\tSpilled Records=15606\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=31525\n",
            "2021-05-07 07:51:01,512 INFO mapred.LocalJobRunner: Finishing task: attempt_local1680296910_0001_r_000000_0\n",
            "2021-05-07 07:51:01,512 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 07:51:01,982 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 07:51:01,983 INFO mapreduce.Job: Job job_local1680296910_0001 completed successfully\n",
            "2021-05-07 07:51:02,003 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1333662\n",
            "\t\tFILE: Number of bytes written=1545393\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=15606\n",
            "\t\tMap output records=15606\n",
            "\t\tMap output bytes=62424\n",
            "\t\tMap output materialized bytes=93642\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=19\n",
            "\t\tReduce shuffle bytes=93642\n",
            "\t\tReduce input records=15606\n",
            "\t\tReduce output records=19\n",
            "\t\tSpilled Records=31212\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=729808896\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=571582\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=31525\n",
            "2021-05-07 07:51:02,004 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 07:51:05,949 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Third Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEWVLasq88cE"
      },
      "source": [
        "#evaluation \n",
        "\n",
        "import seaborn as sb\n",
        "import string\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "def parse_args():\n",
        "    '''Parse command line arguments'''\n",
        "    parser = argparse.ArgumentParser(description=\"Run classification evaluation\")\n",
        "    \n",
        "    parser.add_argument('--input', nargs='?', default='predictions.txt',help='Input File path')\n",
        "    parser.add_argument('--output', nargs='?', default='performance.png',help='Visualization plot name')\n",
        "    \n",
        "    return parser.parse_args()\n",
        "\n",
        "def get_performance(args):\n",
        "    fp = open(args.input,'r')\n",
        "\n",
        "    # Processing alphabets and preparing confusion matrix\n",
        "    cm = np.zeros((26, 26))\n",
        "    alphabets = string.ascii_uppercase[:26]\n",
        "    alphabet_dict = dict()\n",
        "    x_axis_labels = []\n",
        "    y_axis_labels = []\n",
        "    for i in range(26):\n",
        "        x_axis_labels.append(alphabets[i])\n",
        "        y_axis_labels.append(alphabets[i])\n",
        "        alphabet_dict[alphabets[i]] = i\n",
        "\n",
        "    # Filling up the confusion matrix\n",
        "    for i,line in enumerate(fp):\n",
        "        line_split = line.split('\\t')[0].split(',')\n",
        "        actual = line_split[0]\n",
        "        predictions = list(line_split[1:])\n",
        "        total_pred = len(predictions)\n",
        "        for p in predictions:\n",
        "          cm[alphabet_dict[actual]][alphabet_dict[p]] += 1.0\n",
        "\n",
        "    fp.close()\n",
        "\n",
        "    # Calculate accuracy with total sum of the array and trace of the array\n",
        "    print('Classification Accuracy: ' + str((np.trace(cm)/np.sum(cm))*100) + '%')\n",
        "    print(cm)\n",
        "    \n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cm = np.true_divide(cm, cm.sum(axis=1, keepdims=True))\n",
        "    cm[np.isnan(cm)] = 0\n",
        "\n",
        "    # Get heatmap of the confusion matrix\n",
        "    ax = sb.heatmap(cm, vmin=0, vmax=1, xticklabels=x_axis_labels, yticklabels=y_axis_labels).set_title('Confusion matrix')\n",
        "    fig = ax.get_figure()\n",
        "    fig.savefig(args.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5BYB4lj0k2X",
        "outputId": "1061e214-81ae-4467-b7b2-a9b4308b4ad6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    get_performance(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Accuracy: 27.17544534153531%\n",
            "[[392.   0.   0.   0.   0.   0.   0.   0.   0. 121.   0.  12.   7.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0. 148.   2.  55.  46.   3.  67.   7.   2.   4.   5.  17.   0.   0.\n",
            "   41.   9.  52. 108. 103.   2.   0.   0.   0.  14.   0.  29.]\n",
            " [  0.   0. 228.   0. 133.   0.  36.   0.   3.   0.   9.   2.   0.   0.\n",
            "    0.   0.   0.   0.  10.   0.   0.   0.   0.   0.   0.   8.]\n",
            " [ 17.  67.   0. 158.   9.   8.   1.  62.  20.  26.  18.  15.  57.  69.\n",
            "   13.  28.   3.  74.  22.   0.  11.   0.   0.  46.   3.  13.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.  43.   0.  27. 152.   0.   9.   0.   0.  20.   0.   0.   0.\n",
            "    0.  75.   0.   0.  15.  92.  77.   2.   0.  17.  70.   3.]\n",
            " [ 35.  63.  53.  55.  50.   0.  61.  22.  44.   0.  48.   0.   1.   2.\n",
            "   61.  31.  43.  66.  29.   0.   3.   0.   0.  61.   0.  49.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.  15.   1.  10.  39.   4.   0.   5. 222. 160.  14.  77.   0.   1.\n",
            "    0.   4.   0.   1.  24.   0.   0.   0.   0.   6.   0.  31.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.  28.   5.  20.  86.   2.   4.  41.  60.  58.  67. 359.   1.   8.\n",
            "    1.   0.   4.   1.  88.   6.   0.   0.   0.  93.   0.  94.]\n",
            " [ 17.  70.  37.  24.  45.  20.  66. 173.   3.   7.  55.  17. 333. 146.\n",
            "   28.  25.  70.  51.  38.  15.  88.  36.  91.  23.  17.   3.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.  34.  37. 128.   5.   3. 149.  42.  30.   9.  61.  11.   7.  76.\n",
            "  267.  41. 143.  67.   9.  18. 103.   3.   6.  56.   2.   0.]\n",
            " [  0.  18.  43.  38.  27. 292.  30.  36. 118. 126.  19.   5.   0.  27.\n",
            "   32. 355.   2.   6.  38. 139.   7.  76.   0.  23.  59.  38.]\n",
            " [  6.   9.  20.   1.  19.   1. 133.  53.   1.   2.  78.   4.  53.   8.\n",
            "  112.   4. 191.  31.  10.   2.  28.   4.   8.  63.   5.   0.]\n",
            " [102.  88.   0.  74.  17.   9.  21.  36.  39.  13.  49.  27.  39.  27.\n",
            "   13.   2.  63. 102.  48.   0.   4.   0.   0.  17.   0.  26.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   2.   0.   0.  12.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0. 141.  37. 140.   3.   0. 120.   0.]\n",
            " [  0.   0.  83.   0.  30.  40.   2.  60.   0.  18.  89.   0.  59.  90.\n",
            "    0.   2.   0.   0.  10.  31. 251.  88. 100.  53. 105.   4.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   1.   0.   0. 110.   7. 215.  32.   0. 150.   0.]\n",
            " [ 23.   9.   0.  13.   2.  34.  12.  18.   8.  13.   0.  19.  58. 151.\n",
            "    1.  47.  25.  16.  13.   6.  13.  29. 348.   8.  81.   8.]\n",
            " [ 26.  55.  15.  47.  62.  20.  19.   9.  25.  28.  48.  29.   3.   5.\n",
            "   19.   4.  20.  65.  63.  54.   3.   6.   0. 111.   8.  59.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   2.   0.   0.   0.  16.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.  61.   0.   0.   0.   0.  22.   0. 207.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WA_etWs9A_T"
      },
      "source": [
        "### For n=40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3vDduyQ-Afx",
        "outputId": "d82cbd0a-9903-4cf0-c94f-73c07c343f9b"
      },
      "source": [
        "!sh model.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count---------+1\n",
            "2021-05-07 08:36:59,184 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob11225168224082914421.jar tmpDir=null\n",
            "2021-05-07 08:37:00,064 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:00,231 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:00,232 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:00,257 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:00,447 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:00,488 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:00,840 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1593543342_0001\n",
            "2021-05-07 08:37:00,840 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:01,286 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1593543342_0001_1a9c23d7-20dc-4703-a38f-d1af7ed95d24/centroids.txt\n",
            "2021-05-07 08:37:01,399 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:01,401 INFO mapreduce.Job: Running job: job_local1593543342_0001\n",
            "2021-05-07 08:37:01,407 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:01,409 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:01,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:01,414 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:01,466 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:01,471 INFO mapred.LocalJobRunner: Starting task: attempt_local1593543342_0001_m_000000_0\n",
            "2021-05-07 08:37:01,502 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:01,505 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:01,553 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:01,580 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:01,609 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:01,690 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:01,690 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:01,690 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:01,690 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:01,690 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:01,694 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:01,703 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:01,727 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:01,728 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:01,728 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:01,729 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:01,730 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:01,730 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:01,730 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:01,731 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:01,731 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:01,732 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:01,732 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:01,733 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:01,777 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:01,777 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:01,779 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:01,795 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:01,941 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:02,406 INFO mapreduce.Job: Job job_local1593543342_0001 running in uber mode : false\n",
            "2021-05-07 08:37:02,407 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:03,111 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:03,112 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:03,116 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:03,116 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:03,116 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:03,116 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:03,116 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:03,187 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:03,202 INFO mapred.Task: Task:attempt_local1593543342_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:03,207 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:03,207 INFO mapred.Task: Task 'attempt_local1593543342_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:03,215 INFO mapred.Task: Final Counters for attempt_local1593543342_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903210\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=332398592\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:03,215 INFO mapred.LocalJobRunner: Finishing task: attempt_local1593543342_0001_m_000000_0\n",
            "2021-05-07 08:37:03,215 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:03,219 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:03,221 INFO mapred.LocalJobRunner: Starting task: attempt_local1593543342_0001_r_000000_0\n",
            "2021-05-07 08:37:03,233 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:03,233 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:03,233 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:03,242 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7dbac6ef\n",
            "2021-05-07 08:37:03,243 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:03,264 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:03,267 INFO reduce.EventFetcher: attempt_local1593543342_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:03,314 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1593543342_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:03,322 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1593543342_0001_m_000000_0\n",
            "2021-05-07 08:37:03,326 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:03,331 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:03,333 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:03,333 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:03,340 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:03,340 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:03,384 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:03,384 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:03,385 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:03,385 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:03,387 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:03,389 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:03,403 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:03,408 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:03,409 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:03,410 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:03,426 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:03,427 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:03,428 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:03,444 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:03,714 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:03,815 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:03,836 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:03,837 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:03,840 INFO mapred.Task: Task:attempt_local1593543342_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:03,842 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:03,842 INFO mapred.Task: Task attempt_local1593543342_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:03,844 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1593543342_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:03,845 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:03,845 INFO mapred.Task: Task 'attempt_local1593543342_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:03,846 INFO mapred.Task: Final Counters for attempt_local1593543342_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190908\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=332398592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:03,846 INFO mapred.LocalJobRunner: Finishing task: attempt_local1593543342_0001_r_000000_0\n",
            "2021-05-07 08:37:03,846 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:04,410 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:04,411 INFO mapreduce.Job: Job job_local1593543342_0001 completed successfully\n",
            "2021-05-07 08:37:04,429 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094118\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=664797184\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:04,429 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:08,006 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+2\n",
            "2021-05-07 08:37:09,242 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob2023513414593641111.jar tmpDir=null\n",
            "2021-05-07 08:37:09,949 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:10,103 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:10,103 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:10,135 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:10,315 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:10,349 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:10,695 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1135705561_0001\n",
            "2021-05-07 08:37:10,695 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:11,158 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1135705561_0001_653bb6b6-8e9a-4f3f-8b4d-c345e465a6ff/centroids.txt\n",
            "2021-05-07 08:37:11,301 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:11,303 INFO mapreduce.Job: Running job: job_local1135705561_0001\n",
            "2021-05-07 08:37:11,311 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:11,313 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:11,324 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:11,324 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:11,379 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:11,384 INFO mapred.LocalJobRunner: Starting task: attempt_local1135705561_0001_m_000000_0\n",
            "2021-05-07 08:37:11,428 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:11,431 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:11,474 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:11,483 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:11,500 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:11,572 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:11,572 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:11,572 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:11,572 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:11,572 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:11,576 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:11,591 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:11,608 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:11,609 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:11,617 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:11,617 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:11,618 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:11,618 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:11,619 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:11,619 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:11,619 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:11,620 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:11,621 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:11,621 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:11,662 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:11,663 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:11,665 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:11,700 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:11,847 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:12,309 INFO mapreduce.Job: Job job_local1135705561_0001 running in uber mode : false\n",
            "2021-05-07 08:37:12,310 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:13,016 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:13,016 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:13,019 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:13,019 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:13,019 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:13,019 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:13,019 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:13,082 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:13,099 INFO mapred.Task: Task:attempt_local1135705561_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:13,105 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:13,105 INFO mapred.Task: Task 'attempt_local1135705561_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:13,114 INFO mapred.Task: Final Counters for attempt_local1135705561_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:13,115 INFO mapred.LocalJobRunner: Finishing task: attempt_local1135705561_0001_m_000000_0\n",
            "2021-05-07 08:37:13,115 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:13,118 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:13,118 INFO mapred.LocalJobRunner: Starting task: attempt_local1135705561_0001_r_000000_0\n",
            "2021-05-07 08:37:13,129 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:13,129 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:13,131 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:13,135 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@16ec4847\n",
            "2021-05-07 08:37:13,139 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:13,159 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:13,161 INFO reduce.EventFetcher: attempt_local1135705561_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:13,209 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1135705561_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:13,218 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1135705561_0001_m_000000_0\n",
            "2021-05-07 08:37:13,223 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:13,227 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:13,239 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:13,239 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:13,252 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:13,253 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:13,293 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:13,294 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:13,294 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:13,295 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:13,296 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:13,296 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:13,304 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:13,310 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:13,311 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:13,313 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:13,335 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:13,336 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:13,337 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:13,344 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:13,610 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:13,697 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:13,718 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:13,719 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:13,720 INFO mapred.Task: Task:attempt_local1135705561_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:13,721 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:13,722 INFO mapred.Task: Task attempt_local1135705561_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:13,724 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1135705561_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:13,725 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:13,725 INFO mapred.Task: Task 'attempt_local1135705561_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:13,726 INFO mapred.Task: Final Counters for attempt_local1135705561_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=358612992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:13,726 INFO mapred.LocalJobRunner: Finishing task: attempt_local1135705561_0001_r_000000_0\n",
            "2021-05-07 08:37:13,726 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:14,314 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:14,315 INFO mapreduce.Job: Job job_local1135705561_0001 completed successfully\n",
            "2021-05-07 08:37:14,332 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=717225984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:14,334 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:17,879 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+3\n",
            "2021-05-07 08:37:19,062 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob5268673594080813733.jar tmpDir=null\n",
            "2021-05-07 08:37:19,954 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:20,151 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:20,152 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:20,176 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:20,372 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:20,404 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:20,754 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1693692568_0001\n",
            "2021-05-07 08:37:20,755 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:21,172 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1693692568_0001_52d78677-9d73-4e29-958e-467b73dbf7ce/centroids.txt\n",
            "2021-05-07 08:37:21,272 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:21,273 INFO mapreduce.Job: Running job: job_local1693692568_0001\n",
            "2021-05-07 08:37:21,281 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:21,284 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:21,289 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:21,289 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:21,336 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:21,339 INFO mapred.LocalJobRunner: Starting task: attempt_local1693692568_0001_m_000000_0\n",
            "2021-05-07 08:37:21,373 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:21,376 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:21,412 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:21,422 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:21,449 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:21,524 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:21,524 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:21,524 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:21,524 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:21,525 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:21,527 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:21,543 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:21,553 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:21,553 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:21,554 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:21,555 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:21,556 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:21,556 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:21,556 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:21,557 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:21,557 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:21,558 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:21,559 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:21,559 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:21,605 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:21,606 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:21,607 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:21,626 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:21,760 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:22,279 INFO mapreduce.Job: Job job_local1693692568_0001 running in uber mode : false\n",
            "2021-05-07 08:37:22,280 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:22,926 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:22,927 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:22,930 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:22,930 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:22,930 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:22,931 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:22,931 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:22,996 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:23,016 INFO mapred.Task: Task:attempt_local1693692568_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:23,020 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:23,020 INFO mapred.Task: Task 'attempt_local1693692568_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:23,029 INFO mapred.Task: Final Counters for attempt_local1693692568_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:23,029 INFO mapred.LocalJobRunner: Finishing task: attempt_local1693692568_0001_m_000000_0\n",
            "2021-05-07 08:37:23,029 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:23,032 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:23,033 INFO mapred.LocalJobRunner: Starting task: attempt_local1693692568_0001_r_000000_0\n",
            "2021-05-07 08:37:23,047 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:23,047 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:23,049 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:23,052 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@44c00489\n",
            "2021-05-07 08:37:23,053 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:23,076 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:23,077 INFO reduce.EventFetcher: attempt_local1693692568_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:23,150 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1693692568_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:23,158 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1693692568_0001_m_000000_0\n",
            "2021-05-07 08:37:23,161 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:23,166 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:23,168 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:23,168 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:23,176 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:23,176 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:23,228 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:23,229 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:23,230 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:23,230 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:23,231 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:23,232 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:23,245 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:23,251 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:23,252 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:23,268 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:23,268 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:23,271 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:23,282 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:23,290 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:23,633 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:23,693 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:23,710 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:23,711 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:23,712 INFO mapred.Task: Task:attempt_local1693692568_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:23,713 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:23,713 INFO mapred.Task: Task attempt_local1693692568_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:23,717 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1693692568_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:23,719 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:23,719 INFO mapred.Task: Task 'attempt_local1693692568_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:23,720 INFO mapred.Task: Final Counters for attempt_local1693692568_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=376438784\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:23,720 INFO mapred.LocalJobRunner: Finishing task: attempt_local1693692568_0001_r_000000_0\n",
            "2021-05-07 08:37:23,720 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:24,283 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:24,284 INFO mapreduce.Job: Job job_local1693692568_0001 completed successfully\n",
            "2021-05-07 08:37:24,301 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=752877568\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:24,302 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:27,911 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+4\n",
            "2021-05-07 08:37:29,185 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob5233799315953217208.jar tmpDir=null\n",
            "2021-05-07 08:37:29,928 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:30,082 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:30,083 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:30,108 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:30,307 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:30,346 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:30,697 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1699321439_0001\n",
            "2021-05-07 08:37:30,697 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:31,085 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1699321439_0001_2a87cf7e-f9d4-4c99-83a9-fa112bfd5d3f/centroids.txt\n",
            "2021-05-07 08:37:31,246 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:31,248 INFO mapreduce.Job: Running job: job_local1699321439_0001\n",
            "2021-05-07 08:37:31,254 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:31,258 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:31,263 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:31,263 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:31,325 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:31,344 INFO mapred.LocalJobRunner: Starting task: attempt_local1699321439_0001_m_000000_0\n",
            "2021-05-07 08:37:31,383 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:31,385 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:31,424 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:31,444 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:31,481 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:31,550 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:31,550 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:31,550 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:31,550 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:31,550 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:31,556 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:31,567 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:31,574 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:31,575 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:31,576 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:31,576 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:31,577 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:31,577 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:31,578 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:31,578 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:31,579 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:31,580 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:31,580 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:31,580 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:31,623 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:31,624 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:31,626 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:31,642 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:31,782 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:32,252 INFO mapreduce.Job: Job job_local1699321439_0001 running in uber mode : false\n",
            "2021-05-07 08:37:32,254 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:32,976 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:32,976 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:32,979 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:32,979 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:32,980 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:32,980 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:32,980 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:33,044 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:33,057 INFO mapred.Task: Task:attempt_local1699321439_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:33,060 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:33,060 INFO mapred.Task: Task 'attempt_local1699321439_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:33,069 INFO mapred.Task: Final Counters for attempt_local1699321439_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:33,069 INFO mapred.LocalJobRunner: Finishing task: attempt_local1699321439_0001_m_000000_0\n",
            "2021-05-07 08:37:33,069 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:33,073 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:33,078 INFO mapred.LocalJobRunner: Starting task: attempt_local1699321439_0001_r_000000_0\n",
            "2021-05-07 08:37:33,091 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:33,091 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:33,091 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:33,099 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3a0390a5\n",
            "2021-05-07 08:37:33,101 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:33,121 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:33,123 INFO reduce.EventFetcher: attempt_local1699321439_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:33,171 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1699321439_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:33,180 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1699321439_0001_m_000000_0\n",
            "2021-05-07 08:37:33,182 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:33,186 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:33,188 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:33,188 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:33,195 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:33,195 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:33,221 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:33,222 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:33,222 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:33,223 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:33,224 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:33,224 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:33,250 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:33,255 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:33,256 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:33,256 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:33,281 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:33,282 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:33,283 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:33,289 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:33,627 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:33,730 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:33,761 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:33,761 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:33,762 INFO mapred.Task: Task:attempt_local1699321439_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:33,764 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:33,764 INFO mapred.Task: Task attempt_local1699321439_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:33,766 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1699321439_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:33,769 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:33,769 INFO mapred.Task: Task 'attempt_local1699321439_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:33,770 INFO mapred.Task: Final Counters for attempt_local1699321439_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:33,770 INFO mapred.LocalJobRunner: Finishing task: attempt_local1699321439_0001_r_000000_0\n",
            "2021-05-07 08:37:33,770 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:34,257 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:34,258 INFO mapreduce.Job: Job job_local1699321439_0001 completed successfully\n",
            "2021-05-07 08:37:34,271 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=725614592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:34,271 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:37,909 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+5\n",
            "2021-05-07 08:37:39,192 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob8454074770931526159.jar tmpDir=null\n",
            "2021-05-07 08:37:39,987 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:40,152 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:40,153 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:40,176 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:40,312 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:40,331 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:40,681 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local905247377_0001\n",
            "2021-05-07 08:37:40,681 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:41,112 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local905247377_0001_8a387021-8b12-4a2f-a3f5-0a8eee48c9e3/centroids.txt\n",
            "2021-05-07 08:37:41,226 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:41,228 INFO mapreduce.Job: Running job: job_local905247377_0001\n",
            "2021-05-07 08:37:41,234 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:41,236 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:41,241 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:41,241 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:41,291 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:41,300 INFO mapred.LocalJobRunner: Starting task: attempt_local905247377_0001_m_000000_0\n",
            "2021-05-07 08:37:41,336 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:41,337 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:41,371 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:41,381 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:41,406 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:41,480 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:41,480 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:41,480 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:41,480 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:41,480 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:41,484 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:41,500 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:41,509 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:41,509 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:41,510 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:41,510 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:41,511 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:41,511 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:41,512 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:41,512 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:41,512 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:41,513 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:41,513 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:41,514 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:41,543 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:41,544 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:41,545 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:41,571 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:41,717 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:42,232 INFO mapreduce.Job: Job job_local905247377_0001 running in uber mode : false\n",
            "2021-05-07 08:37:42,234 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:42,872 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:42,873 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:42,876 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:42,877 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:42,877 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:42,877 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:42,878 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:42,942 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:42,955 INFO mapred.Task: Task:attempt_local905247377_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:42,958 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:42,958 INFO mapred.Task: Task 'attempt_local905247377_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:42,977 INFO mapred.Task: Final Counters for attempt_local905247377_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=900225\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:42,978 INFO mapred.LocalJobRunner: Finishing task: attempt_local905247377_0001_m_000000_0\n",
            "2021-05-07 08:37:42,980 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:42,984 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:42,985 INFO mapred.LocalJobRunner: Starting task: attempt_local905247377_0001_r_000000_0\n",
            "2021-05-07 08:37:43,000 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:43,000 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:43,001 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:43,003 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5f05af38\n",
            "2021-05-07 08:37:43,008 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:43,031 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:43,035 INFO reduce.EventFetcher: attempt_local905247377_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:43,072 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local905247377_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:43,076 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local905247377_0001_m_000000_0\n",
            "2021-05-07 08:37:43,078 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:43,081 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:43,082 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:43,082 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:43,089 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:43,089 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:43,122 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:43,123 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:43,124 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:43,124 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:43,125 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:43,126 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:43,139 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:43,146 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:43,148 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:43,173 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:43,174 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:43,175 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:43,187 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:43,236 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:43,481 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:43,570 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:43,592 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:43,593 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:43,594 INFO mapred.Task: Task:attempt_local905247377_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:43,595 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:43,596 INFO mapred.Task: Task attempt_local905247377_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:43,601 INFO output.FileOutputCommitter: Saved output of task 'attempt_local905247377_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:43,602 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:43,603 INFO mapred.Task: Task 'attempt_local905247377_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:43,604 INFO mapred.Task: Final Counters for attempt_local905247377_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1187923\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=363855872\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:43,604 INFO mapred.LocalJobRunner: Finishing task: attempt_local905247377_0001_r_000000_0\n",
            "2021-05-07 08:37:43,604 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:44,237 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:44,238 INFO mapreduce.Job: Job job_local905247377_0001 completed successfully\n",
            "2021-05-07 08:37:44,250 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2088148\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=727711744\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:44,250 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:47,767 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+6\n",
            "2021-05-07 08:37:49,059 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob8929400455825992251.jar tmpDir=null\n",
            "2021-05-07 08:37:49,817 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:49,965 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:49,965 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:49,987 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:50,114 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:37:50,133 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:37:50,402 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local103737810_0001\n",
            "2021-05-07 08:37:50,402 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:37:50,814 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local103737810_0001_f4f3323e-9ffc-4965-a069-e665533a2c45/centroids.txt\n",
            "2021-05-07 08:37:50,937 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:37:50,939 INFO mapreduce.Job: Running job: job_local103737810_0001\n",
            "2021-05-07 08:37:50,945 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:37:50,947 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:37:50,952 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:50,952 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:50,998 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:37:51,007 INFO mapred.LocalJobRunner: Starting task: attempt_local103737810_0001_m_000000_0\n",
            "2021-05-07 08:37:51,052 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:51,052 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:51,078 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:51,095 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:37:51,117 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:37:51,188 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:37:51,188 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:37:51,188 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:37:51,188 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:37:51,188 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:37:51,192 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:37:51,204 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:37:51,212 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:37:51,213 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:37:51,213 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:37:51,213 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:37:51,214 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:37:51,214 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:37:51,215 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:37:51,215 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:37:51,216 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:37:51,216 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:37:51,217 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:37:51,217 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:37:51,248 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:51,248 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:51,250 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:51,262 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:51,399 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:37:51,944 INFO mapreduce.Job: Job job_local103737810_0001 running in uber mode : false\n",
            "2021-05-07 08:37:51,945 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:37:52,571 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:52,572 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:52,575 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:37:52,575 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:37:52,576 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:37:52,576 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:37:52,576 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:37:52,644 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:37:52,658 INFO mapred.Task: Task:attempt_local103737810_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:52,661 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:37:52,661 INFO mapred.Task: Task 'attempt_local103737810_0001_m_000000_0' done.\n",
            "2021-05-07 08:37:52,670 INFO mapred.Task: Final Counters for attempt_local103737810_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=900225\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:37:52,670 INFO mapred.LocalJobRunner: Finishing task: attempt_local103737810_0001_m_000000_0\n",
            "2021-05-07 08:37:52,670 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:37:52,674 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:37:52,675 INFO mapred.LocalJobRunner: Starting task: attempt_local103737810_0001_r_000000_0\n",
            "2021-05-07 08:37:52,685 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:37:52,685 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:37:52,686 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:37:52,691 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5576b588\n",
            "2021-05-07 08:37:52,693 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:37:52,721 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:37:52,723 INFO reduce.EventFetcher: attempt_local103737810_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:37:52,771 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local103737810_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:37:52,778 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local103737810_0001_m_000000_0\n",
            "2021-05-07 08:37:52,782 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:37:52,786 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:37:52,789 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:52,789 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:37:52,796 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:52,796 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:52,822 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:37:52,822 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:37:52,823 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:37:52,823 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:37:52,824 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:37:52,825 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:52,835 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:37:52,842 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:37:52,844 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:37:52,868 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:52,868 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:52,869 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:52,881 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:52,949 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:37:53,171 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:37:53,246 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:37:53,263 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:37:53,264 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:37:53,264 INFO mapred.Task: Task:attempt_local103737810_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:37:53,266 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:37:53,266 INFO mapred.Task: Task attempt_local103737810_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:37:53,268 INFO output.FileOutputCommitter: Saved output of task 'attempt_local103737810_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:37:53,270 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:37:53,270 INFO mapred.Task: Task 'attempt_local103737810_0001_r_000000_0' done.\n",
            "2021-05-07 08:37:53,271 INFO mapred.Task: Final Counters for attempt_local103737810_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1187923\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:53,272 INFO mapred.LocalJobRunner: Finishing task: attempt_local103737810_0001_r_000000_0\n",
            "2021-05-07 08:37:53,273 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:37:53,950 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:37:53,951 INFO mapreduce.Job: Job job_local103737810_0001 completed successfully\n",
            "2021-05-07 08:37:53,964 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2088148\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=656408576\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:37:53,964 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:37:57,661 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+7\n",
            "2021-05-07 08:37:58,865 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob4476341093196853961.jar tmpDir=null\n",
            "2021-05-07 08:37:59,634 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:37:59,841 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:37:59,842 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:37:59,868 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:00,031 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:00,061 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:00,411 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1582740578_0001\n",
            "2021-05-07 08:38:00,411 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:00,807 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1582740578_0001_b6807dd3-bb67-4908-bb87-4cb3f8106c70/centroids.txt\n",
            "2021-05-07 08:38:00,930 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:00,932 INFO mapreduce.Job: Running job: job_local1582740578_0001\n",
            "2021-05-07 08:38:00,939 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:00,941 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:00,951 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:00,951 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:01,005 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:01,015 INFO mapred.LocalJobRunner: Starting task: attempt_local1582740578_0001_m_000000_0\n",
            "2021-05-07 08:38:01,081 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:01,084 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:01,120 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:01,132 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:01,149 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:01,214 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:01,214 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:01,214 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:01,214 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:01,214 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:01,219 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:01,233 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:01,241 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:01,241 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:01,242 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:01,242 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:01,243 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:01,243 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:01,243 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:01,244 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:01,244 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:01,245 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:01,246 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:01,246 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:01,275 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:01,276 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:01,278 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:01,291 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:01,432 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:01,938 INFO mapreduce.Job: Job job_local1582740578_0001 running in uber mode : false\n",
            "2021-05-07 08:38:01,939 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:02,617 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:02,618 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:02,621 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:02,621 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:02,622 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:02,622 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:02,622 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:02,682 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:02,694 INFO mapred.Task: Task:attempt_local1582740578_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:02,698 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:02,698 INFO mapred.Task: Task 'attempt_local1582740578_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:02,712 INFO mapred.Task: Final Counters for attempt_local1582740578_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=318767104\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:02,712 INFO mapred.LocalJobRunner: Finishing task: attempt_local1582740578_0001_m_000000_0\n",
            "2021-05-07 08:38:02,712 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:02,716 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:02,719 INFO mapred.LocalJobRunner: Starting task: attempt_local1582740578_0001_r_000000_0\n",
            "2021-05-07 08:38:02,729 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:02,729 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:02,729 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:02,735 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f6670e4\n",
            "2021-05-07 08:38:02,737 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:02,763 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:02,765 INFO reduce.EventFetcher: attempt_local1582740578_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:02,799 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1582740578_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:02,802 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1582740578_0001_m_000000_0\n",
            "2021-05-07 08:38:02,807 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:02,813 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:02,815 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:02,815 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:02,823 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:02,823 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:02,860 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:02,861 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:02,862 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:02,862 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:02,864 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:02,866 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:02,876 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:02,884 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:02,886 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:02,909 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:02,911 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:02,912 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:02,929 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:02,942 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:03,203 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:03,311 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:03,333 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:03,334 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:03,335 INFO mapred.Task: Task:attempt_local1582740578_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:03,336 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:03,336 INFO mapred.Task: Task attempt_local1582740578_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:03,338 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1582740578_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:03,339 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:03,339 INFO mapred.Task: Task 'attempt_local1582740578_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:03,340 INFO mapred.Task: Final Counters for attempt_local1582740578_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=318767104\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:03,340 INFO mapred.LocalJobRunner: Finishing task: attempt_local1582740578_0001_r_000000_0\n",
            "2021-05-07 08:38:03,340 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:03,942 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:03,943 INFO mapreduce.Job: Job job_local1582740578_0001 completed successfully\n",
            "2021-05-07 08:38:03,955 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=637534208\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:03,957 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:07,561 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+8\n",
            "2021-05-07 08:38:08,756 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob2078031249111318497.jar tmpDir=null\n",
            "2021-05-07 08:38:09,532 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:09,740 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:09,741 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:09,766 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:09,921 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:09,941 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:10,302 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1247357783_0001\n",
            "2021-05-07 08:38:10,302 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:10,737 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1247357783_0001_531f3f9a-3370-4aa1-950b-94e108bc4ce7/centroids.txt\n",
            "2021-05-07 08:38:10,856 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:10,858 INFO mapreduce.Job: Running job: job_local1247357783_0001\n",
            "2021-05-07 08:38:10,864 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:10,867 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:10,876 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:10,876 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:10,922 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:10,930 INFO mapred.LocalJobRunner: Starting task: attempt_local1247357783_0001_m_000000_0\n",
            "2021-05-07 08:38:10,979 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:10,982 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:11,016 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:11,025 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:11,045 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:11,110 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:11,111 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:11,111 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:11,111 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:11,111 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:11,114 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:11,124 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:11,135 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:11,136 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:11,137 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:11,137 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:11,138 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:11,138 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:11,139 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:11,139 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:11,139 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:11,140 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:11,141 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:11,141 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:11,178 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:11,179 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:11,181 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:11,205 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:11,353 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:11,863 INFO mapreduce.Job: Job job_local1247357783_0001 running in uber mode : false\n",
            "2021-05-07 08:38:11,865 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:12,534 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:12,535 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:12,539 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:12,539 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:12,539 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:12,539 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:12,539 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:12,600 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:12,614 INFO mapred.Task: Task:attempt_local1247357783_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:12,617 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:12,617 INFO mapred.Task: Task 'attempt_local1247357783_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:12,626 INFO mapred.Task: Final Counters for attempt_local1247357783_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:12,626 INFO mapred.LocalJobRunner: Finishing task: attempt_local1247357783_0001_m_000000_0\n",
            "2021-05-07 08:38:12,627 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:12,631 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:12,631 INFO mapred.LocalJobRunner: Starting task: attempt_local1247357783_0001_r_000000_0\n",
            "2021-05-07 08:38:12,645 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:12,645 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:12,646 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:12,662 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50b11c9b\n",
            "2021-05-07 08:38:12,664 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:12,684 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:12,686 INFO reduce.EventFetcher: attempt_local1247357783_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:12,734 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1247357783_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:12,743 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1247357783_0001_m_000000_0\n",
            "2021-05-07 08:38:12,746 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:12,750 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:12,751 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:12,752 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:12,759 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:12,759 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:12,839 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:12,840 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:12,841 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:12,841 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:12,844 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:12,845 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:12,859 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:12,864 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:12,865 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:12,867 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:12,890 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:12,891 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:12,892 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:12,906 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:13,205 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:13,291 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:13,316 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:13,317 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:13,318 INFO mapred.Task: Task:attempt_local1247357783_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:13,320 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:13,321 INFO mapred.Task: Task attempt_local1247357783_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:13,323 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1247357783_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:13,324 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:13,325 INFO mapred.Task: Task 'attempt_local1247357783_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:13,328 INFO mapred.Task: Final Counters for attempt_local1247357783_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=368050176\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:13,328 INFO mapred.LocalJobRunner: Finishing task: attempt_local1247357783_0001_r_000000_0\n",
            "2021-05-07 08:38:13,328 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:13,868 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:13,869 INFO mapreduce.Job: Job job_local1247357783_0001 completed successfully\n",
            "2021-05-07 08:38:13,880 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=736100352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:13,880 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:17,413 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+9\n",
            "2021-05-07 08:38:18,648 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob17078647602460438507.jar tmpDir=null\n",
            "2021-05-07 08:38:19,462 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:19,671 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:19,672 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:19,693 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:19,850 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:19,883 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:20,238 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local742907832_0001\n",
            "2021-05-07 08:38:20,239 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:20,701 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local742907832_0001_4adb3d67-046b-4872-8684-21413e46c3da/centroids.txt\n",
            "2021-05-07 08:38:20,819 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:20,821 INFO mapreduce.Job: Running job: job_local742907832_0001\n",
            "2021-05-07 08:38:20,828 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:20,830 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:20,839 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:20,839 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:20,887 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:20,891 INFO mapred.LocalJobRunner: Starting task: attempt_local742907832_0001_m_000000_0\n",
            "2021-05-07 08:38:20,935 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:20,935 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:20,987 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:21,001 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:21,030 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:21,105 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:21,105 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:21,105 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:21,105 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:21,105 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:21,109 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:21,120 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:21,133 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:21,137 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:21,138 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:21,138 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:21,139 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:21,139 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:21,139 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:21,140 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:21,140 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:21,141 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:21,141 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:21,142 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:21,178 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:21,179 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:21,181 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:21,214 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:21,346 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:21,827 INFO mapreduce.Job: Job job_local742907832_0001 running in uber mode : false\n",
            "2021-05-07 08:38:21,828 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:22,515 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:22,516 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:22,519 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:22,519 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:22,519 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:22,519 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:22,519 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:22,583 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:22,605 INFO mapred.Task: Task:attempt_local742907832_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:22,609 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:22,612 INFO mapred.Task: Task 'attempt_local742907832_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:22,621 INFO mapred.Task: Final Counters for attempt_local742907832_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=900227\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:22,621 INFO mapred.LocalJobRunner: Finishing task: attempt_local742907832_0001_m_000000_0\n",
            "2021-05-07 08:38:22,621 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:22,625 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:22,625 INFO mapred.LocalJobRunner: Starting task: attempt_local742907832_0001_r_000000_0\n",
            "2021-05-07 08:38:22,637 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:22,637 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:22,639 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:22,646 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@494cfc88\n",
            "2021-05-07 08:38:22,648 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:22,670 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:22,672 INFO reduce.EventFetcher: attempt_local742907832_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:22,741 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local742907832_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:22,748 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local742907832_0001_m_000000_0\n",
            "2021-05-07 08:38:22,753 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:22,758 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:22,759 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:22,759 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:22,767 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:22,767 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:22,801 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:22,802 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:22,803 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:22,803 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:22,805 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:22,806 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:22,818 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:22,827 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:22,829 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:22,830 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:22,854 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:22,855 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:22,856 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:22,872 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:23,149 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:23,236 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:23,260 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:23,260 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:23,261 INFO mapred.Task: Task:attempt_local742907832_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:23,262 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:23,262 INFO mapred.Task: Task attempt_local742907832_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:23,264 INFO output.FileOutputCommitter: Saved output of task 'attempt_local742907832_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:23,268 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:23,268 INFO mapred.Task: Task 'attempt_local742907832_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:23,269 INFO mapred.Task: Final Counters for attempt_local742907832_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1187925\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=339738624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:23,269 INFO mapred.LocalJobRunner: Finishing task: attempt_local742907832_0001_r_000000_0\n",
            "2021-05-07 08:38:23,270 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:23,831 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:23,833 INFO mapreduce.Job: Job job_local742907832_0001 completed successfully\n",
            "2021-05-07 08:38:23,843 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2088152\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=679477248\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:23,843 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:27,505 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+10\n",
            "2021-05-07 08:38:28,824 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob10503843715502636310.jar tmpDir=null\n",
            "2021-05-07 08:38:29,698 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:29,913 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:29,914 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:29,938 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:30,110 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:30,141 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:30,489 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1301523940_0001\n",
            "2021-05-07 08:38:30,489 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:30,879 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1301523940_0001_e55a73b9-031d-4605-b654-5211df14ae42/centroids.txt\n",
            "2021-05-07 08:38:31,047 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:31,049 INFO mapreduce.Job: Running job: job_local1301523940_0001\n",
            "2021-05-07 08:38:31,056 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:31,058 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:31,064 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:31,064 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:31,126 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:31,133 INFO mapred.LocalJobRunner: Starting task: attempt_local1301523940_0001_m_000000_0\n",
            "2021-05-07 08:38:31,183 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:31,185 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:31,225 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:31,234 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:31,253 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:31,321 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:31,321 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:31,321 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:31,321 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:31,321 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:31,325 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:31,336 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:31,344 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:31,345 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:31,346 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:31,346 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:31,347 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:31,347 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:31,348 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:31,348 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:31,349 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:31,350 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:31,350 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:31,351 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:31,387 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:31,387 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:31,389 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:31,410 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:31,541 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:32,054 INFO mapreduce.Job: Job job_local1301523940_0001 running in uber mode : false\n",
            "2021-05-07 08:38:32,055 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:32,724 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:32,725 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:32,729 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:32,729 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:32,729 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:32,729 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:32,729 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:32,794 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:32,818 INFO mapred.Task: Task:attempt_local1301523940_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:32,823 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:32,824 INFO mapred.Task: Task 'attempt_local1301523940_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:32,833 INFO mapred.Task: Final Counters for attempt_local1301523940_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903210\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:32,833 INFO mapred.LocalJobRunner: Finishing task: attempt_local1301523940_0001_m_000000_0\n",
            "2021-05-07 08:38:32,833 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:32,837 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:32,837 INFO mapred.LocalJobRunner: Starting task: attempt_local1301523940_0001_r_000000_0\n",
            "2021-05-07 08:38:32,851 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:32,851 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:32,852 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:32,855 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@448cc9a2\n",
            "2021-05-07 08:38:32,859 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:32,881 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:32,884 INFO reduce.EventFetcher: attempt_local1301523940_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:32,923 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1301523940_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:32,927 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1301523940_0001_m_000000_0\n",
            "2021-05-07 08:38:32,930 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:32,933 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:32,935 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:32,935 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:32,935 WARN io.ReadaheadPool: Failed readahead on ifile\n",
            "EBADF: Bad file descriptor\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:419)\n",
            "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:296)\n",
            "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:209)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "2021-05-07 08:38:32,942 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:32,942 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:32,970 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:32,971 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:32,971 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:32,972 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:32,973 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:32,974 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:32,984 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:32,989 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:32,990 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:33,012 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:33,013 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:33,014 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:33,027 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:33,058 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:33,369 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:33,456 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:33,473 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:33,474 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:33,474 INFO mapred.Task: Task:attempt_local1301523940_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:33,475 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:33,476 INFO mapred.Task: Task attempt_local1301523940_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:33,478 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1301523940_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:33,479 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:33,480 INFO mapred.Task: Task 'attempt_local1301523940_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:33,481 INFO mapred.Task: Final Counters for attempt_local1301523940_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190908\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=342884352\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:33,481 INFO mapred.LocalJobRunner: Finishing task: attempt_local1301523940_0001_r_000000_0\n",
            "2021-05-07 08:38:33,481 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:34,058 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:34,060 INFO mapreduce.Job: Job job_local1301523940_0001 completed successfully\n",
            "2021-05-07 08:38:34,072 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094118\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=685768704\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:34,072 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:37,693 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+11\n",
            "2021-05-07 08:38:38,961 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16941963855216851422.jar tmpDir=null\n",
            "2021-05-07 08:38:39,713 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:39,872 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:39,872 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:39,895 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:40,047 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:40,066 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:40,400 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1331723244_0001\n",
            "2021-05-07 08:38:40,400 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:40,861 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1331723244_0001_297f0897-69a3-417b-a92c-f7e8fe04e3b0/centroids.txt\n",
            "2021-05-07 08:38:40,956 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:40,958 INFO mapreduce.Job: Running job: job_local1331723244_0001\n",
            "2021-05-07 08:38:40,964 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:40,966 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:40,971 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:40,971 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:41,013 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:41,019 INFO mapred.LocalJobRunner: Starting task: attempt_local1331723244_0001_m_000000_0\n",
            "2021-05-07 08:38:41,052 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:41,055 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:41,098 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:41,119 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:41,142 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:41,216 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:41,216 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:41,216 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:41,217 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:41,217 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:41,219 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:41,230 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:41,242 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:41,243 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:41,243 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:41,244 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:41,245 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:41,245 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:41,245 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:41,246 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:41,246 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:41,247 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:41,247 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:41,250 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:41,286 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:41,286 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:41,288 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:41,303 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:41,445 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:41,963 INFO mapreduce.Job: Job job_local1331723244_0001 running in uber mode : false\n",
            "2021-05-07 08:38:41,964 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:42,629 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:42,630 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:42,633 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:42,635 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:42,635 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:42,635 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:42,635 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:42,720 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:42,740 INFO mapred.Task: Task:attempt_local1331723244_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:42,747 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:42,747 INFO mapred.Task: Task 'attempt_local1331723244_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:42,756 INFO mapred.Task: Final Counters for attempt_local1331723244_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903210\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=335544320\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:42,756 INFO mapred.LocalJobRunner: Finishing task: attempt_local1331723244_0001_m_000000_0\n",
            "2021-05-07 08:38:42,756 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:42,764 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:42,765 INFO mapred.LocalJobRunner: Starting task: attempt_local1331723244_0001_r_000000_0\n",
            "2021-05-07 08:38:42,784 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:42,785 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:42,791 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:42,794 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@468f0ef3\n",
            "2021-05-07 08:38:42,795 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:42,815 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:42,829 INFO reduce.EventFetcher: attempt_local1331723244_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:42,862 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1331723244_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:42,865 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1331723244_0001_m_000000_0\n",
            "2021-05-07 08:38:42,870 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:42,873 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:42,876 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:42,876 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:42,883 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:42,884 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:42,933 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:42,935 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:42,936 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:42,936 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:42,937 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:42,938 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:42,950 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:42,956 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:42,958 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:42,967 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:42,974 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:42,975 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:42,977 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:42,992 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:43,286 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:43,361 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:43,382 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:43,383 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:43,384 INFO mapred.Task: Task:attempt_local1331723244_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:43,386 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:43,386 INFO mapred.Task: Task attempt_local1331723244_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:43,392 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1331723244_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:43,393 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:43,393 INFO mapred.Task: Task 'attempt_local1331723244_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:43,394 INFO mapred.Task: Final Counters for attempt_local1331723244_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190908\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=335544320\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:43,395 INFO mapred.LocalJobRunner: Finishing task: attempt_local1331723244_0001_r_000000_0\n",
            "2021-05-07 08:38:43,395 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:43,968 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:43,969 INFO mapreduce.Job: Job job_local1331723244_0001 completed successfully\n",
            "2021-05-07 08:38:43,982 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094118\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=671088640\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:43,983 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:47,567 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+12\n",
            "2021-05-07 08:38:48,841 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16158973139193258212.jar tmpDir=null\n",
            "2021-05-07 08:38:49,635 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:49,830 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:49,831 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:49,853 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:49,995 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:50,015 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:38:50,333 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1411808396_0001\n",
            "2021-05-07 08:38:50,333 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:38:50,813 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1411808396_0001_8f4d6fdb-ae3e-4b0b-adfd-7b50f8a020ad/centroids.txt\n",
            "2021-05-07 08:38:50,940 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:38:50,942 INFO mapreduce.Job: Running job: job_local1411808396_0001\n",
            "2021-05-07 08:38:50,949 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:38:50,954 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:38:50,965 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:50,965 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:51,014 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:38:51,018 INFO mapred.LocalJobRunner: Starting task: attempt_local1411808396_0001_m_000000_0\n",
            "2021-05-07 08:38:51,053 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:51,057 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:51,115 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:51,133 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:38:51,158 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:38:51,219 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:38:51,219 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:38:51,220 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:38:51,220 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:38:51,220 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:38:51,222 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:38:51,232 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:38:51,246 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:38:51,247 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:38:51,247 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:38:51,248 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:38:51,249 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:38:51,249 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:38:51,249 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:38:51,250 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:38:51,250 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:38:51,251 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:38:51,251 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:38:51,252 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:38:51,287 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:51,287 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:51,289 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:51,312 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:51,444 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:38:51,947 INFO mapreduce.Job: Job job_local1411808396_0001 running in uber mode : false\n",
            "2021-05-07 08:38:51,948 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:38:52,622 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:52,623 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:52,630 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:38:52,631 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:38:52,631 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:38:52,631 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:38:52,631 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:38:52,714 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:38:52,735 INFO mapred.Task: Task:attempt_local1411808396_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:52,740 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:38:52,740 INFO mapred.Task: Task 'attempt_local1411808396_0001_m_000000_0' done.\n",
            "2021-05-07 08:38:52,749 INFO mapred.Task: Final Counters for attempt_local1411808396_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903210\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:38:52,749 INFO mapred.LocalJobRunner: Finishing task: attempt_local1411808396_0001_m_000000_0\n",
            "2021-05-07 08:38:52,749 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:38:52,756 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:38:52,763 INFO mapred.LocalJobRunner: Starting task: attempt_local1411808396_0001_r_000000_0\n",
            "2021-05-07 08:38:52,781 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:38:52,781 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:38:52,781 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:38:52,784 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5a497b1c\n",
            "2021-05-07 08:38:52,787 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:52,815 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:38:52,823 INFO reduce.EventFetcher: attempt_local1411808396_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:38:52,892 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1411808396_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:38:52,903 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1411808396_0001_m_000000_0\n",
            "2021-05-07 08:38:52,910 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:38:52,915 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:38:52,917 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:52,918 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:38:52,924 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:52,924 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:52,951 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:38:52,965 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:38:52,966 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:38:52,967 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:38:52,967 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:38:52,968 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:38:52,970 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:52,982 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:38:52,988 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:38:52,989 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:38:53,018 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:53,019 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:53,021 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:53,037 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:53,329 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:38:53,416 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:38:53,436 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:38:53,437 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:38:53,438 INFO mapred.Task: Task:attempt_local1411808396_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:38:53,439 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:38:53,439 INFO mapred.Task: Task attempt_local1411808396_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:38:53,441 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1411808396_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:38:53,442 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:38:53,442 INFO mapred.Task: Task 'attempt_local1411808396_0001_r_000000_0' done.\n",
            "2021-05-07 08:38:53,443 INFO mapred.Task: Final Counters for attempt_local1411808396_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190908\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=362807296\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:53,443 INFO mapred.LocalJobRunner: Finishing task: attempt_local1411808396_0001_r_000000_0\n",
            "2021-05-07 08:38:53,444 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:38:53,952 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:38:53,952 INFO mapreduce.Job: Job job_local1411808396_0001 completed successfully\n",
            "2021-05-07 08:38:53,974 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094118\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=725614592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:38:53,974 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:38:57,516 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+13\n",
            "2021-05-07 08:38:58,716 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob6318503018754017471.jar tmpDir=null\n",
            "2021-05-07 08:38:59,601 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:38:59,744 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:38:59,744 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:38:59,767 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:38:59,924 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:38:59,958 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:39:00,318 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1529664191_0001\n",
            "2021-05-07 08:39:00,318 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:39:00,727 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1529664191_0001_b897bfc2-93d6-49b2-9f8d-e4784a939f9f/centroids.txt\n",
            "2021-05-07 08:39:00,819 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:39:00,821 INFO mapreduce.Job: Running job: job_local1529664191_0001\n",
            "2021-05-07 08:39:00,827 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:39:00,830 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:39:00,839 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:00,839 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:00,882 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:39:00,885 INFO mapred.LocalJobRunner: Starting task: attempt_local1529664191_0001_m_000000_0\n",
            "2021-05-07 08:39:00,921 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:00,924 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:00,970 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:00,987 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:39:01,011 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:39:01,088 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:39:01,088 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:39:01,088 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:39:01,088 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:39:01,089 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:39:01,099 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:39:01,107 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:39:01,117 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:39:01,117 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:39:01,118 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:39:01,118 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:39:01,119 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:39:01,119 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:39:01,120 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:39:01,120 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:39:01,120 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:39:01,121 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:39:01,122 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:39:01,122 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:39:01,151 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:01,152 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:01,153 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:01,169 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:01,309 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:39:01,826 INFO mapreduce.Job: Job job_local1529664191_0001 running in uber mode : false\n",
            "2021-05-07 08:39:01,827 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:39:02,509 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:02,510 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:02,515 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:39:02,515 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:39:02,515 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:39:02,516 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:39:02,516 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:39:02,581 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:39:02,593 INFO mapred.Task: Task:attempt_local1529664191_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:02,597 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:39:02,597 INFO mapred.Task: Task 'attempt_local1529664191_0001_m_000000_0' done.\n",
            "2021-05-07 08:39:02,615 INFO mapred.Task: Final Counters for attempt_local1529664191_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=903208\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=334495744\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:39:02,615 INFO mapred.LocalJobRunner: Finishing task: attempt_local1529664191_0001_m_000000_0\n",
            "2021-05-07 08:39:02,615 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:39:02,621 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:39:02,626 INFO mapred.LocalJobRunner: Starting task: attempt_local1529664191_0001_r_000000_0\n",
            "2021-05-07 08:39:02,635 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:02,635 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:02,636 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:02,645 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@418fbaa5\n",
            "2021-05-07 08:39:02,647 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:02,668 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:39:02,670 INFO reduce.EventFetcher: attempt_local1529664191_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:39:02,744 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1529664191_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:39:02,753 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1529664191_0001_m_000000_0\n",
            "2021-05-07 08:39:02,755 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:39:02,760 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:39:02,762 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:02,762 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:39:02,768 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:02,768 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:02,797 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:39:02,798 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:39:02,799 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:39:02,799 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:02,800 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:02,802 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:02,813 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:39:02,818 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:39:02,819 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:39:02,829 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:39:02,846 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:02,846 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:02,848 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:02,857 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:03,169 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:03,271 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:39:03,286 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:03,286 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:03,287 INFO mapred.Task: Task:attempt_local1529664191_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:03,288 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:03,288 INFO mapred.Task: Task attempt_local1529664191_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:39:03,290 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1529664191_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:39:03,292 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:39:03,293 INFO mapred.Task: Task 'attempt_local1529664191_0001_r_000000_0' done.\n",
            "2021-05-07 08:39:03,295 INFO mapred.Task: Final Counters for attempt_local1529664191_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1190906\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=334495744\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:03,296 INFO mapred.LocalJobRunner: Finishing task: attempt_local1529664191_0001_r_000000_0\n",
            "2021-05-07 08:39:03,296 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:39:03,830 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:39:03,831 INFO mapreduce.Job: Job job_local1529664191_0001 completed successfully\n",
            "2021-05-07 08:39:03,844 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2094114\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=668991488\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:03,853 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:39:07,426 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+14\n",
            "2021-05-07 08:39:08,703 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob15905061362603106800.jar tmpDir=null\n",
            "2021-05-07 08:39:09,445 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:39:09,589 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:39:09,589 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:39:09,613 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:09,738 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:39:09,758 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:39:10,080 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local620197389_0001\n",
            "2021-05-07 08:39:10,080 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:39:10,523 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local620197389_0001_a35268f0-025c-4bf4-994c-fffe9d2776ca/centroids.txt\n",
            "2021-05-07 08:39:10,659 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:39:10,661 INFO mapreduce.Job: Running job: job_local620197389_0001\n",
            "2021-05-07 08:39:10,667 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:39:10,670 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:39:10,674 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:10,674 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:10,719 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:39:10,725 INFO mapred.LocalJobRunner: Starting task: attempt_local620197389_0001_m_000000_0\n",
            "2021-05-07 08:39:10,755 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:10,756 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:10,796 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:10,814 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:39:10,848 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:39:10,929 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:39:10,929 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:39:10,929 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:39:10,929 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:39:10,929 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:39:10,933 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:39:10,946 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:39:10,959 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:39:10,961 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:39:10,961 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:39:10,962 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:39:10,963 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:39:10,963 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:39:10,964 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:39:10,964 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:39:10,965 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:39:10,967 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:39:10,970 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:39:10,971 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:39:11,008 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:11,009 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:11,013 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:11,043 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:11,194 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:39:11,666 INFO mapreduce.Job: Job job_local620197389_0001 running in uber mode : false\n",
            "2021-05-07 08:39:11,667 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:39:12,372 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:12,373 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:12,377 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:39:12,378 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:39:12,378 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:39:12,378 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:39:12,378 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:39:12,457 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:39:12,471 INFO mapred.Task: Task:attempt_local620197389_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:12,474 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:39:12,474 INFO mapred.Task: Task 'attempt_local620197389_0001_m_000000_0' done.\n",
            "2021-05-07 08:39:12,485 INFO mapred.Task: Final Counters for attempt_local620197389_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=900227\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:39:12,485 INFO mapred.LocalJobRunner: Finishing task: attempt_local620197389_0001_m_000000_0\n",
            "2021-05-07 08:39:12,485 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:39:12,489 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:39:12,493 INFO mapred.LocalJobRunner: Starting task: attempt_local620197389_0001_r_000000_0\n",
            "2021-05-07 08:39:12,503 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:12,503 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:12,505 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:12,512 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7168a552\n",
            "2021-05-07 08:39:12,516 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:12,541 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:39:12,544 INFO reduce.EventFetcher: attempt_local620197389_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:39:12,606 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local620197389_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:39:12,610 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local620197389_0001_m_000000_0\n",
            "2021-05-07 08:39:12,612 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:39:12,617 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:39:12,621 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:12,621 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:39:12,630 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:12,630 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:12,669 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:39:12,670 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:39:12,670 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:39:12,671 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:39:12,671 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:12,675 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:12,677 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:12,691 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:39:12,697 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:39:12,698 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:39:12,718 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:12,719 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:12,721 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:12,732 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:13,013 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:13,091 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:39:13,115 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:13,115 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:13,116 INFO mapred.Task: Task:attempt_local620197389_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:13,118 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:13,118 INFO mapred.Task: Task attempt_local620197389_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:39:13,120 INFO output.FileOutputCommitter: Saved output of task 'attempt_local620197389_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:39:13,121 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:39:13,121 INFO mapred.Task: Task 'attempt_local620197389_0001_r_000000_0' done.\n",
            "2021-05-07 08:39:13,123 INFO mapred.Task: Final Counters for attempt_local620197389_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1187925\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:13,123 INFO mapred.LocalJobRunner: Finishing task: attempt_local620197389_0001_r_000000_0\n",
            "2021-05-07 08:39:13,123 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:39:13,671 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:39:13,672 INFO mapreduce.Job: Job job_local620197389_0001 completed successfully\n",
            "2021-05-07 08:39:13,690 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2088152\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:13,690 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:39:17,268 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "count---------+15\n",
            "2021-05-07 08:39:18,579 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/centroids.txt] [] /tmp/streamjob16363409219090751284.jar tmpDir=null\n",
            "2021-05-07 08:39:19,404 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:39:19,639 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:39:19,639 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:39:19,659 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:19,824 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:39:19,859 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:39:20,185 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local476673007_0001\n",
            "2021-05-07 08:39:20,185 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:39:20,571 INFO mapred.LocalDistributedCacheManager: Localized file:/content/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local476673007_0001_9551d1be-23e5-4a9d-a3fa-359b662dd46d/centroids.txt\n",
            "2021-05-07 08:39:20,715 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:39:20,717 INFO mapreduce.Job: Running job: job_local476673007_0001\n",
            "2021-05-07 08:39:20,724 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:39:20,727 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:39:20,732 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:20,732 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:20,778 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:39:20,785 INFO mapred.LocalJobRunner: Starting task: attempt_local476673007_0001_m_000000_0\n",
            "2021-05-07 08:39:20,818 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:20,820 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:20,846 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:20,857 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:39:20,883 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:39:20,958 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:39:20,959 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:39:20,959 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:39:20,959 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:39:20,959 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:39:20,963 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:39:20,976 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:39:20,984 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:39:20,985 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:39:20,985 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:39:20,986 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:39:20,986 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:39:20,986 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:39:20,986 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:39:20,987 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:39:20,987 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:39:20,988 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:39:20,988 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:39:20,988 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:39:21,025 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:21,025 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:21,027 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:21,035 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:21,181 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:39:21,723 INFO mapreduce.Job: Job job_local476673007_0001 running in uber mode : false\n",
            "2021-05-07 08:39:21,724 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:39:22,359 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:22,360 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:22,365 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:39:22,365 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:39:22,365 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:39:22,365 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:39:22,365 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:39:22,433 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:39:22,445 INFO mapred.Task: Task:attempt_local476673007_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:22,448 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:39:22,448 INFO mapred.Task: Task 'attempt_local476673007_0001_m_000000_0' done.\n",
            "2021-05-07 08:39:22,457 INFO mapred.Task: Final Counters for attempt_local476673007_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242345\n",
            "\t\tFILE: Number of bytes written=900227\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=322961408\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:39:22,457 INFO mapred.LocalJobRunner: Finishing task: attempt_local476673007_0001_m_000000_0\n",
            "2021-05-07 08:39:22,457 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:39:22,461 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:39:22,463 INFO mapred.LocalJobRunner: Starting task: attempt_local476673007_0001_r_000000_0\n",
            "2021-05-07 08:39:22,482 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:22,482 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:22,482 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:22,487 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@13bb272a\n",
            "2021-05-07 08:39:22,489 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:22,513 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:39:22,515 INFO reduce.EventFetcher: attempt_local476673007_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:39:22,566 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local476673007_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:39:22,573 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local476673007_0001_m_000000_0\n",
            "2021-05-07 08:39:22,574 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:39:22,579 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:39:22,581 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:22,581 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:39:22,588 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:22,588 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:22,615 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:39:22,615 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:39:22,616 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:39:22,616 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:22,617 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:22,618 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:22,628 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer2.py]\n",
            "2021-05-07 08:39:22,647 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:39:22,649 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:39:22,664 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:22,665 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:22,666 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:22,680 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:22,728 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:39:22,960 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:23,023 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:39:23,040 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:23,040 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:23,041 INFO mapred.Task: Task:attempt_local476673007_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:23,042 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:23,042 INFO mapred.Task: Task attempt_local476673007_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:39:23,043 INFO output.FileOutputCommitter: Saved output of task 'attempt_local476673007_0001_r_000000_0' to file:/content/output1\n",
            "2021-05-07 08:39:23,046 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:39:23,046 INFO mapred.Task: Task 'attempt_local476673007_0001_r_000000_0' done.\n",
            "2021-05-07 08:39:23,047 INFO mapred.Task: Final Counters for attempt_local476673007_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815877\n",
            "\t\tFILE: Number of bytes written=1187925\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=322961408\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:23,047 INFO mapred.LocalJobRunner: Finishing task: attempt_local476673007_0001_r_000000_0\n",
            "2021-05-07 08:39:23,047 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:39:23,729 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:39:23,730 INFO mapreduce.Job: Job job_local476673007_0001 completed successfully\n",
            "2021-05-07 08:39:23,744 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058222\n",
            "\t\tFILE: Number of bytes written=2088152\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=645922816\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=948\n",
            "2021-05-07 08:39:23,747 INFO streaming.StreamJob: Output directory: output1\n",
            "2021-05-07 08:39:27,296 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output1\n",
            "First Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 08:39:28,645 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids.txt] [] /tmp/streamjob5466220569104364911.jar tmpDir=null\n",
            "2021-05-07 08:39:29,504 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:39:29,701 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:39:29,701 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:39:29,726 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:29,868 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:39:29,889 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:39:30,240 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1721910779_0001\n",
            "2021-05-07 08:39:30,240 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:39:30,661 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids.txt as file:/tmp/hadoop-root/mapred/local/job_local1721910779_0001_b012fb28-d660-4c70-9d6e-5404ee1c5b15/centroids.txt\n",
            "2021-05-07 08:39:30,787 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:39:30,789 INFO mapreduce.Job: Running job: job_local1721910779_0001\n",
            "2021-05-07 08:39:30,796 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:39:30,799 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:39:30,803 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:30,804 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:30,849 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:39:30,852 INFO mapred.LocalJobRunner: Starting task: attempt_local1721910779_0001_m_000000_0\n",
            "2021-05-07 08:39:30,882 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:30,885 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:30,924 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:30,937 INFO mapred.MapTask: Processing split: file:/content/train.txt:0+240740\n",
            "2021-05-07 08:39:30,967 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:39:31,041 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:39:31,041 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:39:31,041 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:39:31,041 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:39:31,041 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:39:31,045 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:39:31,059 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper2.py]\n",
            "2021-05-07 08:39:31,068 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:39:31,069 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:39:31,069 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:39:31,070 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:39:31,071 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:39:31,071 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:39:31,071 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:39:31,072 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:39:31,072 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:39:31,073 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:39:31,073 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:39:31,074 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:39:31,099 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:31,100 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:31,102 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:31,119 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:31,262 INFO streaming.PipeMapRed: Records R/W=3699/1\n",
            "2021-05-07 08:39:31,794 INFO mapreduce.Job: Job job_local1721910779_0001 running in uber mode : false\n",
            "2021-05-07 08:39:31,796 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:39:32,443 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:32,444 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:32,447 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:39:32,447 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:39:32,447 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:39:32,447 INFO mapred.MapTask: bufstart = 0; bufend = 260456; bufvoid = 104857600\n",
            "2021-05-07 08:39:32,447 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26161824(104647296); length = 52573/6553600\n",
            "2021-05-07 08:39:32,521 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:39:32,533 INFO mapred.Task: Task:attempt_local1721910779_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:32,536 INFO mapred.LocalJobRunner: Records R/W=3699/1\n",
            "2021-05-07 08:39:32,536 INFO mapred.Task: Task 'attempt_local1721910779_0001_m_000000_0' done.\n",
            "2021-05-07 08:39:32,546 INFO mapred.Task: Final Counters for attempt_local1721910779_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=242320\n",
            "\t\tFILE: Number of bytes written=903205\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13144\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "2021-05-07 08:39:32,546 INFO mapred.LocalJobRunner: Finishing task: attempt_local1721910779_0001_m_000000_0\n",
            "2021-05-07 08:39:32,546 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:39:32,553 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:39:32,557 INFO mapred.LocalJobRunner: Starting task: attempt_local1721910779_0001_r_000000_0\n",
            "2021-05-07 08:39:32,569 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:32,571 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:32,571 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:32,575 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4bfdc301\n",
            "2021-05-07 08:39:32,578 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:32,608 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:39:32,610 INFO reduce.EventFetcher: attempt_local1721910779_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:39:32,700 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1721910779_0001_m_000000_0 decomp: 286746 len: 286750 to MEMORY\n",
            "2021-05-07 08:39:32,706 INFO reduce.InMemoryMapOutput: Read 286746 bytes from map-output for attempt_local1721910779_0001_m_000000_0\n",
            "2021-05-07 08:39:32,710 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 286746, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->286746\n",
            "2021-05-07 08:39:32,720 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:39:32,721 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:32,722 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:39:32,730 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:32,730 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:32,756 INFO reduce.MergeManagerImpl: Merged 1 segments, 286746 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:39:32,757 INFO reduce.MergeManagerImpl: Merging 1 files, 286750 bytes from disk\n",
            "2021-05-07 08:39:32,758 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:39:32,758 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:32,759 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 286743 bytes\n",
            "2021-05-07 08:39:32,760 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:32,770 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, u_reducer.py]\n",
            "2021-05-07 08:39:32,778 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:39:32,780 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:39:32,795 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:32,795 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:32,796 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:32,798 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2021-05-07 08:39:32,804 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:33,112 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:33,180 INFO streaming.PipeMapRed: Records R/W=13144/1\n",
            "2021-05-07 08:39:33,197 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:33,199 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:33,200 INFO mapred.Task: Task:attempt_local1721910779_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:33,201 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:33,201 INFO mapred.Task: Task attempt_local1721910779_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:39:33,203 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1721910779_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 08:39:33,204 INFO mapred.LocalJobRunner: Records R/W=13144/1 > reduce\n",
            "2021-05-07 08:39:33,204 INFO mapred.Task: Task 'attempt_local1721910779_0001_r_000000_0' done.\n",
            "2021-05-07 08:39:33,205 INFO mapred.Task: Final Counters for attempt_local1721910779_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=815852\n",
            "\t\tFILE: Number of bytes written=1190902\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=13144\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=349175808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=947\n",
            "2021-05-07 08:39:33,205 INFO mapred.LocalJobRunner: Finishing task: attempt_local1721910779_0001_r_000000_0\n",
            "2021-05-07 08:39:33,205 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:39:33,799 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:39:33,800 INFO mapreduce.Job: Job job_local1721910779_0001 completed successfully\n",
            "2021-05-07 08:39:33,812 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058172\n",
            "\t\tFILE: Number of bytes written=2094107\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6572\n",
            "\t\tMap output records=13144\n",
            "\t\tMap output bytes=260456\n",
            "\t\tMap output materialized bytes=286750\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=27\n",
            "\t\tReduce shuffle bytes=286750\n",
            "\t\tReduce input records=13144\n",
            "\t\tReduce output records=26\n",
            "\t\tSpilled Records=26288\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=698351616\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=240740\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=947\n",
            "2021-05-07 08:39:33,818 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 08:39:37,254 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Second Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
            "2021-05-07 08:39:38,503 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [/content/home/centroids1.txt] [] /tmp/streamjob17351266473512334569.jar tmpDir=null\n",
            "2021-05-07 08:39:39,342 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2021-05-07 08:39:39,592 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2021-05-07 08:39:39,593 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2021-05-07 08:39:39,617 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:39,804 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2021-05-07 08:39:39,834 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2021-05-07 08:39:40,166 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1180266547_0001\n",
            "2021-05-07 08:39:40,167 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2021-05-07 08:39:40,572 INFO mapred.LocalDistributedCacheManager: Localized file:/content/home/centroids1.txt as file:/tmp/hadoop-root/mapred/local/job_local1180266547_0001_e2b17abe-b0ce-4ef4-80b0-27754d8d76b1/centroids1.txt\n",
            "2021-05-07 08:39:40,696 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2021-05-07 08:39:40,697 INFO mapreduce.Job: Running job: job_local1180266547_0001\n",
            "2021-05-07 08:39:40,704 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2021-05-07 08:39:40,722 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2021-05-07 08:39:40,731 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:40,731 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:40,794 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2021-05-07 08:39:40,801 INFO mapred.LocalJobRunner: Starting task: attempt_local1180266547_0001_m_000000_0\n",
            "2021-05-07 08:39:40,838 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:40,841 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:40,897 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:40,912 INFO mapred.MapTask: Processing split: file:/content/test.txt:0+491825\n",
            "2021-05-07 08:39:40,941 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2021-05-07 08:39:41,017 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2021-05-07 08:39:41,017 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2021-05-07 08:39:41,017 INFO mapred.MapTask: soft limit at 83886080\n",
            "2021-05-07 08:39:41,017 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2021-05-07 08:39:41,017 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2021-05-07 08:39:41,020 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2021-05-07 08:39:41,030 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_mapper.py]\n",
            "2021-05-07 08:39:41,040 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2021-05-07 08:39:41,041 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2021-05-07 08:39:41,042 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2021-05-07 08:39:41,043 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2021-05-07 08:39:41,044 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2021-05-07 08:39:41,044 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2021-05-07 08:39:41,045 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2021-05-07 08:39:41,045 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2021-05-07 08:39:41,046 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2021-05-07 08:39:41,047 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2021-05-07 08:39:41,047 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2021-05-07 08:39:41,048 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2021-05-07 08:39:41,090 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:41,091 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:41,093 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:41,107 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:41,702 INFO mapreduce.Job: Job job_local1180266547_0001 running in uber mode : false\n",
            "2021-05-07 08:39:41,704 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2021-05-07 08:39:41,949 INFO streaming.PipeMapRed: Records R/W=7400/1\n",
            "2021-05-07 08:39:42,518 INFO streaming.PipeMapRed: R/W/S=10000/4097/0 in:10000=10000/1 [rec/s] out:4097=4097/1 [rec/s]\n",
            "2021-05-07 08:39:43,976 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:43,977 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:43,980 INFO mapred.LocalJobRunner: \n",
            "2021-05-07 08:39:43,980 INFO mapred.MapTask: Starting flush of map output\n",
            "2021-05-07 08:39:43,981 INFO mapred.MapTask: Spilling map output\n",
            "2021-05-07 08:39:43,981 INFO mapred.MapTask: bufstart = 0; bufend = 53712; bufvoid = 104857600\n",
            "2021-05-07 08:39:43,981 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26160688(104642752); length = 53709/6553600\n",
            "2021-05-07 08:39:44,050 INFO mapred.MapTask: Finished spill 0\n",
            "2021-05-07 08:39:44,072 INFO mapred.Task: Task:attempt_local1180266547_0001_m_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:44,075 INFO mapred.LocalJobRunner: Records R/W=7400/1\n",
            "2021-05-07 08:39:44,075 INFO mapred.Task: Task 'attempt_local1180266547_0001_m_000000_0' done.\n",
            "2021-05-07 08:39:44,084 INFO mapred.Task: Final Counters for attempt_local1180266547_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=493406\n",
            "\t\tFILE: Number of bytes written=697037\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13428\n",
            "\t\tMap output records=13428\n",
            "\t\tMap output bytes=53712\n",
            "\t\tMap output materialized bytes=80574\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=13428\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=491825\n",
            "2021-05-07 08:39:44,084 INFO mapred.LocalJobRunner: Finishing task: attempt_local1180266547_0001_m_000000_0\n",
            "2021-05-07 08:39:44,085 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2021-05-07 08:39:44,088 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2021-05-07 08:39:44,088 INFO mapred.LocalJobRunner: Starting task: attempt_local1180266547_0001_r_000000_0\n",
            "2021-05-07 08:39:44,096 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2021-05-07 08:39:44,096 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2021-05-07 08:39:44,097 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2021-05-07 08:39:44,100 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4d4cf9e5\n",
            "2021-05-07 08:39:44,104 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2021-05-07 08:39:44,125 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2389914368, maxSingleShuffleLimit=597478592, mergeThreshold=1577343488, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2021-05-07 08:39:44,128 INFO reduce.EventFetcher: attempt_local1180266547_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2021-05-07 08:39:44,181 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1180266547_0001_m_000000_0 decomp: 80570 len: 80574 to MEMORY\n",
            "2021-05-07 08:39:44,186 INFO reduce.InMemoryMapOutput: Read 80570 bytes from map-output for attempt_local1180266547_0001_m_000000_0\n",
            "2021-05-07 08:39:44,191 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 80570, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->80570\n",
            "2021-05-07 08:39:44,195 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2021-05-07 08:39:44,197 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:44,197 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2021-05-07 08:39:44,203 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:44,203 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 80566 bytes\n",
            "2021-05-07 08:39:44,231 INFO reduce.MergeManagerImpl: Merged 1 segments, 80570 bytes to disk to satisfy reduce memory limit\n",
            "2021-05-07 08:39:44,232 INFO reduce.MergeManagerImpl: Merging 1 files, 80574 bytes from disk\n",
            "2021-05-07 08:39:44,233 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2021-05-07 08:39:44,233 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2021-05-07 08:39:44,234 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 80566 bytes\n",
            "2021-05-07 08:39:44,234 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:44,248 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, c_reducer.py]\n",
            "2021-05-07 08:39:44,253 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2021-05-07 08:39:44,254 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2021-05-07 08:39:44,295 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:44,296 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:44,297 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:44,311 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:44,339 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2021-05-07 08:39:44,605 INFO streaming.PipeMapRed: Records R/W=13428/1\n",
            "2021-05-07 08:39:44,635 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2021-05-07 08:39:44,635 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2021-05-07 08:39:44,636 INFO mapred.Task: Task:attempt_local1180266547_0001_r_000000_0 is done. And is in the process of committing\n",
            "2021-05-07 08:39:44,638 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2021-05-07 08:39:44,638 INFO mapred.Task: Task attempt_local1180266547_0001_r_000000_0 is allowed to commit now\n",
            "2021-05-07 08:39:44,640 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1180266547_0001_r_000000_0' to file:/content/output2\n",
            "2021-05-07 08:39:44,641 INFO mapred.LocalJobRunner: Records R/W=13428/1 > reduce\n",
            "2021-05-07 08:39:44,641 INFO mapred.Task: Task 'attempt_local1180266547_0001_r_000000_0' done.\n",
            "2021-05-07 08:39:44,642 INFO mapred.Task: Final Counters for attempt_local1180266547_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=654586\n",
            "\t\tFILE: Number of bytes written=804744\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=19\n",
            "\t\tReduce shuffle bytes=80574\n",
            "\t\tReduce input records=13428\n",
            "\t\tReduce output records=19\n",
            "\t\tSpilled Records=13428\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=328204288\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27133\n",
            "2021-05-07 08:39:44,642 INFO mapred.LocalJobRunner: Finishing task: attempt_local1180266547_0001_r_000000_0\n",
            "2021-05-07 08:39:44,643 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2021-05-07 08:39:44,711 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2021-05-07 08:39:44,712 INFO mapreduce.Job: Job job_local1180266547_0001 completed successfully\n",
            "2021-05-07 08:39:44,723 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1147992\n",
            "\t\tFILE: Number of bytes written=1501781\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13428\n",
            "\t\tMap output records=13428\n",
            "\t\tMap output bytes=53712\n",
            "\t\tMap output materialized bytes=80574\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=19\n",
            "\t\tReduce shuffle bytes=80574\n",
            "\t\tReduce input records=13428\n",
            "\t\tReduce output records=19\n",
            "\t\tSpilled Records=26856\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=656408576\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=491825\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27133\n",
            "2021-05-07 08:39:44,724 INFO streaming.StreamJob: Output directory: output2\n",
            "2021-05-07 08:39:48,318 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /content/output2\n",
            "Third Map reduce Done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-68g5cG9iAW"
      },
      "source": [
        "#evaluation \n",
        "\n",
        "import seaborn as sb\n",
        "import string\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "def parse_args():\n",
        "    '''Parse command line arguments'''\n",
        "    parser = argparse.ArgumentParser(description=\"Run classification evaluation\")\n",
        "    \n",
        "    parser.add_argument('--input', nargs='?', default='predictions.txt',help='Input File path')\n",
        "    parser.add_argument('--output', nargs='?', default='performance.png',help='Visualization plot name')\n",
        "    \n",
        "    return parser.parse_args()\n",
        "\n",
        "def get_performance(args):\n",
        "    fp = open(args.input,'r')\n",
        "\n",
        "    # Processing alphabets and preparing confusion matrix\n",
        "    cm = np.zeros((26, 26))\n",
        "    alphabets = string.ascii_uppercase[:26]\n",
        "    alphabet_dict = dict()\n",
        "    x_axis_labels = []\n",
        "    y_axis_labels = []\n",
        "    for i in range(26):\n",
        "        x_axis_labels.append(alphabets[i])\n",
        "        y_axis_labels.append(alphabets[i])\n",
        "        alphabet_dict[alphabets[i]] = i\n",
        "\n",
        "    # Filling up the confusion matrix\n",
        "    for i,line in enumerate(fp):\n",
        "        line_split = line.split('\\t')[0].split(',')\n",
        "        actual = line_split[0]\n",
        "        predictions = list(line_split[1:])\n",
        "        total_pred = len(predictions)\n",
        "        for p in predictions:\n",
        "          cm[alphabet_dict[actual]][alphabet_dict[p]] += 1.0\n",
        "\n",
        "    fp.close()\n",
        "\n",
        "    # Calculate accuracy with total sum of the array and trace of the array\n",
        "    print('Classification Accuracy: ' + str((np.trace(cm)/np.sum(cm))*100) + '%')\n",
        "    print(cm)\n",
        "    \n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    cm = np.true_divide(cm, cm.sum(axis=1, keepdims=True))\n",
        "    cm[np.isnan(cm)] = 0\n",
        "\n",
        "    # Get heatmap of the confusion matrix\n",
        "    ax = sb.heatmap(cm, vmin=0, vmax=1, xticklabels=x_axis_labels, yticklabels=y_axis_labels).set_title('Confusion matrix')\n",
        "    fig = ax.get_figure()\n",
        "    fig.savefig(args.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vwbzLk5L-Af3",
        "outputId": "7f697887-7c17-4793-efe8-219b121adef3"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    get_performance(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Accuracy: 28.030980041703902%\n",
            "[[352.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  12.   2.   0.\n",
            "    0.   0.   2.   0.   9.   0.   0.   0.   0.   0.   0.   4.]\n",
            " [  9. 143.  12. 120.  20.   8.  49.  36.  24.  36.  23.  27.   0.  32.\n",
            "  134.  14. 106. 107.  54.  18.  37.   0.   0.  46.   3.  53.]\n",
            " [  0.   0. 165.   0.   7.   2.  76.   9.   1.  14.  96.   0.   3.   2.\n",
            "   10.   0.   3.   1.   0.   0.  47.   0.   0.   0.   0.   0.]\n",
            " [ 35. 153.   1. 187.  11.   0.  64.  41.   0.   1.  17.  20.  11.  18.\n",
            "  114.   7.  76.  94.  43.   2.   7.   0.   2.  15.   0.   3.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  2.  27.  52.  16.  77.   1. 130.  12.  55.  16.  51.  98.   1.   5.\n",
            "   90.   1.  59.  69.  65.   2.   3.   0.   0.  62.   0.  48.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.  43.   0.  32.   5.   3.   0.  76.  16.  13.   0.   0.   0.\n",
            "    0.   0.   0.   0.  20.   5.   1.   0.   0.  33.   1.  41.]\n",
            " [ 19.   7.   0.   9.  57.   0.   0.  26. 141. 291.  28.  74.   3.   0.\n",
            "    0.   0.   0.   1.  31.   0.   0.   0.   0.  50.   0.  47.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 206.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [ 94.  79.  29. 104.  41.  32.  59.  81.  11.  14.  92.  17. 271. 112.\n",
            "   24.  38.  49. 100.  40.  23.  60.  27.  91.  40.  60.  17.]\n",
            " [  0.   0.   0.   0.   0.   0.   0. 101.   0.   0.   0.   0. 108. 111.\n",
            "    0.   0.   0.   0.   0.   0.  57.   0.   0.   0.   0.   0.]\n",
            " [  0.   2.   1.   4.  11.   0.  48.  26.  86.   0.   9.   0.   6.  24.\n",
            "   77.   0.  70.  25.   0.   0.  48.   0.   0.  35.   0.   0.]\n",
            " [  0.  62.  38.  68.  38. 211.   5.  56.  43.  72.  50.  20.   1.  50.\n",
            "   27. 396.   0.  46.  66.  64.  15.  46.  27.  70.  35.  50.]\n",
            " [  7.  21.  39.   6.  40.  20.  45.  43.  11.  12.  21.  19.  42.  11.\n",
            "   18.  21. 150.  58.  41.  12.  27.  23.  18.  10.  28.   5.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   1.   0.   3. 177.   0.   1.  31.   1.   0.   0.   0.   0.\n",
            "    0.  21.   0.   0.   8. 135.   0.  15.   0.   4.  77.  12.]\n",
            " [  0.   0.  38.   0.  27.  37.   0.  59.   0.   2.  74.   1.  46.  52.\n",
            "    1.   1.   0.   0.  15.  29. 217. 105.  17.  49.  86.   4.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0. 204.   6. 247.   0.   0. 211.   0.]\n",
            " [  5.   0.   0.   3.   0.  26.   0.   5.   1.   2.   0.   0.  43. 102.\n",
            "    3.  33.   0.   0.   0.  14.   6.  51. 351.   0.  22.   4.]\n",
            " [ 15.  25.   3.  25.   4.   4.   6.   2.  26.  21.   7.  11.   0.   6.\n",
            "    7.   0.   2.   9.  22.  20.   2.   0.   0.  75.   6.  32.]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            " [  0.   1.  73.   0. 146.   0.  29.   0.   0.   0.   8.   4.   0.   0.\n",
            "    3.   0.   0.   0.  90.   0.   0.   0.   0.  45.   0. 184.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAEICAYAAAD8yyfzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c83naSzkQQIEJMAQQUVQQMyKFEUQUdQR2DcQEZkhp+MM6IDiOM6ijiL4whuLE5wQXRk0Rk1KqPMCAhCQAIiEIQQwpaEJWSD7L08vz/u7UzRqeo6t7q2VH/fvO6LrltPnXuqu3P61LnPOUcRgZmZNceoVlfAzGwkcaNrZtZEbnTNzJrIja6ZWRO50TUzayI3umZmTeRG1wCQNF7SzyStk/TDYZRzkqRr6lm3VpF0uKT7W10P6yxynu6ORdJ7gLOAFwPPAncC/xQRvx1mue8FPgTMjYjeYVe0zUkKYN+IWNLqutjI4p7uDkTSWcBXgH8G9gD2Ai4Cjq1D8XsDi0dCg5tC0uhW18E6VET42AEOYAqwHnjnEDHdZI3yivz4CtCdP3cEsAz4CPAU8Djwl/lznwO2Aj35NU4FzgG+X1L2bCCA0fnjU4ClZL3th4CTSs7/tuR1c4HbgHX5/+eWPHc98Hngpryca4BpFd7bQP3/vqT+xwFvBhYDq4FPlsQfCiwA1uaxFwBj8+duyN/Lhvz9vruk/I8BTwDfGziXv+YF+TUOzh/PAFYCR7T6d8PHjnW4p7vjOAwYB/x4iJhPAa8C5gAvJ2t4Pl3y/HSyxnsmWcN6oaSdI+KzZL3nKyNiUkR8a6iKSJoIfA04JiJ2ImtY7ywTtwvwizx2V+B84BeSdi0Jew/wl8DuwFjg7CEuPZ3sezAT+AxwCfAXwCuAw4F/kLRPHtsHnAlMI/veHQX8LUBEvDaPeXn+fq8sKX8Xsl7/aaUXjogHyRrk70uaAHwH+G5EXD9Efc2240Z3x7Er8HQM/fH/JODciHgqIlaS9WDfW/J8T/58T0RcTdbLe1GN9ekHDpA0PiIej4hFZWLeAjwQEd+LiN6IuBy4D/izkpjvRMTiiNgEXEX2B6OSHrLx6x7gCrIG9asR8Wx+/XvJ/tgQEbdHxC35dR8G/h14XcJ7+mxEbMnr8xwRcQmwBLgVeB7ZHzmzQtzo7jhWAdOqjDXOAB4pefxIfm5bGYMa7Y3ApKIViYgNZB/JPwA8LukXkl6cUJ+BOs0sefxEgfqsioi+/OuBRvHJkuc3Dbxe0n6Sfi7pCUnPkPXkpw1RNsDKiNhcJeYS4ADg6xGxpUqs2Xbc6O44FgBbyMYxK1lB9tF4wF75uVpsACaUPJ5e+mRE/Coi3kjW47uPrDGqVp+BOi2vsU5FXExWr30jYjLwSUBVXjNkKo+kSWTj5N8CzsmHT8wKcaO7g4iIdWTjmBdKOk7SBEljJB0j6Yt52OXApyXtJmlaHv/9Gi95J/BaSXtJmgJ8YuAJSXtIOjYf291CNkzRX6aMq4H9JL1H0mhJ7wb2B35eY52K2Al4Blif98L/ZtDzTwLPL1jmV4GFEfH/yMaqvzHsWtqI40Z3BxIR55Hl6H6a7M75Y8DpwE/ykH8EFgJ3AXcDd+TnarnW/wBX5mXdznMbylF5PVaQ3dF/Hds3akTEKuCtZBkTq8gyD94aEU/XUqeCzia7SfcsWS/8ykHPnwN8V9JaSe+qVpikY4Gj+b/3eRZwsKST6lZjGxE8OcLMrInc0zUzayI3umZmFUj6tqSnJN1T4XlJ+pqkJZLuknRwtTLd6JqZVXYp2Vh+JccA++bHaWRZM0Nyo2tmVkFE3EB2s7iSY4HLInMLMFXS84Yqs+GLemy+Y37SnbpJr9ru5re1iQljupPiNvWkzxVo1e3baom6pVLr2D16TFLclt6eAlfvHKOU/l3fumVZkR9RWT1PL03+9Rq72wv+mudO+Z4XEfMKXG4mWRbRgGX5uccrvaBwoyvpOLL5/y+JiPuKvt7M+TLWLvIGtkgjO2y1DC+cCPw2/7+ZWXvp70s/hm85sGfJ41lUmXFZqNHNp0G+hmyFqhOK1s7MrOH6etOP4ZsPnJxnMbwKWBcRFYcWoPjwwrHALyNisaRVkl4REbfXWlszs3qLKDcjvTaSLidbV3mapGXAZ4Ex2XXiG2RT3d9MtvrcRrJlSodUtNE9kWz+OWRL651INkV0cEVPIx+cvuBTf8upf/6mgpcxM6tRf/0a3YgYchg1sim9HyxSZnKjm6+odCRwYL6/VBcQkj4ag+YSlw5Op2YvmJnVRR17uo1QZEz3HcD3ImLviJgdEXuSbdNyeGOqZmZWg+beSCusyPDCicC/Djr3n/n5Gyq9aPJhf5tU+IG7zE6KW7Rm8JrY5b1gyozqQbmHnnmiehDQl/hD2ql7QvWgXG9imT2JcWNGdSVfO7XMvsSeQ5GPNONGj02Ke/7k6dWDgPvXLkuK22un3ZPiiliz5dmkuL4CH3vHdqX909xYIDc61U5jxyfFPbt1u801yppc4N9DXbR5Tze50Y2I15c597X6VsdGgtQG16wWUZ+shIYpdCNNUh/ZOq0i2/jv9Ii4uREVMzOrSR1vpDVC0eyFTRExB0DSm4B/ofpmf2ZmzdMpwwtlTAbW1KsiZmZ10aIbZKmKNrrjJd0JjCPbkPDIckGlebpdXVMZ1TVxWJU0M0vWYT3d0uGFw4DLJB0wVJ7u2O5ZztM1s+bppBtppSJiQb7j7G7AU/WrkpnZMHTYjbRt8m2tu8h2eTUzawsRnTmmC1na2PuiyjvsT9xt+O7VDyfFXbDHdunCZZ3+5HVJcQCjC0woSHHQlH2SY29edX9SXE/iR6YikyPeuvucpLgfP74wKS519ektvVsTI+GBdSuS4lInHjy8Lm0iDLR23d99EieFPLB2yFUEa5I66SHV2s0b6lpeVW0+pltoaceI6CLbL+g+YBJwrqSrJe3XiMqZmRXW359+tEDRyREi2zXiuxFxQn7u5cAewOL6V8/MrKA27+kWHV54PdCTryMJQET8ob5VMjMbhr723ouuaKN7AGXWzx2sNE9XXVMYNcp5umbWJJ2avTCU0jzd0WNnOk/XzJqnw4YXFpGtq2tm1p7avKdbdDfga4HufPgAAEkvk+SFzM2sPXRS9kJEhKTjga9I+hiwGXgYOKPSayaOHZdU9kFT03JbU/Nvj5l+UFIcwDVPpd0LTB0nuW9Deu7k+MS1ZVPzdIssav2zJ3+fFDcmcUHt/gIf65SY1bvflJlJcfeuTlvcftZO05LiALq70n42T21amxT3zJaNydde0oD821SpeeupC/A3W3TKjbSStXTHAL3ARcCXo55bb9qIkNrgmtWkzZukIj3d0sVudgd+QLa842cbUTEzs5p02JguABHxFFlK2On5hAkzs/YQ/elHCwxnlbGlkrqA3YEn61clM7NhaPOebkPydEsnR4wbO42xYyY34jJmZtvroDHd55D0fLLNKbdbS7d0csSUSS/w5Agza57eDlzEXNJuwDeACwbvGmFm1lJt3tNVaptZJmXse8D51VLGPA3YzFL1bl0+7Bvzm+Z/KbnNGf+2s5ueCJDc083X0jUza29t3tMtPLxQ0uMdcEVEfKF+VTIzG4YOzF7YNknCzKztdFpP18ysrbV59kItM9LGS7qz5Hj34ABJp0laKGlhf3+TN6Uzs5EtIv1ogYYML3gRczNrmQ4c0zUza19t3ujWtOCNmVnbquOCN5KOlnS/pCWSPl7m+b0kXSfp95LukvTmamXW0tMdL+nOkse/jIjtKmNm1hJ99VlcPV/Q60LgjcAy4DZJ8yPi3pKwTwNXRcTFkvYHrgZmD1VuPYYXvlE9xMysSeo3vHAosCQilgJIugI4FihtdINsXXGAKcCKaoU6T9fMOkuBRrd0RcTcvDwRAGAm8FjJc8uAVw4q4hzgGkkfAiYCb6h2Td9IM7POUmByRGmmVY1OBC6NiPMkHQZ8T9IBQ61JM9wx3Yci4vjBAaV/PdQ1hVGjJtZwGTOz4qK/blmqy4E9Sx7Pys+VOhU4GiAiFkgaB0yjzJK3A5yna2adpX5jurcB+0rah6yxPQF4z6CYR4GjgEslvQQYB6wcqlAPL5hZZ6lT9kJE9Eo6HfgV0AV8OyIWSToXWBgR84GPAJdIOpPsptop1dYYd6NrZp2ljpMjIuJqsjSw0nOfKfn6XuDVRcpsm0Z33OixSXETx3Qnxa3bsjH52ntMnJoUt/zZVUlxB017QfK171r9UFLc1O60cfEi73un7vFJcbt2p+1xt2Rt1WyZbVJXjp49ZXpS3EPrnkiK223ClMQrQ9eotLlDqRsBPLlhbfK1Wyn1e7Ry47qkuJ3HTxpOdYpr8xlphRvdiGjyd9A6TdOX6reRpc13ECs0DVjSHpJ+IGmppNslLZC0XfaCmVnL9PenHy2Q3OhKEvAT4IaIeH5EvILsbt6sRlXOzKyw/kg/WqDI8MKRwNaI2DbtNyIeAb5e91qZmdWqTtkLjVKk0X0pcEdKoCdHmFmrRJvfSKt5aUdJF0r6g6TbBj8XEfMi4pCIOMQNrpk1VQcNLywC3j7wICI+KGkasLDutTIzq1Wbb0yp1BzD/EbaLWSLO1ycn9uL7Mba7Eqv8zRgM0vVu3X5sDMKN5x7UnKbM/Ez/9H0DMbknm5EhKTjgC9L+nuy+cUbgI81qnJmZoX1ds6NNIAHSidHSDoFOBy4sp6VMjOrWZsPL7TNNGAzs7po0Q2yVG50zayjtHvKWNFGd/CmlLsA8wcHOU/XzFqmw3q6z1nAPB/TPWRwkBcxN7OW6bBG18ysvXXQNGAzs7ZXxz3SGmKHa3T/ZLf9kuJuW7k4ucwX7Zy2UNqD6x5PiusvkLLSnzg55dQZc5PifrLm7uRr9/Sn9QieKbAw+kiUugj+qk3PJpfZm/izaYQX77xn9SDgvjWPVQ8CDp72wuFUp7hOanTLLGB+gRc1N7O20mHZC2Zm7a2TerpmZm1vJDa6ztM1s1aJvhE4vOA8XTNrmZHY0zUzaxWnjJmZNdNIb3S7R49Jiuvp602KS82/HaX0tYnX9WxIimtE7uSEMd1JcQ/1peV4fnbydrOyK/rEmgVJcftMmZ4U99C6J5Kv3Uk29mxJimtl7m0Rq7Y8U9fyHt34VF3Lq6q9h3SH1+g6R9fM2k30tnerO+yerqT1bnzNrG20d5vrMV0z6yy+kWZm1kwjsadbOjli7JhdGD16p0ZcxsxsO+3e0x3ViEIjYl5EHBIRh7jBNbOm6i9wtICHF8yso0Ra9mnLKBLXc61YQJXsBU8DNrNUvVuXpyfYV/D0Ma9LbnOm/fdvhn29ooY1vCBpNJCWGW5m1gx1HF6QdLSk+yUtkfTxCjHvknSvpEWSflCtzMLDC4N6tn8NTJC0d0Q8UrQsM7N6K7Bxy5AkdQEXAm8ElgG3SZofEfeWxOwLfAJ4dUSskbR7tXJrHtOV9GXgg8D73eCaWbuoV6MLHAosiYilAJKuAI4F7i2JeT9wYUSsAYiIqnOea2p0Jb02v/jLIuK+WsowM2uE6Esfpi1Nb83Ny5emBZgJlG4Etwx45aAi9svLuQnoAs6JiF8Odc1aGt1u4CfAEZUaXC9ibmatUqSnW7r2d41GA/sCRwCzgBskHRgRayu9oJYbaT3AzcCplQJK83Td4JpZM0W/ko8qlgOlWyPPys+VWgbMj4ieiHgIWEzWCFdUS6PbD7wLOFTSJ2t4vZlZw0R/+lHFbcC+kvaRNBY4AZg/KOYnZL1cJE0jG25YOlShNY3pRsRGSW8BbpT0ZER8q5ZyzMzqLaI+qbcR0SvpdOBXZOO1346IRZLOBRZGxPz8uT+VdC/QB3w0IlYNVW7hyRGlKWOS9gRuAP4ur8B2PDlix7dpxY1JceNnHN7gmlinq8fkiGWvPDK5zZl167VNnxxRuKcbEZMGGt6IeAzYpwH1MjOrSX+B7IVW8NoLZtZREm6QtZQbXTPrKG50zcyaaJhreDVcwxcx9+QIM2umEdnTLZ3l4ewFM2umeqWMNYqHF8yso/R1aPbCBEnLSh6fHxHnlwvcdXzadj19iROm12/dnBS387j0XeGf2boxKa4/cbBobFf6t3XMqK6kuHWbNyTFSem/cKnfo4kzX5sUd/ju+ydfe/GGFUlxT296JiludOL3UaR/fyaNHZcU9+zWTUlxvf19ydceN3psUtyW3p6kuCndE5Kvvb4n7d9Y6vd8U09zl9zuyJ5uRDRkbzUbGVIbXLNatPuYblLjKSkkfb/k8WhJKyX9vHFVMzMrLiL9aIXUnu4G4ABJ4yNiE9lK6oNX2zEza7mO6Onmrgbekn99InB5/atjZjY8ff2jko9WKHLVK4ATJI0DXgbcWilQ0mmSFkpauGlrxbV8zczqrt2HF5Ib3Yi4C5hN1su9ukrstkXMx4+dOrwampkV0B9KPlqhaPbCfOBLZIv27lr32piZDVOnpYx9G1gbEXdLOqIB9TEzG5aOWnshIpYBXyvymlWbni1UoXpZuXFdS64L0NPXmxz7/hmvToq7ZMVNSXFFFqWv989mzui0Dz9zpuxKV+Ikha9suCEprq+/fvtuD9jcu7XuZabaf8peSXG3rVycFNeIf4dbSJuY0WytGjZIlTSmW7JTREg6Lz93PXC9pHMaVjvrSKkNrlktOil7AWAL8Of5BmxmZm0nChytULTR7SVbPezMBtTFzGzY2j17oZb+9YXASZKmVAoozdPt709bqMXMrB4ilHy0QuFGNyKeAS4DPjxEzLY8XS9gbmbN1F/gaIVaR5K/ApwKuEU1s7YSKPlohZoa3YhYDVxF1vCambWN3lDy0QrD2TniPOD0elUk9e2n3nEcVWAx767ExZhT829fOHVG8rX/a81dSXFH7nFgUty1T96dfO3URahTF9++6Im0XGKAyYmLas/d7cVJcTevvC8p7pBp+ybFATy44fGkuDWb1ieXmWpNT/3LTDV90s5JcU+sX9PgmtSmVT3YVEUb3RdL+imwP1kv+RLgn+teK+toqQ2uWS1aNVabKnl4Qdk+MP8F/CQi9gX2AyYB/9SgupmZFdZJY7pHApsj4jsAEdFHlq/7V5LcdTGzttBJ2QsvBW4vPZGnjz0KvLCelTIzq1UfSj5aoSFbsEs6DTgNQF1TcK6umTVLm+/WU6iney/witITkiYDewFLSs97coSZtUo/Sj5aoUij+2tggqSTASR1kaWNXRoRGxtROTOzotp9wZvk4YWICEnHAxdJ+geyBvtq4JP1qMikseOT4rb2p69V2901JiluS1/auqBzdn1+Utydq5YmxQFMHDsuKe7mVfcnxRVJx0rNv43EX8/UNW3XbFrPhDHdSbG/W/VAUtwpMw5Lirt0xYKkOIBxo8cmxe2U+LtbxNJ1aTnCqXVM/VkDbOzZkhQ3piut+RiTmA9eL+2eMlZ0EfPHgD9rUF3qKrXBteZLbXDNatFfYGJUKxS+kSapD7g7f+0fgfd5eMHM2kV6n741all7YVNEzImIA4CtwAfqXCczs5r1K/2oRtLRku6XtETSx4eIe3u+s84h1coc7n4VN+IcXTNrI/XKXsiTBS4EjiFb+uBESfuXidsJ+Dvg1pT61dzoShqdV2a7FVa8iLmZtUodsxcOBZZExNKI2ApcARxbJu7zwL8Cm1PqV0ujO17SncBCstlo3xoc4DxdM2uVIsMLpR3E/DitpKiZwGMlj5fl57aRdDCwZ0T8IrV+tcxI2xQRc2p4nZlZwxVJGYuIeWT7PhYmaRRwPnBKkdc1ZBqwmVmr9NUvY2w5sGfJ41n5uQE7AQcA12eLMDIdmC/pbRGxsFKhDW90UxcT70lM3k7N8Vy3OX0secq4tCGQP657rHoQMG3C5ORrR6RNPFjbm/Z+iiTBpy5inho3tTt9KGntlrT3k3rtK566vXoQcMz0g5LiAP6w/pGkuNTJNeu2pGdWjk/8PU+dkNI1Kn0kcUNP0tBk8sSMjVvTyquXOk6OuA3YV9I+ZI3tCcB7Bp6MiHXAtIHHkq4Hzh6qwYWCY7qSZgG/lvSApKWSLpDkTHcrJLXBNatFvZZ2jIhest1xfkU2J+GqiFgk6VxJb6u1fsk93ZJFzC+OiGPzdIp5wBfJ0iXMzFqunlufRcTVZMsdlJ77TIXYI1LKrMci5idLmlSgHDOzhhkJi5g/zKAJEs/J0+3zR0kza56+AkcrNORGWmkaxtjuWa1aQc3MRqCRsIj5dCBt3UEzswbrpOGFSouYXxARmxpROTOzotq90a1lEfML80XMdwOujIght2DvT8xD3dy7ta5xRazZtL6u5W3pTcvbbLUiOb0pGvG+U/NQU/33E7+va3mN0ru13RcohA1Nzr9N1e7jmbUsYv42AElzgcslHRwRdzSicmZmRbX7mG6hRnfQAuYPAS+PiLWNqJiZWS3a/TNC0VXGShcwXw18sAF1MjOrWT+RfLTCcFLGFgAvq1dFzMzqod03pqxpEfM8c+EoYH6F572IuZm1RLtvwV600R1YwPwJYA/gf8oFeRFzM2uVdk8Zq2lMF9gbEB7TNbM206tIPlqhpuGFfMv1DwMfyfdKMzNrC502vLBNRPweuAs4sX7VMTMbnnYfXig6OWKSpE+RrZ7eR1bvxY2omJlZLVqVCpaq6OSIw4C3AgdHxBZJ04C0PTvMzJqgvZvc4nm6zwOejogtABHxdP2rZGZWu07L070G2FPSYkkXSXpduSDn6ZpZq/QRyUcrFGp0I2I92Zq6pwErgSslnVImznm6ZtYSHXUjDbbtjXY92V7vdwPvAy6tb7XMzGoTbT6qW/RG2ouA/oh4ID81B3ik7rUyM6tRu4/pFu3pTgK+Lmkq0AssIRtq2KGlLr95/h6vT4o788nraq9MBbtNmJIUt3Ljurpfe0cwuXtCUtwzWzbW/dqjR3UlxZ02/bDkMi9a8dukuHGj05KHiiz+P0pp/yJeOe1FSXF3rHkw+dr10FEpY2Q7/04AtpLtjbYr8L/KfkiHRkT9t3UwMyugvZvc4pMjVpENKSDpHGB9RHypAfUyM6tJb5s3u143wcw6SkfdSEsl6TTysV51TcFpY2bWLJ12Iy1JRMwD5gGMHjuzvf/smFlHGZE9XTOzVhmRPV0zs1bpC/d066oR+Zjjx3QnxX127a3JZabaY+LUpLgnN9R/p/u9Ju+eFPfoM08lxe08flLytbuUNgP96Y3PJMU1Iv+23i5deVty7Mkz0nJ6b9+8Iilu8brlydeePHZ8UtyTW9Nywrf29iRfux46LU93m4g4p471sBEktcE1q0W7j+kW+u2XNFvSPYPOnSPp7PpWy8ysNh234I2ZWTtr9+EFf84zs44SBf6rRtLRku6XtETSx8s8f5akeyXdJenXkvauVmZDGl0vYm5mrdIXkXwMRVIXcCFwDLA/cKKk/QeF/R44JCJeBvwI+GK1+hVtdCvV8jnnvYi5mbVKP5F8VHEosCQiluaLeV0BHFsaEBHXRcRA6swtwKxqhRZtdFcBOw86twvgvdLMrC0UuZFW+qk8P0qXqp0JPFbyeFl+rpJTgf+uVr+iq4ytl/S4pCMj4lpJuwBHA18tUs5wNCIfc2PPlrqXmaoR+bepUvNvU63ZtL6u5e0oevv7kuM+OqPstoLbOe/xG5Lizt/9iKS4M/rS9xpYtenZpLj373xwUtwX1j2efO16KJIyVrpkwXBI+gvgEKDqD7iW7IWTgQslnZ8//lxENHeVYrMdUGqDa8NTx+yF5cCeJY9n5eeeQ9IbgE8BrxvYKX0oycMLkq6T9KaIuDciXh8Rc8j2RntNahlmZo0WEclHFbcB+0raR9JY4ARgfmmApIOAfwfeFhFJHx2LjOlenl+01An5eTOztlCvLdgjohc4HfgV8EfgqohYJOlcSW/Lw/6NbBuzH0q6U9L8CsVtU2R44UfAP0oaGxFbJc0GZgA3FijDzKyh6jk5IiKuBq4edO4zJV+/oWiZyT3diFgN/I4sZw2yXu5VUaaP7jxdM2uVOg4vNETRlLHSIYaKQwvO0zWzVqljnm5DFG10fwocJelgYEJE3N6AOpmZ1aye04AboZY83euAb+MbaGbWhtp9EXMVHdeQdBzwY+AlEXFftfgxiXuk1fvbpAKxqdeeOHZcUtyGrZuTr51az/b+NcrsNmFKcuzsiXskxS1cuTgpbsq4tGGstZtbd49hTFd6H6cvccLF9ImDJ4iW94sps5OvfdDyO5LiJiQu/r+pwOSjnq3Li/zTLevVM49M/udy0/Jrh329oqoOL0j6sqQzSk79DfCtgQZX0nmSzmpUBa3zpDa4ZrXohDHdm4C5AJJGAdOAl5Y8Pxe4uf5VMzMrrhOyF24GBjZseilwD/CspJ0ldQMvAdI+j5iZNVi793SrDjJFxApJvZL2IuvVLiBbaecwYB1wd77smZlZy7X7HmmpI/s3kzW4c4HzyRrduWSN7k2Dg/Pl0U4DGNU1Befqmlmz9EWrdj9Lk5qnOzCueyDZ8MItZD3dsuO5nhxhZq3SCWO6kDWsbwVWR0RfPiV4KlnD65toZtY2dvgx3dzdZFkLPxh0blJEDLlrxE7dE5IuMGlMWg7sExvWJMWl5i8CPLkxbSHxzb1pQ9ejR3UlXzs1H3PmTrsmxW3t602+9totaTmrYxLfz8qN65LjUr9HoxNzW1Pzb4skZY4dPSYpbmp32qe5Ih97V218JiluxfrVSXEHJcYBvPN5f5IU9+Mn0+6fz54yPfna9dARY7oR0QdMHnTulEZUyDpfkT9KZkX1t/mMtCKLmO8p6aF8ix7ylLGH8iUezczaQruvvVBkacfHgIuBL+SnvgDMi4iHG1AvM7Oa9EV/8tEKRfdI+zJwez4t+DVkq6qbmbWNdh9eKLrKWI+kjwK/BP40InrKxZXm6U7o3o3uMekLoZiZDUe730grup4uZDtHPA4cUCmgNE/XDa6ZNVN/RPLRCoUaXUlzgDcCrwLOlPS8htTKzKxGHXMjTZLIbqSdERGPku2C+aVGVczMrBZ90Zd8tELyIub5OO1REfHu/HEX2b7wZ0bEbyq9bnTiIuZmZr11WMR8r10OTG5zHl19d/stYj4gIuYBJ0j6rbLYk7oAAAnWSURBVKRj8unABwO7S/pl46poZpauU6YBAxARIekDwA/zvdJGA/8MHN2IypmZFdWqhWxSFc3TJSLukfQz4GPAROCyiHiw7jUzM6tBR+Xplvgc2W4RW4FDBj9Zmqcrr6drZk3U7nm6NTW6EbFB0pXA+ojYbqvPfPx3HvhGmpk1V7svYl5rTxegPz/MzNpGx43pmpm1s04d022ZA3eZnRR39+qHk8s8ecZh1YOAy1YsSIqbOi59DDt18e1G6BqVljHY35/2gaYRv+qpSZSt/Gf2yt1elBR368r7G1yT+pgxaZekuNQF1D8+43XDqU5h7d7TLToN+HhJd0q6EzgO+AtJ/ZKOaUz1zMyK6bQ83R8DPx54nGcpnAT8qs71MjOrSbv3dGseXpC0H/AZYG5Em98uNLMRoyOzFySNIduk8iP54jdmZm2hU2+kfR5YFBFXlnvSkyPMrFXafXih8CLmko4A3s4QW/WULmLuBtfMmqme6+lKOlrS/ZKWSPp4mee7JV2ZP39ryka9RbMXdga+A5wcEc8Wea2ZWTNERPIxlHz52gvJdsvZHzhR0v6Dwk4F1kTEC8n2kPzXavUr2tP9ALA7cPFA6lh+vLtgOWZmDVHH7XoOBZZExNKI2ApcARw7KOZY4Lv51z8Cjso3fKisyF+Feh3AafWObVWcr92+cb52+8YVjW3UQXbvaWHJcVrJc+8Avlny+L3ABYNefw8wq+Txg8C0Ia/Zoje6sN6xrYrztds3ztdu37iisa04GtXo1rIbsJnZSLAc2LPk8az8XNkYSaOBKcCqoQp1o2tmVt5twL6S9pE0FjgBmD8oZj7wvvzrdwDXRt7lraRVC97Ma0Bsq+J87faN87XbN65obNNFRK+k08mWOegCvh0RiySdSzY0Mh/4FvA9SUuA1WQN85CSdwM2M7Ph8/CCmVkTudE1M2uipje6ko6TFJJePERMXz7p4g+S7pA0d4jY6ZKukPSgpNslXZ2vgFauvEV5mR+RVPa9l8QOHNtN/RsidnaFuD0k/UDS0ryOCyQdXyZu/aDHp0i6YIjrr6/0XNHY0uclvVnSYkl713rd/Gf8/ZLHoyWtlPTzMnHnlTw+W9I5FcqcJemnkh7If95fzW9wlIsd+NncI+mHkiYklLlU0gWSuquU9zNJU4d475/Kf9fuyl/zyjIxu5b83jwhaXnJ47ElcbMl3TPotedIOnvQueskvWnQuTMkXVzy+MuSzih5/CtJ3yx5fJ6ks0oe7ynpIUm75I93zh/PHnQdSfqtStbVlvROSb8s876P13P/zdypkbYmdwty364EbgQ+N0TM+pKv3wT8pkKcgAXAB0rOvRw4fIjydgf+t9L1S2MT3kvV2Ap13Bv4ULXygFMYlBfYqLoOPA8cBSwBXjDM970euBMYnz8+Jn/880Fxm4GHyHMbgbOBcyp8H38H/GX+uIvsJsa/JfwO/QdwVoEyv1qlvO8Cn6pw3cPyn3d3/ngaMKPK9+oc4OwKz80G7qkWT5bk/51B524BXlvy+B3AVfnXo4DbgQUlzy8AXjWojL8H5uVf/zvwiQr1PAD4IzAOmAQ8MNTv0KB6/wYYlfq7vKMfTe3pSpoEvIZsvnLVu3y5ycCaCs+9HuiJiG8MnIiIP0TEjZUKi4inyH7Qp0tVpuvVx5HA1kF1fCQivt6Eaxci6bXAJcBbI+LBOhR5NfCW/OsTgcvLxPSS3cU+s0pZRwKbI+I7ABHRl7/mryr1YkvcCLywQJkn57+rlSwAZlZ47nnA05Hvkh0RT0fEiir1q4cfAW8Z6CXnvdEZZO99wM1kfxQAXkqW2P9s3oPtBl4C3DGo3C8Dr8p7yK8BvlTu4hFxD/Az4GNk62xfVu13SP+3Jvd7YwStyd3s4YVjgV9GxGJglaRXVIgbn3/suA/4JtlSkuUcQPbXupCIWErWq9l9iGunrCtRGvvjCjEvZftf5JTy7gTOTXxdPXQDPwGOi4j76lTmFcAJksYBLwNurRB3IXCSpClDlPVSBv2sI+IZ4FHKN6jAtoT1Y4C7C5T5cKUylS2CchTb52sOuAbYMx+euUhSUzYIi4jVZL32gY/pJ5D1aqMkZgXQK2kvYC7ZH49byRriQ4C7I1tjoLTcHuCjZI3vGfnjSj4HvCevwxeHqq9G8Jrczc7TPRH4av71Ffnjco3mpoiYAyDpMOAySQeU/gI10LZr1zkWAEkXkvUYtkbEnwxVnqRTyP4xNEMPWU/oVODv6lFgRNyV97hOJOv1Vop7RtJlwIeBTfW4NvkfsPzrG8mGDepR3kyyj9H/Uy4oItbnnYnDyT6JXSnp4xFxaY3XrfQ7X+785WSN7U/z/59aJuZmsgZ3LnA+2fuZC6wDbqpwrWOAx8k6OWXfN0BEbJB0JdlQzJZKcbkh1+TuZE3r6eaD8UcC35T0MNlfz3dV+4gfEQvIxsV2K/P0IqBSb3moujwf6AOeKvraGiwCDh54EBEfJOsplXs/rdQPvAs4VNIn61jufLKPpOWGFkp9hayRqLQA870M+llLmgzsRTYGPdimiJiTHx8a3IOrUuZ0YPDWvQN/EPcmGwv+YKU3EhF9EXF9RHyWbN3pt1eKTbAK2HnQuV2Ap8vE/pRslauDgQkRUa5DcxNZI3sg2fDCLWQ93blkDfJzSJoDvBF4FXCmpOdVqW9/flSkhDW5O1kzhxfeAXwvIvaOiNkRsSfZDZTDh3qRsiyHLsrPZ74W6Fa2U8VA/MskVSxT0m7AN8huUDWj53wtME7S35ScqzYG2RIRsZFsDPYkSeV6SbX4NtlNy3If70uvvRq4ivK9M4BfAxMknQzbPuafB1ya17sWlcq8ICLK9rjza30Y+Eg+dPEckl4kad+SU3OAR2qsHxGxHnhc0pF5+bsARwO/rRB7Hdn3vNIfuZuBtwKr8z8Oq4GpZA3vcxrdvEN0MdmwwqPAv1FhTDeVvCZ3UxvdEynZSTj3n/n5wbaNbZJlO7wvv8nxHHmjeTzwBmUpRIuAfwGeqFDeIrLMhWvIxp/KGTym+4Xkd1hGXsfjgNfl6Ta/I7v7/bHhlFtE3jhU+7gHbGv8jgY+LeltFcImSFpWcpxVIY6IWBYRX0us6nlkn2rKlTPws36npAeAxWSZDzX3ykvKfEde5iqgPyL+qcrrfg/cRfnf3UnAdyXdK+kussWvz6m1jrmTgX/I/z1cS/ZHrNJNqsvJMngqNbp3k32Pbxl0bl1EDO49vx94NCIGhhQuAl4yzHHqEb8mt6cBjwCSXg5cEhGHtrou7UxZPvjlwPERkXrz06wQN7odTtIHyD4OnxER17S6PmYjnRtdM7Mm8toLZmZN5EbXzKyJ3OiamTWRG10zsyZyo2tm1kT/H83uMBMCwahEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}